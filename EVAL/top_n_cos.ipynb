{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pth_models = \"/home/sondors/Documents/price/BERT_data/data/27-03-2024_Timofey/740101_models.csv\"\n",
    "pth_offers = \"/home/sondors/Documents/price/ColBERT_data/10_categories/740101/test/740101_lr04_bsize230_offers_top_n_model_id_0.csv\"\n",
    "\n",
    "df_models = pd.read_csv(pth_models, sep=\";\")\n",
    "# df_models = df_models.drop(columns=['average_price', 'comment'])\n",
    "df_offers = pd.read_csv(pth_offers, sep=\";\")\n",
    "# df_offers = df_offers.drop(columns=['true_match', 'false_match'])\n",
    "\n",
    "df_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = []\n",
    "\n",
    "for index, row in df_offers.iterrows():\n",
    "    model_ids = [row[f'model_id_pred_{i}'] for i in range(1, 6)]\n",
    "    similarities = [row[f'similarity_{i}'] for i in range(1, 6)]\n",
    "    \n",
    "    top_n.append({'model_ids': model_ids, 'similarity': similarities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from interface import prepare_tsv, save_index, top_n_similar, Collection, pair_scores, load_model, get_query_emb_batch, cosine_similarity_batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "ckpt_pth = \"/home/sondors/Documents/ColBERT_weights/740101_lr04_bsize230/colbert-489-finish\"\n",
    "\n",
    "doc_maxlen = 300\n",
    "nbits = 2   # bits определяет количество битов у каждого измерения в семантическом пространстве во время индексации\n",
    "nranks = 1  # nranks определяет количество GPU для использования, если они доступны\n",
    "kmeans_niters = 4 # kmeans_niters указывает количество итераций k-means кластеризации; 4 — хороший и быстрый вариант по умолчанию. \n",
    "\n",
    "id_to_name = dict(zip(df_models['model_id'], df_models['full_name']))\n",
    "checkpoint = load_model(ckpt_pth, doc_maxlen, nbits, kmeans_niters, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, List, Dict, Union, Any\n",
    "\n",
    "def top_n_similar_cos(checkpoint, id_to_name, offers: List[str], top_n: List[Dict[str, List]], batch_size: int, batch_size2: int) -> List[Dict[str, List]]:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between each offer and the full names of each model in top_n using batch processing.\n",
    "\n",
    "    Args:\n",
    "        checkpoint: The checkpoint object used for generating embeddings.\n",
    "        id_to_name: id_to_name = dict(zip(df_models['model_id'], df_models['full_name']))\n",
    "        offers (List[str]): List of offer descriptions.\n",
    "        top_n (List[Dict[str, List]]): List of dictionaries containing model_ids, similarity scores, and full names.\n",
    "        batch_size (int): The batch size to use during inference.\n",
    "        batch_size2 (int): The size of the sub-batches to split the input sentences into for batching.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, List]]: List of dictionaries containing model_ids, similarity scores, full names, and cosine similarities.\n",
    "    \"\"\"\n",
    "    top_n_extended = top_n.copy()\n",
    "    for item in top_n_extended:\n",
    "        model_ids = item['model_ids']\n",
    "        full_names = [id_to_name[model_id] for model_id in model_ids]\n",
    "        item['full_names'] = full_names\n",
    "\n",
    "    offer_embs = get_query_emb_batch(offers, checkpoint, batch_size, batch_size2)\n",
    "\n",
    "    for i, offer_emb in enumerate(offer_embs):\n",
    "        cosine_sims = []\n",
    "\n",
    "        for j, model_info in enumerate(top_n_extended[i]['full_names']):\n",
    "            \n",
    "            model_emb = get_query_emb_batch([model_info], checkpoint, batch_size, batch_size2)[0]\n",
    "            similarity_scores = cosine_similarity_batch([offer_emb], [model_emb], batch_size)\n",
    "            cosine_sims.append(similarity_scores[0][0])\n",
    "\n",
    "        top_n[i]['cosine_sims'] = cosine_sims\n",
    "    return top_n\n",
    "\n",
    "top_n_cos_sim = top_n_similar_cos(checkpoint, id_to_name, list(df_offers['name']), top_n, batch_size=100, batch_size2=1000)\n",
    "top_n_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers_cos = df_offers.copy()\n",
    "df_offers_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_to_df(df, top_n, n):\n",
    "    indices = [i for i in range(len(df))]\n",
    "    for idx, insert_dict in zip(indices, top_n):\n",
    "        for i in range(n):\n",
    "            col_similarity = f'cosine_sims{i+1}'\n",
    "            df.loc[idx, col_similarity] = round(float(insert_dict['cosine_sims'][i]), 2)\n",
    "    return df\n",
    "\n",
    "df_offers_cos = df_offers.copy()\n",
    "df_offers_cos = top_n_to_df(df_offers_cos, top_n, 5)\n",
    "\n",
    "df_offers_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers_cos.to_csv('/home/sondors/Documents/price/ColBERT/EVAL/740101_lr04_bsize230_offers_top_n_model_id_0_cos.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
