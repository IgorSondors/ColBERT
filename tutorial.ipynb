{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pandas as pd\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23022/17807022.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_models = pd.read_csv(pth_models, sep=\";\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>average_price</th>\n",
       "      <th>name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>category_name</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>920-005619</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>Logitech 920-005619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zipper Bag</td>\n",
       "      <td>Hama</td>\n",
       "      <td>Hama Zipper Bag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CC-3064</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>Nokia CC-3064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751488</td>\n",
       "      <td>990.0</td>\n",
       "      <td>CKS-X7/R</td>\n",
       "      <td>Sony</td>\n",
       "      <td>Sony CKS-X7/R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-031023</td>\n",
       "      <td>Era Pro</td>\n",
       "      <td>Era Pro EP-031023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103209</th>\n",
       "      <td>7049424</td>\n",
       "      <td>16459.0</td>\n",
       "      <td>MD-108</td>\n",
       "      <td>Mivo</td>\n",
       "      <td>Mivo MD-108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103210</th>\n",
       "      <td>7049425</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>MD-165</td>\n",
       "      <td>Mivo</td>\n",
       "      <td>Mivo MD-165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103211</th>\n",
       "      <td>7049426</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>Boost 20W</td>\n",
       "      <td>Rocket</td>\n",
       "      <td>Rocket Boost 20W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103212</th>\n",
       "      <td>7049427</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>Motion 10W</td>\n",
       "      <td>Rocket</td>\n",
       "      <td>Rocket Motion 10W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103213</th>\n",
       "      <td>7049428</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>Z1</td>\n",
       "      <td>SmartBuy</td>\n",
       "      <td>SmartBuy Z1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103214 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_id  average_price        name brand_name            full_name  \\\n",
       "0         623742            NaN  920-005619   Logitech  Logitech 920-005619   \n",
       "1         721952            NaN  Zipper Bag       Hama      Hama Zipper Bag   \n",
       "2         721970            NaN     CC-3064      Nokia        Nokia CC-3064   \n",
       "3         751488          990.0    CKS-X7/R       Sony        Sony CKS-X7/R   \n",
       "4         751989            NaN   EP-031023    Era Pro    Era Pro EP-031023   \n",
       "...          ...            ...         ...        ...                  ...   \n",
       "103209   7049424        16459.0      MD-108       Mivo          Mivo MD-108   \n",
       "103210   7049425         8812.0      MD-165       Mivo          Mivo MD-165   \n",
       "103211   7049426         4240.0   Boost 20W     Rocket     Rocket Boost 20W   \n",
       "103212   7049427         2990.0  Motion 10W     Rocket    Rocket Motion 10W   \n",
       "103213   7049428         8900.0          Z1   SmartBuy          SmartBuy Z1   \n",
       "\n",
       "       comment                                      category_name  category_id  \n",
       "0          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "1          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "2          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "3          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "4          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "...        ...                                                ...          ...  \n",
       "103209     NaN                               портативная акустика         3904  \n",
       "103210     NaN                               портативная акустика         3904  \n",
       "103211     NaN                               портативная акустика         3904  \n",
       "103212     NaN                               портативная акустика         3904  \n",
       "103213     NaN                               портативная акустика         3904  \n",
       "\n",
       "[103214 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_models = \"/home/sondors/Documents/price/ColBERT_data/18_categories/test/models_18_categories.csv\"\n",
    "df_models = pd.read_csv(pth_models, sep=\";\")\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tsv(df, pth, category_id):\n",
    "    \"\"\"\n",
    "    Делим модели по full_name и соответствующие им model_id на два tsv файла \n",
    "\n",
    "    {category_id}_models.tsv:\n",
    "\n",
    "    0   model0\n",
    "    1   model1\n",
    "    2   model2\n",
    "\n",
    "    {category_id}_models_id.tsv:\n",
    "\n",
    "    0   model_id0\n",
    "    1   model_id1\n",
    "    2   model_id2\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def df_split(df):\n",
    "        df1 = pd.DataFrame()\n",
    "        df1[\"id\"], df1[\"full_name\"] = [i for i in range(len(df))], df[\"full_name\"]\n",
    "        \n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"id\"], df2[\"model_id\"] = [i for i in range(len(df))], df[\"model_id\"]\n",
    "\n",
    "        return df1, df2\n",
    "    \n",
    "    models, models_id = df_split(df)\n",
    "    models.to_csv(os.path.join(pth,f\"{category_id}_models.tsv\"), sep='\\t', header=False, index=False)\n",
    "    models_id.to_csv(os.path.join(pth,f\"{category_id}_models_id.tsv\"), sep='\\t', header=False, index=False)\n",
    "\n",
    "categories = [\n",
    "    \"диктофоны, портативные рекордеры\",\n",
    "    \"электронные книги\",\n",
    "    \"автомобильные телевизоры, мониторы\",\n",
    "    \"смарт-часы и браслеты\",\n",
    "    \"портативные медиаплееры\",\n",
    "    # \"чехлы, обложки для гаджетов (телефонов, планшетов etc)\",\n",
    "    \"портативная акустика\",\n",
    "    \"мобильные телефоны\",\n",
    "    \"VR-гарнитуры (VR-очки, шлемы, очки виртуальной реальности, FPV очки для квадрокоптеров)\",\n",
    "    \"планшетные компьютеры и мини-планшеты\",\n",
    "    \"наушники, гарнитуры, наушники c микрофоном\",\n",
    "    \"радиоприемники, радиобудильники, радиочасы\",\n",
    "    \"магнитолы\",\n",
    "    \"GPS-навигаторы\"\n",
    "    ]\n",
    "\n",
    "categories_id = [\n",
    "    3902,\n",
    "    510402,\n",
    "    4302,\n",
    "    2815,\n",
    "    3901,\n",
    "    # 3994,\n",
    "    3904,\n",
    "    2801,\n",
    "    3908,\n",
    "    510401,\n",
    "    2102,\n",
    "    3903,\n",
    "    3907,\n",
    "    280801\n",
    "    ]\n",
    "\n",
    "dst_fld = \"/home/sondors/Documents/price/ColBERT/tmp_tutorial\"\n",
    "for cat_id in categories_id:\n",
    "    category_models = df_models[df_models.category_id == cat_id].reset_index(drop=True)\n",
    "    prepare_tsv(category_models, dst_fld, cat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индексируем модели и сохраняем индекс на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:21:39] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:21:39] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3902_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3902_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3902_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:21:42] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:21:43] [0] \t\t # of sampled PIDs = 488 \t sampled_pids[:3] = [213, 375, 5]\n",
      "[Feb 02, 22:21:43] [0] \t\t #> Encoding 488 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:04<00:00,  1.96it/s]\n",
      "WARNING clustering 4691 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:21:47] [0] \t\t avg_doclen_est = 10.116803169250488 \t len(local_sample) = 488\n",
      "[Feb 02, 22:21:47] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:21:47] [0] \t\t *Estimated* 4,936 embeddings.\n",
      "[Feb 02, 22:21:47] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3902_2bits/plan.json ..\n",
      "Clustering 4691 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.08 s, search 0.07 s): objective=392.74 imbalance=1.505 nsplit=0        \n",
      "[0.009, 0.008, 0.01, 0.007, 0.011, 0.008, 0.01, 0.01, 0.009, 0.009, 0.008, 0.009, 0.008, 0.007, 0.008, 0.008, 0.01, 0.008, 0.01, 0.008, 0.009, 0.01, 0.009, 0.009, 0.008, 0.011, 0.008, 0.008, 0.007, 0.009, 0.01, 0.01, 0.008, 0.009, 0.009, 0.009, 0.008, 0.008, 0.009, 0.007, 0.011, 0.009, 0.009, 0.01, 0.01, 0.009, 0.008, 0.008, 0.007, 0.007, 0.009, 0.007, 0.009, 0.008, 0.008, 0.008, 0.008, 0.01, 0.009, 0.007, 0.009, 0.007, 0.009, 0.01, 0.008, 0.008, 0.008, 0.01, 0.007, 0.008, 0.009, 0.009, 0.008, 0.007, 0.008, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.009, 0.007, 0.008, 0.008, 0.007, 0.008, 0.01, 0.009, 0.009, 0.008, 0.008, 0.007, 0.009, 0.01, 0.007, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.007, 0.009, 0.009, 0.009, 0.008, 0.01, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.01, 0.01, 0.009, 0.008, 0.008, 0.009, 0.01, 0.008, 0.007, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.01, 0.008, 0.008, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.011, 0.009, 0.009, 0.009, 0.009, 0.007, 0.011, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.01, 0.009, 0.009, 0.009, 0.009, 0.008, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.01, 0.006, 0.01, 0.008, 0.007, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.009, 0.011, 0.008, 0.009, 0.008, 0.007, 0.011, 0.009, 0.009, 0.009, 0.009, 0.009, 0.007, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.01, 0.007, 0.009, 0.008, 0.008, 0.009, 0.006, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.007, 0.008, 0.009, 0.01, 0.008, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.01, 0.008, 0.01, 0.01, 0.008, 0.011, 0.008, 0.01, 0.008, 0.008, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.01, 0.007, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.007, 0.009, 0.007, 0.009, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.008, 0.009, 0.009, 0.007, 0.007, 0.01, 0.008, 0.008, 0.007, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.008, 0.011, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.007, 0.009, 0.008, 0.007, 0.009, 0.009, 0.01, 0.009, 0.012, 0.008, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.008, 0.009, 0.008, 0.008, 0.011, 0.008, 0.009, 0.01, 0.007, 0.01, 0.007, 0.011, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.009, 0.009, 0.009, 0.009, 0.007, 0.01, 0.01, 0.007, 0.012, 0.008, 0.008, 0.01, 0.008, 0.009, 0.01, 0.011, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.008, 0.006, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.01, 0.008, 0.01, 0.008, 0.008, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.008, 0.009, 0.009, 0.009, 0.007, 0.01, 0.008, 0.01, 0.008, 0.008, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.008, 0.007, 0.008, 0.007, 0.008, 0.007, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.01, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.011, 0.01, 0.009, 0.008, 0.01, 0.008, 0.007, 0.009, 0.009, 0.009, 0.009, 0.007, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.009, 0.01, 0.01, 0.007, 0.009, 0.008, 0.009, 0.008, 0.008, 0.011, 0.009, 0.009, 0.009, 0.01, 0.009, 0.009, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.009, 0.009, 0.011, 0.008, 0.008, 0.008, 0.009, 0.008, 0.007, 0.009, 0.008, 0.009, 0.01, 0.009, 0.009, 0.009, 0.007, 0.008, 0.01, 0.009, 0.008, 0.01, 0.01, 0.007, 0.009, 0.01, 0.007, 0.008, 0.009, 0.01, 0.008, 0.008, 0.008, 0.008, 0.009, 0.01, 0.009, 0.009, 0.007, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.007, 0.008, 0.007, 0.009, 0.009, 0.007, 0.009, 0.009, 0.01, 0.009, 0.007, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.007, 0.009, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.011, 0.007, 0.01, 0.009, 0.007, 0.009, 0.008, 0.01, 0.008, 0.009, 0.01, 0.008, 0.009, 0.007, 0.009, 0.009, 0.008, 0.007, 0.008, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.009, 0.009, 0.008, 0.008, 0.008, 0.008, 0.007, 0.008, 0.01, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.008, 0.007, 0.008, 0.007, 0.01, 0.01, 0.008, 0.009, 0.008, 0.007, 0.008, 0.008, 0.009, 0.009, 0.01, 0.008, 0.009, 0.01, 0.01, 0.009, 0.009, 0.009, 0.009, 0.01, 0.007, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.007, 0.008, 0.008, 0.009, 0.009, 0.009, 0.008, 0.008, 0.009, 0.01, 0.007, 0.008, 0.008, 0.008, 0.01, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.008, 0.008, 0.007, 0.009, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008]\n",
      "[Feb 02, 22:21:47] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:21:47] #> Got bucket_cutoffs = tensor([-5.3006e-03, -2.9246e-05,  5.1356e-03]) and bucket_weights = tensor([-0.0118, -0.0021,  0.0020,  0.0116])\n",
      "[Feb 02, 22:21:47] avg_residual = 0.008613561280071735\n",
      "[Feb 02, 22:21:47] [0] \t\t #> Encoding 488 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:01<00:03,  1.88it/s]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:01<00:02,  2.01it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [00:01<00:01,  2.09it/s]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:02<00:01,  2.11it/s]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:02<00:00,  2.11it/s]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:03<00:00,  2.07it/s]\u001b[A\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.14it/s]\u001b[A\n",
      "1it [00:03,  3.80s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2394.01it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 146092.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:21:51] [0] \t\t #> Saving chunk 0: \t 488 passages and 4,937 embeddings. From #0 onward.\n",
      "[Feb 02, 22:21:51] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:21:51] [0] \t\t Found all files!\n",
      "[Feb 02, 22:21:51] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:21:51] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:21:51] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:21:51] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:21:51] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:21:51] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:21:51] len(emb2pid) = 4937\n",
      "[Feb 02, 22:21:51] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3902_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:21:51] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3902_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:21:51] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:21:51] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510402_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/510402_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_510402_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:21:55] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:21:56] [0] \t\t # of sampled PIDs = 680 \t sampled_pids[:3] = [426, 10, 305]\n",
      "[Feb 02, 22:21:56] [0] \t\t #> Encoding 680 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.80it/s]\n",
      "WARNING clustering 6031 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:02] [0] \t\t avg_doclen_est = 9.335293769836426 \t len(local_sample) = 680\n",
      "[Feb 02, 22:22:02] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:22:02] [0] \t\t *Estimated* 6,347 embeddings.\n",
      "[Feb 02, 22:22:02] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510402_2bits/plan.json ..\n",
      "Clustering 6031 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.08 s, search 0.08 s): objective=767.196 imbalance=1.570 nsplit=0       \n",
      "[0.012, 0.011, 0.012, 0.011, 0.014, 0.011, 0.012, 0.012, 0.012, 0.014, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.013, 0.012, 0.01, 0.011, 0.009, 0.01, 0.011, 0.01, 0.01, 0.013, 0.011, 0.011, 0.009, 0.012, 0.01, 0.011, 0.01, 0.009, 0.01, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.013, 0.01, 0.01, 0.012, 0.009, 0.01, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.009, 0.011, 0.012, 0.012, 0.01, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.012, 0.009, 0.011, 0.011, 0.014, 0.012, 0.012, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.01, 0.012, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.013, 0.011, 0.009, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.012, 0.01, 0.011, 0.01, 0.01, 0.012, 0.01, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.012, 0.011, 0.011, 0.013, 0.01, 0.01, 0.01, 0.009, 0.009, 0.009, 0.012, 0.009, 0.009, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.01, 0.014, 0.01, 0.01, 0.011, 0.01, 0.012, 0.011, 0.01, 0.013, 0.012, 0.012, 0.009, 0.01, 0.01, 0.01, 0.012, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.009, 0.01, 0.011, 0.011, 0.012, 0.012, 0.01, 0.01, 0.009, 0.012, 0.011, 0.01, 0.01, 0.012, 0.013, 0.012, 0.011, 0.012, 0.012, 0.01, 0.013, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.013, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.01, 0.012, 0.01, 0.01, 0.012, 0.012, 0.01, 0.01, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.009, 0.012, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.012, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.01, 0.01, 0.012, 0.009, 0.011, 0.012, 0.01, 0.013, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.011, 0.011, 0.01, 0.013, 0.01, 0.013, 0.01, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.013, 0.012, 0.012, 0.011, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.012, 0.01, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.013, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.014, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.009, 0.01, 0.011, 0.013, 0.009, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.01, 0.011, 0.01, 0.01, 0.011, 0.012, 0.012, 0.01, 0.013, 0.01, 0.011, 0.011, 0.011, 0.009, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.01, 0.011, 0.01, 0.009, 0.012, 0.01, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.01, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.009, 0.011, 0.01, 0.012, 0.013, 0.009, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.009, 0.009, 0.01, 0.011, 0.012, 0.011, 0.012, 0.012, 0.01, 0.013, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.011, 0.013, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.013, 0.01, 0.011, 0.013, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.011, 0.008, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.014, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.012, 0.011, 0.009, 0.012, 0.01, 0.009, 0.012, 0.009, 0.01, 0.012, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.009, 0.009, 0.01, 0.012, 0.012, 0.01, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.01, 0.012, 0.01, 0.013, 0.01, 0.011, 0.012, 0.01, 0.01, 0.012, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.009, 0.012, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.01, 0.012, 0.01, 0.01, 0.013, 0.01, 0.01, 0.012, 0.01, 0.011, 0.01, 0.011, 0.012, 0.009, 0.01, 0.011, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.01, 0.011, 0.01, 0.012, 0.009, 0.01, 0.012, 0.01, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.011, 0.009, 0.011, 0.01, 0.01, 0.012, 0.011, 0.012, 0.013, 0.012, 0.01, 0.01, 0.012, 0.014, 0.011, 0.011, 0.012, 0.009, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.011, 0.013, 0.01, 0.012, 0.01, 0.012, 0.01, 0.012, 0.011, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012]\n",
      "[Feb 02, 22:22:02] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:22:02] #> Got bucket_cutoffs = tensor([-7.3699e-03,  3.0249e-05,  7.3755e-03]) and bucket_weights = tensor([-0.0153, -0.0029,  0.0029,  0.0153])\n",
      "[Feb 02, 22:22:02] avg_residual = 0.010851368308067322\n",
      "[Feb 02, 22:22:02] [0] \t\t #> Encoding 680 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:06,  1.56it/s]\u001b[A\n",
      " 18%|█▊        | 2/11 [00:01<00:05,  1.74it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:01<00:04,  1.81it/s]\u001b[A\n",
      " 36%|███▋      | 4/11 [00:02<00:03,  1.84it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:02<00:03,  1.83it/s]\u001b[A\n",
      " 55%|█████▍    | 6/11 [00:03<00:02,  1.85it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:03<00:02,  1.86it/s]\u001b[A\n",
      " 73%|███████▎  | 8/11 [00:04<00:01,  1.86it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:04<00:01,  1.86it/s]\u001b[A\n",
      " 91%|█████████ | 10/11 [00:05<00:00,  1.87it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:05<00:00,  1.90it/s]\u001b[A\n",
      "1it [00:05,  5.87s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2308.37it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 159013.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:08] [0] \t\t #> Saving chunk 0: \t 680 passages and 6,348 embeddings. From #0 onward.\n",
      "[Feb 02, 22:22:08] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:22:08] [0] \t\t Found all files!\n",
      "[Feb 02, 22:22:08] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:22:08] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:22:08] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:22:08] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:22:08] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:22:08] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:22:08] len(emb2pid) = 6348\n",
      "[Feb 02, 22:22:08] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510402_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:22:08] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510402_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:22:08] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:22:08] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_4302_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/4302_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_4302_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:22:12] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:12] [0] \t\t # of sampled PIDs = 790 \t sampled_pids[:3] = [426, 750, 10]\n",
      "[Feb 02, 22:22:12] [0] \t\t #> Encoding 790 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:05<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:18] [0] \t\t avg_doclen_est = 9.383543968200684 \t len(local_sample) = 790\n",
      "[Feb 02, 22:22:18] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:22:18] [0] \t\t *Estimated* 7,412 embeddings.\n",
      "[Feb 02, 22:22:18] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_4302_2bits/plan.json ..\n",
      "Clustering 7043 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.10 s, search 0.09 s): objective=962.952 imbalance=1.495 nsplit=0       \n",
      "[0.013, 0.012, 0.013, 0.01, 0.016, 0.01, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.014, 0.011, 0.012, 0.013, 0.013, 0.011, 0.011, 0.011, 0.012, 0.012, 0.013, 0.012, 0.012, 0.011, 0.012, 0.01, 0.011, 0.01, 0.01, 0.013, 0.012, 0.011, 0.011, 0.013, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.01, 0.013, 0.012, 0.011, 0.012, 0.011, 0.014, 0.01, 0.01, 0.012, 0.01, 0.011, 0.013, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.012, 0.012, 0.011, 0.011, 0.01, 0.012, 0.013, 0.012, 0.012, 0.011, 0.011, 0.013, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.014, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.015, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.01, 0.012, 0.011, 0.012, 0.01, 0.012, 0.013, 0.01, 0.01, 0.011, 0.013, 0.013, 0.011, 0.013, 0.01, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.011, 0.012, 0.011, 0.011, 0.009, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.013, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.013, 0.01, 0.01, 0.012, 0.012, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.012, 0.009, 0.011, 0.011, 0.011, 0.011, 0.014, 0.01, 0.011, 0.011, 0.011, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.012, 0.009, 0.011, 0.01, 0.009, 0.01, 0.011, 0.009, 0.011, 0.01, 0.012, 0.012, 0.013, 0.011, 0.013, 0.013, 0.012, 0.011, 0.011, 0.012, 0.012, 0.013, 0.011, 0.012, 0.012, 0.011, 0.015, 0.01, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.012, 0.013, 0.013, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.014, 0.01, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.013, 0.012, 0.01, 0.011, 0.01, 0.012, 0.01, 0.011, 0.01, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.009, 0.013, 0.012, 0.011, 0.009, 0.01, 0.011, 0.013, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.012, 0.014, 0.01, 0.011, 0.013, 0.01, 0.013, 0.01, 0.011, 0.011, 0.01, 0.011, 0.013, 0.011, 0.01, 0.012, 0.011, 0.012, 0.009, 0.011, 0.01, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.011, 0.013, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.012, 0.013, 0.013, 0.012, 0.014, 0.011, 0.012, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.012, 0.012, 0.01, 0.012, 0.014, 0.01, 0.012, 0.011, 0.013, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.013, 0.01, 0.011, 0.012, 0.012, 0.01, 0.015, 0.011, 0.012, 0.012, 0.01, 0.011, 0.012, 0.014, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.012, 0.011, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.013, 0.011, 0.012, 0.01, 0.01, 0.011, 0.013, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.011, 0.009, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.009, 0.011, 0.011, 0.009, 0.012, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.013, 0.011, 0.011, 0.011, 0.013, 0.013, 0.011, 0.012, 0.013, 0.011, 0.01, 0.011, 0.012, 0.012, 0.013, 0.01, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.011, 0.013, 0.011, 0.013, 0.01, 0.012, 0.013, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.014, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.013, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.009, 0.012, 0.012, 0.012, 0.01, 0.012, 0.013, 0.011, 0.011, 0.01, 0.011, 0.011, 0.012, 0.012, 0.012, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.013, 0.011, 0.009, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.011, 0.013, 0.011, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.011, 0.013, 0.01, 0.013, 0.01, 0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.011, 0.01, 0.011, 0.01, 0.012, 0.008, 0.012, 0.012, 0.011, 0.01, 0.013, 0.013, 0.012, 0.011, 0.01, 0.011, 0.012, 0.011, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.013, 0.012, 0.012, 0.01, 0.011, 0.015, 0.011, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.013, 0.009, 0.011, 0.013, 0.01, 0.01, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.01, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.013, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.012, 0.009, 0.012, 0.012, 0.012, 0.01, 0.012, 0.01, 0.01, 0.011, 0.014, 0.012, 0.011, 0.011, 0.014, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.013, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 7043 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:18] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:22:18] #> Got bucket_cutoffs = tensor([-7.4085e-03,  5.5323e-06,  7.3854e-03]) and bucket_weights = tensor([-0.0161, -0.0029,  0.0029,  0.0161])\n",
      "[Feb 02, 22:22:18] avg_residual = 0.011337448842823505\n",
      "[Feb 02, 22:22:18] [0] \t\t #> Encoding 790 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 1/13 [00:00<00:05,  2.11it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:00<00:05,  2.16it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:01<00:04,  2.18it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:01<00:04,  2.18it/s]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:02<00:03,  2.17it/s]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:02<00:03,  2.16it/s]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:03<00:02,  2.18it/s]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:03<00:02,  2.19it/s]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:04<00:01,  2.18it/s]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:04<00:01,  2.17it/s]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:05<00:00,  2.18it/s]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:05<00:00,  2.17it/s]\u001b[A\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:24] [0] \t\t #> Saving chunk 0: \t 790 passages and 7,413 embeddings. From #0 onward.\n",
      "[Feb 02, 22:22:24] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:22:24] [0] \t\t Found all files!\n",
      "[Feb 02, 22:22:24] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:22:24] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:22:24] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:22:24] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:22:24] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:22:24] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:22:24] len(emb2pid) = 7413\n",
      "[Feb 02, 22:22:24] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_4302_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:22:24] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_4302_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.79s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2166.48it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 163636.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "[Feb 02, 22:22:24] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:22:24] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2815_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/2815_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2815_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:22:28] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:28] [0] \t\t # of sampled PIDs = 2577 \t sampled_pids[:3] = [1706, 41, 1223]\n",
      "[Feb 02, 22:22:28] [0] \t\t #> Encoding 2577 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:23<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:22:52] [0] \t\t avg_doclen_est = 10.490104675292969 \t len(local_sample) = 2,577\n",
      "[Feb 02, 22:22:52] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 02, 22:22:52] [0] \t\t *Estimated* 27,032 embeddings.\n",
      "[Feb 02, 22:22:52] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2815_2bits/plan.json ..\n",
      "Clustering 25682 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 25682 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (0.70 s, search 0.68 s): objective=5036.17 imbalance=1.456 nsplit=0       \n",
      "[0.014, 0.013, 0.017, 0.013, 0.016, 0.013, 0.013, 0.014, 0.014, 0.015, 0.013, 0.012, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.014, 0.015, 0.014, 0.012, 0.012, 0.013, 0.014, 0.014, 0.016, 0.014, 0.013, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.014, 0.014, 0.015, 0.013, 0.015, 0.012, 0.014, 0.013, 0.013, 0.012, 0.012, 0.012, 0.014, 0.015, 0.012, 0.013, 0.014, 0.016, 0.012, 0.012, 0.014, 0.013, 0.013, 0.016, 0.014, 0.013, 0.014, 0.016, 0.013, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.012, 0.016, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.017, 0.012, 0.013, 0.014, 0.014, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.012, 0.012, 0.015, 0.015, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.014, 0.015, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.011, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.015, 0.012, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.012, 0.012, 0.015, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.012, 0.015, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.013, 0.012, 0.013, 0.015, 0.012, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.016, 0.012, 0.013, 0.013, 0.013, 0.011, 0.013, 0.014, 0.011, 0.013, 0.013, 0.012, 0.011, 0.012, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.017, 0.011, 0.013, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.012, 0.011, 0.014, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.011, 0.013, 0.012, 0.013, 0.012, 0.013, 0.012, 0.012, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.014, 0.015, 0.014, 0.013, 0.012, 0.014, 0.014, 0.015, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.015, 0.013, 0.016, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.011, 0.014, 0.015, 0.015, 0.012, 0.013, 0.016, 0.012, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.015, 0.012, 0.013, 0.012, 0.011, 0.012, 0.014, 0.013, 0.013, 0.012, 0.014, 0.012, 0.012, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.014, 0.015, 0.013, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.011, 0.013, 0.013, 0.012, 0.013, 0.013, 0.015, 0.012, 0.014, 0.016, 0.011, 0.013, 0.013, 0.015, 0.015, 0.012, 0.015, 0.012, 0.012, 0.013, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.017, 0.013, 0.013, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.013, 0.011, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.015, 0.013, 0.015, 0.011, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.012, 0.012, 0.013, 0.014, 0.013, 0.014, 0.012, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.011, 0.013, 0.012, 0.013, 0.015, 0.013, 0.011, 0.014, 0.013, 0.014, 0.012, 0.012, 0.015, 0.013, 0.013, 0.013, 0.014, 0.014, 0.012, 0.014, 0.014, 0.013, 0.011, 0.013, 0.014, 0.014, 0.015, 0.011, 0.014, 0.011, 0.014, 0.013, 0.013, 0.012, 0.015, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.012, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.015, 0.014, 0.012, 0.015, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.013, 0.012, 0.011, 0.014, 0.013, 0.012, 0.011, 0.013, 0.015, 0.013, 0.012, 0.013, 0.012, 0.013, 0.014, 0.014, 0.015, 0.011, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.014, 0.013, 0.011, 0.014, 0.013, 0.011, 0.014, 0.011, 0.013, 0.014, 0.012, 0.013, 0.015, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.015, 0.013, 0.014, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.011, 0.015, 0.014, 0.013, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.012, 0.015, 0.012, 0.014, 0.011, 0.014, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.012, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.01, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.012, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.014, 0.012, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.012, 0.014, 0.011, 0.013, 0.014, 0.014, 0.014, 0.012, 0.013, 0.012, 0.013]\n",
      "[Feb 02, 22:22:53] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:22:53] #> Got bucket_cutoffs = tensor([-9.7640e-03,  8.7796e-06,  9.7949e-03]) and bucket_weights = tensor([-0.0185, -0.0042,  0.0043,  0.0187])\n",
      "[Feb 02, 22:22:53] avg_residual = 0.013215147890150547\n",
      "[Feb 02, 22:22:53] [0] \t\t #> Encoding 2577 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/41 [00:00<00:23,  1.69it/s]\u001b[A\n",
      "  5%|▍         | 2/41 [00:01<00:22,  1.70it/s]\u001b[A\n",
      "  7%|▋         | 3/41 [00:01<00:22,  1.70it/s]\u001b[A\n",
      " 10%|▉         | 4/41 [00:02<00:22,  1.68it/s]\u001b[A\n",
      " 12%|█▏        | 5/41 [00:02<00:21,  1.68it/s]\u001b[A\n",
      " 15%|█▍        | 6/41 [00:03<00:20,  1.68it/s]\u001b[A\n",
      " 17%|█▋        | 7/41 [00:04<00:20,  1.69it/s]\u001b[A\n",
      " 20%|█▉        | 8/41 [00:04<00:19,  1.68it/s]\u001b[A\n",
      " 22%|██▏       | 9/41 [00:05<00:18,  1.68it/s]\u001b[A\n",
      " 24%|██▍       | 10/41 [00:05<00:18,  1.68it/s]\u001b[A\n",
      " 27%|██▋       | 11/41 [00:06<00:17,  1.67it/s]\u001b[A\n",
      " 29%|██▉       | 12/41 [00:07<00:17,  1.67it/s]\u001b[A\n",
      " 32%|███▏      | 13/41 [00:07<00:16,  1.68it/s]\u001b[A\n",
      " 34%|███▍      | 14/41 [00:08<00:16,  1.67it/s]\u001b[A\n",
      " 37%|███▋      | 15/41 [00:08<00:15,  1.67it/s]\u001b[A\n",
      " 39%|███▉      | 16/41 [00:09<00:15,  1.66it/s]\u001b[A\n",
      " 41%|████▏     | 17/41 [00:10<00:14,  1.66it/s]\u001b[A\n",
      " 44%|████▍     | 18/41 [00:10<00:13,  1.65it/s]\u001b[A\n",
      " 46%|████▋     | 19/41 [00:11<00:13,  1.65it/s]\u001b[A\n",
      " 49%|████▉     | 20/41 [00:11<00:12,  1.65it/s]\u001b[A\n",
      " 51%|█████     | 21/41 [00:12<00:12,  1.65it/s]\u001b[A\n",
      " 54%|█████▎    | 22/41 [00:13<00:11,  1.65it/s]\u001b[A\n",
      " 56%|█████▌    | 23/41 [00:13<00:10,  1.65it/s]\u001b[A\n",
      " 59%|█████▊    | 24/41 [00:14<00:10,  1.64it/s]\u001b[A\n",
      " 61%|██████    | 25/41 [00:15<00:09,  1.64it/s]\u001b[A\n",
      " 63%|██████▎   | 26/41 [00:15<00:09,  1.65it/s]\u001b[A\n",
      " 66%|██████▌   | 27/41 [00:16<00:08,  1.65it/s]\u001b[A\n",
      " 68%|██████▊   | 28/41 [00:16<00:07,  1.65it/s]\u001b[A\n",
      " 71%|███████   | 29/41 [00:17<00:07,  1.66it/s]\u001b[A\n",
      " 73%|███████▎  | 30/41 [00:18<00:06,  1.66it/s]\u001b[A\n",
      " 76%|███████▌  | 31/41 [00:18<00:05,  1.67it/s]\u001b[A\n",
      " 78%|███████▊  | 32/41 [00:19<00:05,  1.67it/s]\u001b[A\n",
      " 80%|████████  | 33/41 [00:19<00:04,  1.68it/s]\u001b[A\n",
      " 83%|████████▎ | 34/41 [00:20<00:04,  1.68it/s]\u001b[A\n",
      " 85%|████████▌ | 35/41 [00:20<00:03,  1.68it/s]\u001b[A\n",
      " 88%|████████▊ | 36/41 [00:21<00:02,  1.68it/s]\u001b[A\n",
      " 90%|█████████ | 37/41 [00:22<00:02,  1.68it/s]\u001b[A\n",
      " 93%|█████████▎| 38/41 [00:22<00:01,  1.68it/s]\u001b[A\n",
      " 95%|█████████▌| 39/41 [00:23<00:01,  1.69it/s]\u001b[A\n",
      " 98%|█████████▊| 40/41 [00:23<00:00,  1.68it/s]\u001b[A\n",
      "100%|██████████| 41/41 [00:24<00:00,  1.70it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:18] [0] \t\t #> Saving chunk 0: \t 2,577 passages and 27,033 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:24, 24.66s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2265.97it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 146643.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:18] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:23:18] [0] \t\t Found all files!\n",
      "[Feb 02, 22:23:18] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:23:18] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:23:18] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:23:18] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:23:18] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:23:18] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:23:18] len(emb2pid) = 27033\n",
      "[Feb 02, 22:23:18] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2815_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:23:18] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2815_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:23:19] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:23:19] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3901_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3901_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3901_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:23:22] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:22] [0] \t\t # of sampled PIDs = 847 \t sampled_pids[:3] = [426, 750, 10]\n",
      "[Feb 02, 22:23:22] [0] \t\t #> Encoding 847 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.62it/s]\n",
      "WARNING clustering 8372 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:28] [0] \t\t avg_doclen_est = 10.403778076171875 \t len(local_sample) = 847\n",
      "[Feb 02, 22:23:28] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:23:28] [0] \t\t *Estimated* 8,812 embeddings.\n",
      "[Feb 02, 22:23:28] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3901_2bits/plan.json ..\n",
      "Clustering 8372 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.13 s, search 0.12 s): objective=1212.59 imbalance=1.478 nsplit=0       \n",
      "[0.013, 0.013, 0.012, 0.01, 0.013, 0.011, 0.013, 0.012, 0.013, 0.013, 0.011, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.01, 0.012, 0.012, 0.013, 0.012, 0.012, 0.011, 0.01, 0.01, 0.014, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.013, 0.012, 0.012, 0.014, 0.01, 0.011, 0.011, 0.01, 0.009, 0.012, 0.01, 0.013, 0.01, 0.011, 0.011, 0.012, 0.013, 0.011, 0.009, 0.012, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.01, 0.012, 0.01, 0.009, 0.011, 0.01, 0.01, 0.011, 0.013, 0.011, 0.013, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.01, 0.012, 0.013, 0.01, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.012, 0.01, 0.011, 0.01, 0.01, 0.011, 0.013, 0.012, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.013, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.013, 0.011, 0.01, 0.01, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.013, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.012, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.01, 0.011, 0.013, 0.01, 0.011, 0.01, 0.01, 0.011, 0.012, 0.011, 0.012, 0.014, 0.01, 0.012, 0.011, 0.011, 0.009, 0.01, 0.012, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.01, 0.012, 0.012, 0.012, 0.01, 0.011, 0.01, 0.012, 0.011, 0.012, 0.012, 0.011, 0.01, 0.015, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.013, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.011, 0.012, 0.014, 0.012, 0.012, 0.011, 0.013, 0.011, 0.014, 0.011, 0.012, 0.01, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.009, 0.012, 0.012, 0.013, 0.01, 0.011, 0.013, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.009, 0.011, 0.012, 0.01, 0.009, 0.012, 0.011, 0.011, 0.01, 0.013, 0.01, 0.01, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.013, 0.013, 0.011, 0.013, 0.012, 0.013, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.013, 0.01, 0.012, 0.014, 0.009, 0.014, 0.01, 0.013, 0.011, 0.011, 0.012, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.009, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.013, 0.011, 0.013, 0.009, 0.011, 0.01, 0.013, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.012, 0.012, 0.013, 0.01, 0.013, 0.01, 0.013, 0.009, 0.01, 0.011, 0.012, 0.012, 0.011, 0.012, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.011, 0.011, 0.012, 0.011, 0.013, 0.009, 0.01, 0.012, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.01, 0.011, 0.01, 0.01, 0.012, 0.012, 0.013, 0.011, 0.011, 0.01, 0.013, 0.011, 0.012, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.012, 0.013, 0.01, 0.012, 0.01, 0.011, 0.013, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.012, 0.009, 0.012, 0.012, 0.011, 0.012, 0.012, 0.01, 0.012, 0.013, 0.012, 0.01, 0.013, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.013, 0.011, 0.01, 0.011, 0.011, 0.011, 0.013, 0.011, 0.014, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.011, 0.012, 0.013, 0.011, 0.012, 0.012, 0.01, 0.012, 0.011, 0.012, 0.01, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.01, 0.011, 0.009, 0.012, 0.012, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.01, 0.011, 0.014, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.014, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.009, 0.011, 0.011, 0.013, 0.01, 0.012, 0.013, 0.011, 0.01, 0.012, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.013, 0.01, 0.01, 0.01, 0.011, 0.012, 0.01, 0.012, 0.013, 0.011, 0.01, 0.01, 0.012, 0.013, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.014, 0.01, 0.012, 0.011, 0.012, 0.009, 0.011, 0.012, 0.013, 0.012, 0.01, 0.011, 0.011, 0.011]\n",
      "[Feb 02, 22:23:28] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:23:28] #> Got bucket_cutoffs = tensor([-7.5208e-03, -1.3013e-05,  7.4679e-03]) and bucket_weights = tensor([-0.0159, -0.0031,  0.0030,  0.0157])\n",
      "[Feb 02, 22:23:28] avg_residual = 0.011194181628525257\n",
      "[Feb 02, 22:23:28] [0] \t\t #> Encoding 847 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/14 [00:00<00:05,  2.40it/s]\u001b[A\n",
      " 14%|█▍        | 2/14 [00:00<00:04,  2.45it/s]\u001b[A\n",
      " 21%|██▏       | 3/14 [00:01<00:04,  2.48it/s]\u001b[A\n",
      " 29%|██▊       | 4/14 [00:01<00:04,  2.49it/s]\u001b[A\n",
      " 36%|███▌      | 5/14 [00:02<00:03,  2.50it/s]\u001b[A\n",
      " 43%|████▎     | 6/14 [00:02<00:03,  2.50it/s]\u001b[A\n",
      " 50%|█████     | 7/14 [00:02<00:02,  2.50it/s]\u001b[A\n",
      " 57%|█████▋    | 8/14 [00:03<00:02,  2.50it/s]\u001b[A\n",
      " 64%|██████▍   | 9/14 [00:03<00:02,  2.49it/s]\u001b[A\n",
      " 71%|███████▏  | 10/14 [00:04<00:01,  2.49it/s]\u001b[A\n",
      " 79%|███████▊  | 11/14 [00:04<00:01,  2.48it/s]\u001b[A\n",
      " 86%|████████▌ | 12/14 [00:04<00:00,  2.48it/s]\u001b[A\n",
      " 93%|█████████▎| 13/14 [00:05<00:00,  2.48it/s]\u001b[A\n",
      "100%|██████████| 14/14 [00:05<00:00,  2.62it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:33] [0] \t\t #> Saving chunk 0: \t 847 passages and 8,812 embeddings. From #0 onward.\n",
      "[Feb 02, 22:23:33] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:23:33] [0] \t\t Found all files!\n",
      "[Feb 02, 22:23:33] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:23:33] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:23:33] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:23:33] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:23:33] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:23:33] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:23:33] len(emb2pid) = 8812\n",
      "[Feb 02, 22:23:33] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3901_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:23:33] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3901_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.46s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2125.85it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 152293.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "[Feb 02, 22:23:34] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:23:34] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3904_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3904_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3904_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:23:37] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:23:38] [0] \t\t # of sampled PIDs = 7399 \t sampled_pids[:3] = [3412, 6002, 83]\n",
      "[Feb 02, 22:23:38] [0] \t\t #> Encoding 7399 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:24:32] [0] \t\t avg_doclen_est = 8.691850662231445 \t len(local_sample) = 7,399\n",
      "[Feb 02, 22:24:32] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 02, 22:24:32] [0] \t\t *Estimated* 64,311 embeddings.\n",
      "[Feb 02, 22:24:32] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3904_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 61096 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 61096 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.04 s\n",
      "  Iteration 3 (1.57 s, search 1.54 s): objective=16036 imbalance=1.450 nsplit=0          \n",
      "[0.016, 0.015, 0.017, 0.014, 0.018, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.017, 0.015, 0.016, 0.015, 0.016, 0.014, 0.014, 0.014, 0.016, 0.016, 0.017, 0.015, 0.014, 0.014, 0.014, 0.014, 0.015, 0.014, 0.013, 0.017, 0.016, 0.015, 0.016, 0.016, 0.013, 0.016, 0.013, 0.014, 0.014, 0.014, 0.015, 0.017, 0.015, 0.015, 0.015, 0.016, 0.017, 0.014, 0.014, 0.016, 0.013, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.014, 0.015, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.017, 0.016, 0.015, 0.014, 0.013, 0.018, 0.015, 0.014, 0.015, 0.014, 0.014, 0.014, 0.018, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.017, 0.014, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.014, 0.017, 0.015, 0.016, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.017, 0.016, 0.016, 0.017, 0.014, 0.016, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.015, 0.017, 0.015, 0.016, 0.014, 0.014, 0.014, 0.015, 0.015, 0.014, 0.014, 0.017, 0.016, 0.015, 0.016, 0.014, 0.015, 0.015, 0.015, 0.013, 0.016, 0.016, 0.015, 0.016, 0.014, 0.014, 0.015, 0.017, 0.016, 0.015, 0.015, 0.017, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.017, 0.015, 0.016, 0.014, 0.015, 0.017, 0.014, 0.014, 0.017, 0.014, 0.014, 0.016, 0.014, 0.013, 0.016, 0.015, 0.016, 0.017, 0.014, 0.015, 0.014, 0.014, 0.013, 0.015, 0.016, 0.012, 0.015, 0.014, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.017, 0.014, 0.016, 0.016, 0.015, 0.02, 0.013, 0.014, 0.014, 0.013, 0.016, 0.015, 0.016, 0.016, 0.016, 0.016, 0.014, 0.013, 0.015, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.016, 0.013, 0.016, 0.014, 0.015, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.016, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.014, 0.014, 0.017, 0.015, 0.017, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.016, 0.016, 0.014, 0.014, 0.018, 0.013, 0.016, 0.013, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.013, 0.016, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.017, 0.014, 0.015, 0.014, 0.013, 0.013, 0.017, 0.014, 0.015, 0.014, 0.016, 0.014, 0.014, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.014, 0.016, 0.015, 0.016, 0.014, 0.016, 0.016, 0.017, 0.016, 0.014, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.017, 0.015, 0.016, 0.015, 0.016, 0.013, 0.013, 0.015, 0.015, 0.013, 0.014, 0.015, 0.017, 0.014, 0.015, 0.017, 0.013, 0.016, 0.014, 0.016, 0.016, 0.015, 0.017, 0.014, 0.014, 0.015, 0.015, 0.015, 0.016, 0.014, 0.014, 0.016, 0.017, 0.014, 0.016, 0.015, 0.014, 0.016, 0.013, 0.015, 0.016, 0.018, 0.015, 0.016, 0.015, 0.014, 0.015, 0.016, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.014, 0.016, 0.012, 0.015, 0.015, 0.016, 0.013, 0.014, 0.015, 0.015, 0.014, 0.015, 0.014, 0.013, 0.016, 0.014, 0.014, 0.015, 0.014, 0.016, 0.014, 0.017, 0.013, 0.016, 0.014, 0.015, 0.013, 0.015, 0.014, 0.015, 0.015, 0.014, 0.016, 0.017, 0.015, 0.014, 0.015, 0.016, 0.015, 0.017, 0.013, 0.015, 0.015, 0.014, 0.017, 0.014, 0.013, 0.016, 0.014, 0.015, 0.015, 0.014, 0.017, 0.015, 0.014, 0.015, 0.017, 0.016, 0.014, 0.017, 0.015, 0.014, 0.013, 0.015, 0.015, 0.016, 0.017, 0.015, 0.015, 0.014, 0.016, 0.016, 0.014, 0.013, 0.017, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.017, 0.016, 0.016, 0.015, 0.016, 0.015, 0.017, 0.014, 0.014, 0.017, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.017, 0.014, 0.015, 0.014, 0.015, 0.015, 0.014, 0.016, 0.014, 0.015, 0.016, 0.014, 0.016, 0.016, 0.014, 0.015, 0.017, 0.015, 0.013, 0.016, 0.014, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.017, 0.013, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.014, 0.012, 0.015, 0.013, 0.015, 0.016, 0.013, 0.014, 0.016, 0.016, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.015, 0.016, 0.015, 0.016, 0.015, 0.014, 0.014, 0.017, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.015, 0.013, 0.017, 0.015, 0.016, 0.014, 0.015, 0.016, 0.014, 0.017, 0.016, 0.013, 0.016, 0.014, 0.017, 0.012, 0.015, 0.015, 0.013, 0.014, 0.016, 0.017, 0.015, 0.014, 0.014, 0.014, 0.017, 0.014, 0.014, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.015, 0.018, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.016, 0.016, 0.016, 0.015, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.016, 0.017, 0.016, 0.015, 0.014, 0.015, 0.015, 0.014, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.017, 0.013, 0.014, 0.016, 0.014, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.016, 0.016, 0.016, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.016, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.016, 0.013, 0.016, 0.014, 0.015, 0.014, 0.016, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.017, 0.014, 0.016, 0.014, 0.016, 0.013, 0.016, 0.015, 0.016, 0.015, 0.014, 0.014, 0.014, 0.014]\n",
      "[Feb 02, 22:24:35] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:24:35] #> Got bucket_cutoffs = tensor([-1.1059e-02, -5.8170e-06,  1.0992e-02]) and bucket_weights = tensor([-0.0216, -0.0045,  0.0045,  0.0215])\n",
      "[Feb 02, 22:24:35] avg_residual = 0.014988665468990803\n",
      "[Feb 02, 22:24:35] [0] \t\t #> Encoding 7399 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:22,  2.15it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:21,  2.19it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:21,  2.20it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:20,  2.21it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:20,  2.21it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:19,  2.22it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:19,  2.22it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:18,  2.23it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:19,  2.13it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:18,  2.15it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:17,  2.17it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:17,  2.19it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:05<00:16,  2.20it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:16,  2.21it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:06<00:15,  2.21it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:15,  2.22it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:14,  2.23it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:14,  2.23it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:13,  2.22it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:13,  2.22it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:13,  2.21it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:09<00:12,  2.21it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:12,  2.21it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:10<00:11,  2.22it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:11<00:10,  2.22it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:12<00:10,  2.22it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:12<00:09,  2.22it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:14<00:08,  2.23it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:14<00:08,  2.24it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:14<00:07,  2.25it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:15<00:07,  2.26it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:15<00:06,  2.26it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:16<00:06,  2.26it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:16<00:05,  2.26it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:17<00:05,  2.25it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:17<00:04,  2.25it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:18<00:04,  2.26it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:18<00:03,  2.26it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:18<00:03,  2.27it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:19<00:03,  2.26it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:19<00:02,  2.21it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:20<00:02,  2.22it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:20<00:01,  2.22it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:21<00:01,  2.23it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:21<00:00,  2.22it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:22<00:00,  2.21it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.22it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:24,  2.03it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:23,  2.03it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:23,  2.02it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:22,  2.02it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:22,  2.01it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:21,  2.01it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:21,  2.00it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:20,  2.01it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:20,  2.02it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:19,  2.02it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:19,  2.02it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:18,  2.03it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:18,  2.03it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:17,  2.03it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:17,  2.02it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:16,  2.03it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:08<00:16,  2.03it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.04it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:09<00:15,  2.04it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:14,  2.04it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:10<00:14,  2.03it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:10<00:13,  2.03it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:11<00:13,  2.03it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:12,  2.03it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:12<00:12,  2.04it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:12<00:11,  2.04it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:13<00:11,  2.04it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:13<00:10,  2.03it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:14<00:10,  2.03it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:14<00:09,  2.03it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:15<00:09,  2.02it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:15<00:08,  2.03it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:16<00:08,  2.03it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:16<00:07,  2.03it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:17<00:07,  2.03it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:17<00:06,  2.03it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:18<00:06,  2.03it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:18<00:05,  2.02it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:19<00:05,  2.01it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:19<00:04,  2.02it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:20<00:04,  2.03it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:20<00:03,  2.03it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:21<00:03,  2.02it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:21<00:02,  2.02it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:22<00:02,  2.00it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:22<00:02,  1.99it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:23<00:01,  1.97it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:24<00:00,  1.97it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:24<00:00,  2.02it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:05,  2.58it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:05,  2.57it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:01<00:05,  2.55it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:04,  2.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:04,  2.55it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:02<00:03,  2.55it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:02<00:03,  2.56it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:03<00:03,  2.56it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:03<00:02,  2.56it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:03<00:02,  2.56it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:04<00:01,  2.56it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:04<00:01,  2.56it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:05<00:01,  2.56it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:05<00:00,  2.56it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:05<00:00,  2.56it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.62it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:25:28] [0] \t\t #> Saving chunk 0: \t 7,399 passages and 64,311 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:54, 54.70s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2058.05it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 128200.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:25:29] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:25:29] [0] \t\t Found all files!\n",
      "[Feb 02, 22:25:29] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:25:29] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:25:29] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:25:29] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:25:29] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:25:29] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:25:29] len(emb2pid) = 64311\n",
      "[Feb 02, 22:25:29] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3904_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:25:29] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3904_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:25:30] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:25:30] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2801_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/2801_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2801_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:25:33] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:25:34] [0] \t\t # of sampled PIDs = 9494 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Feb 02, 22:25:34] [0] \t\t #> Encoding 9494 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.96it/s]\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.20it/s]\n",
      "100%|██████████| 49/49 [00:23<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:26:46] [0] \t\t avg_doclen_est = 8.817779541015625 \t len(local_sample) = 9,494\n",
      "[Feb 02, 22:26:46] [0] \t\t Creaing 4,096 partitions.\n",
      "[Feb 02, 22:26:46] [0] \t\t *Estimated* 83,715 embeddings.\n",
      "[Feb 02, 22:26:46] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2801_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 79531 points to 4096 centroids: please provide at least 159744 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 79531 points in 768D to 4096 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 3 (3.96 s, search 3.90 s): objective=19720.9 imbalance=1.472 nsplit=0       \n",
      "[0.015, 0.015, 0.018, 0.014, 0.019, 0.015, 0.015, 0.016, 0.016, 0.017, 0.014, 0.014, 0.015, 0.014, 0.015, 0.015, 0.014, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.016, 0.016, 0.017, 0.014, 0.014, 0.014, 0.016, 0.016, 0.017, 0.015, 0.014, 0.015, 0.013, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.017, 0.017, 0.014, 0.015, 0.015, 0.017, 0.014, 0.013, 0.015, 0.013, 0.015, 0.016, 0.015, 0.015, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.013, 0.017, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.018, 0.014, 0.015, 0.014, 0.015, 0.013, 0.014, 0.016, 0.014, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.017, 0.017, 0.016, 0.017, 0.014, 0.016, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.016, 0.017, 0.015, 0.014, 0.016, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.016, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.014, 0.014, 0.016, 0.016, 0.015, 0.016, 0.014, 0.015, 0.015, 0.015, 0.013, 0.016, 0.016, 0.014, 0.016, 0.015, 0.014, 0.015, 0.016, 0.016, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.015, 0.013, 0.015, 0.016, 0.014, 0.014, 0.015, 0.014, 0.013, 0.015, 0.015, 0.015, 0.017, 0.014, 0.016, 0.015, 0.014, 0.013, 0.014, 0.017, 0.012, 0.015, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.019, 0.013, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.016, 0.016, 0.016, 0.014, 0.014, 0.015, 0.014, 0.015, 0.014, 0.016, 0.014, 0.015, 0.015, 0.016, 0.013, 0.015, 0.014, 0.015, 0.014, 0.013, 0.014, 0.013, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.014, 0.014, 0.016, 0.017, 0.015, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.015, 0.014, 0.014, 0.015, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.017, 0.015, 0.017, 0.015, 0.017, 0.015, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.016, 0.017, 0.014, 0.015, 0.018, 0.013, 0.016, 0.012, 0.014, 0.015, 0.013, 0.015, 0.016, 0.014, 0.014, 0.016, 0.015, 0.014, 0.013, 0.014, 0.013, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.017, 0.013, 0.015, 0.015, 0.013, 0.013, 0.017, 0.015, 0.014, 0.013, 0.016, 0.014, 0.014, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.015, 0.013, 0.016, 0.015, 0.016, 0.014, 0.015, 0.016, 0.016, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.014, 0.016, 0.015, 0.016, 0.016, 0.017, 0.015, 0.015, 0.015, 0.016, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.013, 0.015, 0.016, 0.013, 0.016, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.013, 0.014, 0.015, 0.015, 0.016, 0.014, 0.014, 0.016, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.017, 0.014, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.014, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.017, 0.014, 0.016, 0.012, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.014, 0.016, 0.014, 0.016, 0.013, 0.015, 0.013, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.014, 0.015, 0.015, 0.015, 0.013, 0.015, 0.015, 0.014, 0.016, 0.014, 0.013, 0.015, 0.015, 0.016, 0.014, 0.013, 0.017, 0.015, 0.014, 0.015, 0.017, 0.016, 0.015, 0.015, 0.015, 0.014, 0.013, 0.015, 0.014, 0.016, 0.017, 0.014, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.016, 0.015, 0.017, 0.015, 0.017, 0.014, 0.015, 0.016, 0.013, 0.015, 0.016, 0.015, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.014, 0.015, 0.017, 0.015, 0.013, 0.016, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.014, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.017, 0.013, 0.015, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.013, 0.016, 0.014, 0.012, 0.015, 0.013, 0.015, 0.016, 0.013, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.017, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.017, 0.014, 0.014, 0.013, 0.013, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.012, 0.016, 0.014, 0.015, 0.015, 0.015, 0.016, 0.014, 0.016, 0.016, 0.013, 0.016, 0.014, 0.016, 0.013, 0.015, 0.016, 0.013, 0.014, 0.016, 0.017, 0.015, 0.013, 0.014, 0.014, 0.016, 0.014, 0.014, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.014, 0.015, 0.014, 0.014, 0.017, 0.015, 0.015, 0.013, 0.015, 0.018, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.017, 0.013, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.015, 0.016, 0.016, 0.016, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.016, 0.013, 0.015, 0.014, 0.017, 0.013, 0.015, 0.016, 0.015, 0.014, 0.015, 0.015, 0.013, 0.016, 0.014, 0.015, 0.016, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.015, 0.013, 0.016, 0.014, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.015, 0.015, 0.015, 0.014, 0.014]\n",
      "[Feb 02, 22:26:51] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:26:51] #> Got bucket_cutoffs = tensor([-1.1020e-02,  6.5882e-06,  1.1029e-02]) and bucket_weights = tensor([-0.0213, -0.0047,  0.0047,  0.0213])\n",
      "[Feb 02, 22:26:51] avg_residual = 0.014890016056597233\n",
      "[Feb 02, 22:26:51] [0] \t\t #> Encoding 9494 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:25,  1.93it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:24,  1.96it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:23,  1.97it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:23,  1.98it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:22,  1.99it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:22,  1.99it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:21,  1.99it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:21,  1.99it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:20,  1.99it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:20,  1.98it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:19,  1.98it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:19,  1.99it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:18,  1.99it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:07<00:18,  1.99it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:17,  1.99it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:08<00:17,  1.99it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:09<00:16,  1.98it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:10<00:15,  1.98it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:10<00:14,  1.99it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:11<00:14,  1.99it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:11<00:13,  1.99it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:12<00:13,  1.99it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:12<00:12,  2.00it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:13<00:12,  1.99it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:13<00:11,  1.98it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:14<00:11,  1.98it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:14<00:10,  1.99it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:15<00:10,  1.99it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:15<00:09,  2.00it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:16<00:08,  2.01it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:16<00:08,  2.01it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:17<00:07,  2.01it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:17<00:07,  2.02it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:18<00:06,  2.01it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:18<00:06,  2.00it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:19<00:06,  2.00it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:19<00:05,  2.00it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:20<00:05,  1.99it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:20<00:04,  1.98it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:21<00:04,  1.98it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:22<00:02,  2.01it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:22<00:02,  2.01it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:23<00:01,  2.01it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:23<00:01,  2.01it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:24<00:00,  2.01it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:24<00:00,  1.99it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:21,  2.25it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:21,  2.25it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:20,  2.26it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:20,  2.26it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:19,  2.27it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:19,  2.27it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:18,  2.26it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:18,  2.27it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:03<00:18,  2.26it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:17,  2.26it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:04<00:17,  2.26it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:16,  2.27it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:05<00:16,  2.27it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:15,  2.27it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:06<00:15,  2.26it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:15,  2.26it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:14,  2.26it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:07<00:14,  2.26it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:13,  2.26it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:08<00:13,  2.26it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:12,  2.26it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:09<00:12,  2.27it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:11,  2.26it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:10<00:11,  2.26it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:11<00:11,  2.26it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:11<00:10,  2.26it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:11<00:10,  2.26it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:12<00:09,  2.26it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:12<00:09,  2.27it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:13<00:08,  2.27it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:13<00:08,  2.27it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:14<00:07,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:14<00:07,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:15<00:07,  2.27it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:15<00:06,  2.27it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:15<00:06,  2.26it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:16<00:05,  2.26it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:16<00:05,  2.26it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:17<00:04,  2.27it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:17<00:04,  2.27it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:18<00:03,  2.26it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:18<00:03,  2.22it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:19<00:03,  2.20it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:19<00:02,  2.22it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:19<00:02,  2.23it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:20<00:01,  2.23it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:20<00:01,  2.24it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:21<00:00,  2.24it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:21<00:00,  2.23it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.25it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/49 [00:00<00:22,  2.13it/s]\u001b[A\n",
      "  4%|▍         | 2/49 [00:00<00:22,  2.13it/s]\u001b[A\n",
      "  6%|▌         | 3/49 [00:01<00:21,  2.13it/s]\u001b[A\n",
      "  8%|▊         | 4/49 [00:01<00:21,  2.13it/s]\u001b[A\n",
      " 10%|█         | 5/49 [00:02<00:20,  2.14it/s]\u001b[A\n",
      " 12%|█▏        | 6/49 [00:02<00:20,  2.14it/s]\u001b[A\n",
      " 14%|█▍        | 7/49 [00:03<00:19,  2.14it/s]\u001b[A\n",
      " 16%|█▋        | 8/49 [00:03<00:19,  2.14it/s]\u001b[A\n",
      " 18%|█▊        | 9/49 [00:04<00:18,  2.14it/s]\u001b[A\n",
      " 20%|██        | 10/49 [00:04<00:18,  2.13it/s]\u001b[A\n",
      " 22%|██▏       | 11/49 [00:05<00:17,  2.13it/s]\u001b[A\n",
      " 24%|██▍       | 12/49 [00:05<00:17,  2.13it/s]\u001b[A\n",
      " 27%|██▋       | 13/49 [00:06<00:16,  2.13it/s]\u001b[A\n",
      " 29%|██▊       | 14/49 [00:06<00:16,  2.13it/s]\u001b[A\n",
      " 31%|███       | 15/49 [00:07<00:15,  2.14it/s]\u001b[A\n",
      " 33%|███▎      | 16/49 [00:07<00:15,  2.14it/s]\u001b[A\n",
      " 35%|███▍      | 17/49 [00:07<00:14,  2.14it/s]\u001b[A\n",
      " 37%|███▋      | 18/49 [00:08<00:14,  2.14it/s]\u001b[A\n",
      " 39%|███▉      | 19/49 [00:08<00:14,  2.14it/s]\u001b[A\n",
      " 41%|████      | 20/49 [00:09<00:13,  2.14it/s]\u001b[A\n",
      " 43%|████▎     | 21/49 [00:09<00:13,  2.14it/s]\u001b[A\n",
      " 45%|████▍     | 22/49 [00:10<00:12,  2.14it/s]\u001b[A\n",
      " 47%|████▋     | 23/49 [00:10<00:12,  2.14it/s]\u001b[A\n",
      " 49%|████▉     | 24/49 [00:11<00:11,  2.15it/s]\u001b[A\n",
      " 51%|█████     | 25/49 [00:11<00:11,  2.15it/s]\u001b[A\n",
      " 53%|█████▎    | 26/49 [00:12<00:10,  2.15it/s]\u001b[A\n",
      " 55%|█████▌    | 27/49 [00:12<00:10,  2.15it/s]\u001b[A\n",
      " 57%|█████▋    | 28/49 [00:13<00:09,  2.14it/s]\u001b[A\n",
      " 59%|█████▉    | 29/49 [00:13<00:09,  2.13it/s]\u001b[A\n",
      " 61%|██████    | 30/49 [00:14<00:08,  2.13it/s]\u001b[A\n",
      " 63%|██████▎   | 31/49 [00:14<00:08,  2.13it/s]\u001b[A\n",
      " 65%|██████▌   | 32/49 [00:14<00:07,  2.13it/s]\u001b[A\n",
      " 67%|██████▋   | 33/49 [00:15<00:07,  2.14it/s]\u001b[A\n",
      " 69%|██████▉   | 34/49 [00:15<00:06,  2.14it/s]\u001b[A\n",
      " 71%|███████▏  | 35/49 [00:16<00:06,  2.14it/s]\u001b[A\n",
      " 73%|███████▎  | 36/49 [00:16<00:06,  2.13it/s]\u001b[A\n",
      " 76%|███████▌  | 37/49 [00:17<00:05,  2.10it/s]\u001b[A\n",
      " 78%|███████▊  | 38/49 [00:17<00:05,  2.09it/s]\u001b[A\n",
      " 80%|███████▉  | 39/49 [00:18<00:04,  2.10it/s]\u001b[A\n",
      " 82%|████████▏ | 40/49 [00:18<00:04,  2.10it/s]\u001b[A\n",
      " 84%|████████▎ | 41/49 [00:19<00:03,  2.10it/s]\u001b[A\n",
      " 86%|████████▌ | 42/49 [00:19<00:03,  2.10it/s]\u001b[A\n",
      " 88%|████████▊ | 43/49 [00:20<00:02,  2.11it/s]\u001b[A\n",
      " 90%|████████▉ | 44/49 [00:20<00:02,  2.09it/s]\u001b[A\n",
      " 92%|█████████▏| 45/49 [00:21<00:01,  2.09it/s]\u001b[A\n",
      " 94%|█████████▍| 46/49 [00:21<00:01,  2.08it/s]\u001b[A\n",
      " 96%|█████████▌| 47/49 [00:22<00:00,  2.08it/s]\u001b[A\n",
      " 98%|█████████▊| 48/49 [00:22<00:00,  2.09it/s]\u001b[A\n",
      "100%|██████████| 49/49 [00:22<00:00,  2.15it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:02] [0] \t\t #> Saving chunk 0: \t 9,494 passages and 83,716 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:12, 72.65s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1896.16it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 145105.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:04] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:28:04] [0] \t\t Found all files!\n",
      "[Feb 02, 22:28:04] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:28:04] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:28:04] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:28:04] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:28:04] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:28:04] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:28:04] len(emb2pid) = 83716\n",
      "[Feb 02, 22:28:04] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2801_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:28:04] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2801_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:28:04] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:28:04] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3908_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3908_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3908_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:28:08] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:08] [0] \t\t # of sampled PIDs = 146 \t sampled_pids[:3] = [106, 2, 76]\n",
      "[Feb 02, 22:28:08] [0] \t\t #> Encoding 146 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.72it/s]\n",
      "WARNING clustering 1247 points to 512 centroids: please provide at least 19968 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:09] [0] \t\t avg_doclen_est = 8.98630142211914 \t len(local_sample) = 146\n",
      "[Feb 02, 22:28:09] [0] \t\t Creaing 512 partitions.\n",
      "[Feb 02, 22:28:09] [0] \t\t *Estimated* 1,312 embeddings.\n",
      "[Feb 02, 22:28:09] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3908_2bits/plan.json ..\n",
      "Clustering 1247 points in 768D to 512 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.01 s, search 0.01 s): objective=66.8126 imbalance=1.415 nsplit=0       \n",
      "[0.011, 0.008, 0.009, 0.01, 0.01, 0.011, 0.009, 0.009, 0.01, 0.011, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.008, 0.01, 0.012, 0.007, 0.007, 0.012, 0.01, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.01, 0.011, 0.011, 0.01, 0.01, 0.009, 0.01, 0.008, 0.01, 0.007, 0.008, 0.01, 0.01, 0.007, 0.007, 0.008, 0.011, 0.007, 0.007, 0.009, 0.007, 0.009, 0.01, 0.012, 0.011, 0.011, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.011, 0.009, 0.011, 0.01, 0.012, 0.008, 0.009, 0.011, 0.009, 0.009, 0.01, 0.007, 0.007, 0.011, 0.009, 0.009, 0.01, 0.008, 0.01, 0.01, 0.009, 0.011, 0.009, 0.007, 0.009, 0.009, 0.01, 0.011, 0.008, 0.007, 0.01, 0.008, 0.01, 0.01, 0.01, 0.01, 0.01, 0.008, 0.012, 0.012, 0.01, 0.008, 0.009, 0.012, 0.009, 0.01, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.01, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.011, 0.009, 0.009, 0.011, 0.009, 0.01, 0.008, 0.01, 0.007, 0.009, 0.008, 0.009, 0.011, 0.01, 0.01, 0.009, 0.013, 0.009, 0.01, 0.008, 0.01, 0.011, 0.01, 0.01, 0.01, 0.008, 0.009, 0.008, 0.009, 0.01, 0.011, 0.009, 0.012, 0.008, 0.009, 0.008, 0.01, 0.008, 0.01, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.011, 0.008, 0.009, 0.009, 0.009, 0.01, 0.009, 0.01, 0.01, 0.008, 0.01, 0.01, 0.008, 0.009, 0.009, 0.011, 0.01, 0.008, 0.011, 0.009, 0.009, 0.007, 0.006, 0.01, 0.01, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.009, 0.01, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.01, 0.008, 0.009, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.009, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.008, 0.008, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.007, 0.017, 0.007, 0.008, 0.009, 0.008, 0.011, 0.009, 0.008, 0.009, 0.013, 0.009, 0.008, 0.01, 0.007, 0.008, 0.009, 0.008, 0.009, 0.009, 0.011, 0.008, 0.01, 0.009, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.008, 0.008, 0.01, 0.009, 0.007, 0.008, 0.011, 0.007, 0.009, 0.009, 0.007, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.009, 0.007, 0.011, 0.011, 0.009, 0.009, 0.009, 0.008, 0.007, 0.007, 0.012, 0.011, 0.009, 0.008, 0.009, 0.009, 0.009, 0.01, 0.01, 0.008, 0.009, 0.01, 0.011, 0.011, 0.01, 0.011, 0.009, 0.011, 0.01, 0.012, 0.007, 0.009, 0.012, 0.008, 0.011, 0.008, 0.008, 0.008, 0.008, 0.01, 0.009, 0.009, 0.008, 0.01, 0.009, 0.008, 0.008, 0.01, 0.008, 0.009, 0.009, 0.009, 0.012, 0.008, 0.009, 0.01, 0.006, 0.008, 0.009, 0.008, 0.008, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.009, 0.007, 0.008, 0.007, 0.009, 0.011, 0.01, 0.011, 0.007, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.01, 0.009, 0.009, 0.009, 0.009, 0.01, 0.008, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.01, 0.01, 0.011, 0.01, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.011, 0.011, 0.009, 0.008, 0.01, 0.006, 0.01, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.007, 0.01, 0.011, 0.009, 0.008, 0.008, 0.007, 0.01, 0.009, 0.009, 0.011, 0.01, 0.009, 0.01, 0.008, 0.009, 0.01, 0.011, 0.009, 0.011, 0.007, 0.008, 0.009, 0.011, 0.009, 0.007, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.009, 0.008, 0.009, 0.007, 0.009, 0.007, 0.009, 0.01, 0.009, 0.008, 0.01, 0.01, 0.009, 0.013, 0.01, 0.012, 0.008, 0.01, 0.007, 0.009, 0.01, 0.008, 0.011, 0.01, 0.011, 0.009, 0.01, 0.008, 0.01, 0.007, 0.011, 0.01, 0.007, 0.008, 0.007, 0.009, 0.01, 0.008, 0.01, 0.01, 0.008, 0.009, 0.009, 0.01, 0.01, 0.008, 0.009, 0.009, 0.007, 0.008, 0.011, 0.006, 0.011, 0.009, 0.009, 0.009, 0.007, 0.01, 0.008, 0.009, 0.01, 0.007, 0.011, 0.009, 0.006, 0.009, 0.009, 0.008, 0.01, 0.012, 0.008, 0.009, 0.009, 0.012, 0.01, 0.008, 0.008, 0.01, 0.007, 0.008, 0.008, 0.01, 0.008, 0.01, 0.011, 0.01, 0.01, 0.008, 0.009, 0.01, 0.01, 0.011, 0.011, 0.009, 0.01, 0.008, 0.009, 0.008, 0.011, 0.011, 0.008, 0.008, 0.011, 0.009, 0.01, 0.008, 0.013, 0.007, 0.007, 0.009, 0.009, 0.008, 0.01, 0.009, 0.009, 0.008, 0.011, 0.009, 0.01, 0.008, 0.01, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.008, 0.009, 0.009, 0.009, 0.009, 0.01, 0.01, 0.007, 0.008, 0.008, 0.009, 0.011, 0.01, 0.01, 0.011, 0.008, 0.008, 0.011, 0.009, 0.01, 0.011, 0.009, 0.008, 0.01, 0.013, 0.01, 0.008, 0.008, 0.011, 0.009, 0.007, 0.011, 0.007, 0.01, 0.011, 0.008, 0.008, 0.01, 0.009, 0.008, 0.01, 0.008, 0.009, 0.01, 0.009, 0.008, 0.009, 0.01, 0.012, 0.008, 0.009, 0.007, 0.011, 0.01, 0.008, 0.009, 0.009, 0.009, 0.009, 0.009, 0.013, 0.007, 0.007, 0.011, 0.008, 0.009, 0.01, 0.008, 0.009, 0.01, 0.011, 0.013, 0.01, 0.013, 0.009, 0.01, 0.01, 0.007, 0.01, 0.01, 0.009, 0.008, 0.01, 0.01, 0.007, 0.007, 0.009, 0.01, 0.01, 0.008, 0.01, 0.008, 0.01, 0.01, 0.009, 0.009, 0.008, 0.009, 0.008, 0.007, 0.01, 0.008, 0.009, 0.009, 0.008, 0.01, 0.012, 0.009, 0.009, 0.008, 0.009, 0.01, 0.01, 0.008, 0.011, 0.008, 0.012, 0.012, 0.011, 0.008, 0.01, 0.008, 0.01, 0.011, 0.008, 0.009, 0.009, 0.013, 0.009, 0.007, 0.009, 0.008, 0.008, 0.011, 0.011, 0.01, 0.008, 0.011, 0.008, 0.009, 0.008, 0.009, 0.01, 0.007, 0.01, 0.007, 0.011, 0.01, 0.009, 0.008, 0.008, 0.009, 0.009, 0.011, 0.008, 0.011, 0.008, 0.01, 0.011, 0.013, 0.007, 0.007, 0.009, 0.009, 0.01, 0.009, 0.009, 0.01, 0.008, 0.01, 0.009, 0.009, 0.011, 0.01, 0.011, 0.01, 0.009, 0.009, 0.008, 0.009, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.01, 0.008, 0.01, 0.007, 0.009, 0.009, 0.008, 0.01]\n",
      "[Feb 02, 22:28:09] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:28:09] #> Got bucket_cutoffs = tensor([-5.7416e-03, -1.5033e-05,  5.8384e-03]) and bucket_weights = tensor([-0.0126, -0.0023,  0.0023,  0.0127])\n",
      "[Feb 02, 22:28:09] avg_residual = 0.009199653752148151\n",
      "[Feb 02, 22:28:09] [0] \t\t #> Encoding 146 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  2.35it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  2.38it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.09it/s]\u001b[A\n",
      "1it [00:00,  1.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2537.39it/s]\n",
      "100%|██████████| 512/512 [00:00<00:00, 158392.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:10] [0] \t\t #> Saving chunk 0: \t 146 passages and 1,312 embeddings. From #0 onward.\n",
      "[Feb 02, 22:28:10] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:28:10] [0] \t\t Found all files!\n",
      "[Feb 02, 22:28:10] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:28:10] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:28:10] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:28:10] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:28:10] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:28:10] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:28:10] len(emb2pid) = 1312\n",
      "[Feb 02, 22:28:10] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3908_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:28:10] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3908_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:28:11] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:28:11] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510401_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/510401_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_510401_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:28:14] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:15] [0] \t\t # of sampled PIDs = 3839 \t sampled_pids[:3] = [1706, 3001, 41]\n",
      "[Feb 02, 22:28:15] [0] \t\t #> Encoding 3839 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.70it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:28:52] [0] \t\t avg_doclen_est = 10.477989196777344 \t len(local_sample) = 3,839\n",
      "[Feb 02, 22:28:52] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 02, 22:28:52] [0] \t\t *Estimated* 40,225 embeddings.\n",
      "[Feb 02, 22:28:52] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510401_2bits/plan.json ..\n",
      "Clustering 38214 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 38214 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (1.02 s, search 0.99 s): objective=8413.87 imbalance=1.475 nsplit=0       \n",
      "[0.014, 0.013, 0.015, 0.013, 0.017, 0.013, 0.014, 0.014, 0.015, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.016, 0.013, 0.014, 0.014, 0.016, 0.013, 0.013, 0.012, 0.015, 0.015, 0.015, 0.013, 0.014, 0.015, 0.012, 0.012, 0.013, 0.013, 0.012, 0.016, 0.015, 0.015, 0.014, 0.016, 0.012, 0.013, 0.012, 0.013, 0.012, 0.014, 0.012, 0.016, 0.014, 0.013, 0.014, 0.014, 0.016, 0.012, 0.012, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.013, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.012, 0.012, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.016, 0.014, 0.014, 0.014, 0.014, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.014, 0.012, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.015, 0.014, 0.015, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.016, 0.014, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.012, 0.014, 0.015, 0.015, 0.014, 0.016, 0.014, 0.013, 0.016, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.016, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.014, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.014, 0.013, 0.013, 0.015, 0.013, 0.014, 0.015, 0.012, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.017, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.015, 0.011, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.013, 0.013, 0.013, 0.014, 0.015, 0.013, 0.015, 0.013, 0.014, 0.018, 0.011, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.015, 0.014, 0.015, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.014, 0.016, 0.012, 0.015, 0.014, 0.014, 0.012, 0.013, 0.014, 0.015, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.014, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.012, 0.014, 0.015, 0.016, 0.012, 0.013, 0.016, 0.012, 0.014, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.012, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.013, 0.012, 0.015, 0.012, 0.013, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.015, 0.014, 0.015, 0.013, 0.014, 0.015, 0.015, 0.014, 0.013, 0.013, 0.013, 0.015, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.016, 0.014, 0.015, 0.014, 0.013, 0.012, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.015, 0.012, 0.014, 0.016, 0.012, 0.015, 0.013, 0.015, 0.014, 0.014, 0.015, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.016, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.017, 0.013, 0.015, 0.012, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.012, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.015, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.012, 0.014, 0.014, 0.012, 0.014, 0.013, 0.015, 0.013, 0.013, 0.016, 0.014, 0.013, 0.015, 0.016, 0.015, 0.013, 0.014, 0.014, 0.013, 0.012, 0.014, 0.013, 0.015, 0.015, 0.012, 0.014, 0.012, 0.014, 0.014, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.015, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.013, 0.014, 0.013, 0.015, 0.015, 0.012, 0.013, 0.017, 0.014, 0.011, 0.015, 0.013, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.013, 0.016, 0.012, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.012, 0.014, 0.013, 0.012, 0.014, 0.012, 0.014, 0.015, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.015, 0.015, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.014, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.016, 0.013, 0.016, 0.014, 0.012, 0.014, 0.013, 0.015, 0.011, 0.014, 0.014, 0.012, 0.013, 0.015, 0.016, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.013, 0.014, 0.016, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.015, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.014, 0.014, 0.011, 0.013, 0.012, 0.016, 0.012, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.016, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.015, 0.015, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.016, 0.015, 0.013, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.011, 0.015, 0.015, 0.015, 0.013, 0.013, 0.014, 0.013, 0.013]\n",
      "[Feb 02, 22:28:53] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:28:53] #> Got bucket_cutoffs = tensor([-9.5907e-03, -2.2310e-05,  9.5237e-03]) and bucket_weights = tensor([-0.0195, -0.0038,  0.0038,  0.0195])\n",
      "[Feb 02, 22:28:53] avg_residual = 0.013620569370687008\n",
      "[Feb 02, 22:28:53] [0] \t\t #> Encoding 3839 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:29,  1.68it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:28,  1.71it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:27,  1.72it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:26,  1.72it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:26,  1.72it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:25,  1.72it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:04<00:25,  1.72it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:24,  1.72it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:05<00:23,  1.72it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:23,  1.72it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:06<00:22,  1.72it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:22,  1.71it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:07<00:21,  1.70it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:08<00:21,  1.71it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:20,  1.71it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:19,  1.72it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:10<00:18,  1.71it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:11<00:18,  1.71it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:11<00:17,  1.70it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:12<00:17,  1.70it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:12<00:16,  1.69it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:13<00:15,  1.70it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:14<00:15,  1.70it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:14<00:14,  1.69it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:15<00:14,  1.69it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:15<00:13,  1.68it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:16<00:13,  1.68it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:17<00:12,  1.68it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:17<00:11,  1.69it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:18<00:11,  1.69it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:18<00:10,  1.70it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:19<00:10,  1.70it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:19<00:09,  1.71it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:20<00:08,  1.71it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:21<00:08,  1.71it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:21<00:07,  1.70it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:22<00:07,  1.69it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:22<00:06,  1.68it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:23<00:05,  1.68it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:24<00:05,  1.68it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:24<00:04,  1.69it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:25<00:04,  1.69it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:25<00:03,  1.69it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:26<00:02,  1.69it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:27<00:02,  1.68it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:27<00:01,  1.68it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:28<00:01,  1.68it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:28<00:00,  1.69it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:29<00:00,  1.70it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.45it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.47it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:04,  1.47it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:04,  1.47it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.47it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.47it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.46it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.46it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.47it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:29:30] [0] \t\t #> Saving chunk 0: \t 3,839 passages and 40,225 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:37, 37.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2314.74it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 142010.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:29:30] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:29:30] [0] \t\t Found all files!\n",
      "[Feb 02, 22:29:30] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:29:30] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:29:30] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:29:30] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:29:30] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:29:30] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:29:30] len(emb2pid) = 40225\n",
      "[Feb 02, 22:29:30] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510401_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:29:30] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_510401_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:29:31] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:29:31] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2102_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/2102_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2102_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:29:34] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:29:35] [0] \t\t # of sampled PIDs = 11118 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Feb 02, 22:29:35] [0] \t\t #> Encoding 11118 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "100%|██████████| 50/50 [00:29<00:00,  1.68it/s]\n",
      "100%|██████████| 50/50 [00:24<00:00,  2.08it/s]\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:31:07] [0] \t\t avg_doclen_est = 9.072134971618652 \t len(local_sample) = 11,118\n",
      "[Feb 02, 22:31:07] [0] \t\t Creaing 4,096 partitions.\n",
      "[Feb 02, 22:31:07] [0] \t\t *Estimated* 100,863 embeddings.\n",
      "[Feb 02, 22:31:07] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2102_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 95821 points to 4096 centroids: please provide at least 159744 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 95821 points in 768D to 4096 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.06 s\n",
      "  Iteration 3 (4.80 s, search 4.74 s): objective=26311.4 imbalance=1.525 nsplit=0       \n",
      "[0.017, 0.016, 0.018, 0.015, 0.02, 0.015, 0.016, 0.017, 0.016, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.016, 0.016, 0.014, 0.018, 0.015, 0.016, 0.017, 0.017, 0.015, 0.014, 0.015, 0.017, 0.017, 0.017, 0.016, 0.015, 0.016, 0.014, 0.014, 0.015, 0.015, 0.014, 0.017, 0.017, 0.016, 0.016, 0.016, 0.014, 0.016, 0.014, 0.015, 0.015, 0.015, 0.014, 0.017, 0.016, 0.016, 0.015, 0.015, 0.019, 0.015, 0.014, 0.017, 0.014, 0.016, 0.017, 0.016, 0.016, 0.016, 0.017, 0.015, 0.016, 0.016, 0.014, 0.014, 0.015, 0.016, 0.016, 0.017, 0.017, 0.016, 0.015, 0.013, 0.018, 0.015, 0.014, 0.016, 0.015, 0.015, 0.014, 0.018, 0.016, 0.015, 0.016, 0.016, 0.014, 0.015, 0.017, 0.014, 0.016, 0.015, 0.015, 0.015, 0.016, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.016, 0.017, 0.015, 0.016, 0.016, 0.014, 0.014, 0.015, 0.018, 0.016, 0.016, 0.017, 0.014, 0.016, 0.015, 0.016, 0.014, 0.016, 0.015, 0.016, 0.017, 0.017, 0.017, 0.014, 0.016, 0.014, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.017, 0.015, 0.016, 0.015, 0.016, 0.016, 0.016, 0.014, 0.017, 0.016, 0.015, 0.017, 0.015, 0.015, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.016, 0.016, 0.017, 0.015, 0.016, 0.015, 0.017, 0.016, 0.016, 0.014, 0.015, 0.017, 0.014, 0.015, 0.017, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.016, 0.015, 0.015, 0.013, 0.016, 0.016, 0.012, 0.015, 0.015, 0.013, 0.014, 0.015, 0.015, 0.016, 0.014, 0.015, 0.017, 0.016, 0.016, 0.016, 0.016, 0.015, 0.014, 0.016, 0.015, 0.016, 0.017, 0.015, 0.016, 0.016, 0.015, 0.019, 0.013, 0.015, 0.015, 0.014, 0.016, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.017, 0.014, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.016, 0.016, 0.014, 0.015, 0.014, 0.016, 0.015, 0.015, 0.014, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.015, 0.017, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.017, 0.015, 0.017, 0.015, 0.018, 0.016, 0.015, 0.016, 0.017, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.015, 0.018, 0.013, 0.016, 0.013, 0.015, 0.015, 0.014, 0.016, 0.017, 0.014, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.016, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.015, 0.015, 0.013, 0.013, 0.017, 0.015, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.017, 0.016, 0.014, 0.016, 0.015, 0.016, 0.015, 0.016, 0.017, 0.017, 0.016, 0.015, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.016, 0.016, 0.017, 0.016, 0.016, 0.015, 0.017, 0.014, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.018, 0.015, 0.015, 0.018, 0.013, 0.016, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.014, 0.015, 0.016, 0.016, 0.016, 0.015, 0.015, 0.017, 0.017, 0.014, 0.017, 0.015, 0.015, 0.016, 0.014, 0.016, 0.016, 0.018, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.013, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.016, 0.012, 0.015, 0.015, 0.017, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.015, 0.015, 0.015, 0.016, 0.015, 0.017, 0.014, 0.017, 0.014, 0.016, 0.013, 0.016, 0.015, 0.016, 0.015, 0.016, 0.016, 0.017, 0.016, 0.015, 0.015, 0.016, 0.016, 0.016, 0.013, 0.016, 0.015, 0.015, 0.017, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.018, 0.015, 0.015, 0.015, 0.017, 0.017, 0.014, 0.016, 0.016, 0.014, 0.013, 0.015, 0.015, 0.016, 0.017, 0.014, 0.016, 0.014, 0.016, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.017, 0.016, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.016, 0.016, 0.017, 0.015, 0.017, 0.015, 0.015, 0.017, 0.015, 0.015, 0.016, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.015, 0.016, 0.017, 0.015, 0.017, 0.017, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.017, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.017, 0.013, 0.016, 0.015, 0.016, 0.016, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.016, 0.014, 0.017, 0.015, 0.013, 0.016, 0.013, 0.015, 0.017, 0.013, 0.015, 0.016, 0.016, 0.014, 0.015, 0.015, 0.014, 0.017, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.014, 0.014, 0.013, 0.015, 0.016, 0.016, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.016, 0.013, 0.017, 0.015, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.017, 0.014, 0.017, 0.015, 0.017, 0.013, 0.016, 0.016, 0.014, 0.016, 0.016, 0.017, 0.016, 0.014, 0.015, 0.015, 0.016, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.014, 0.016, 0.018, 0.015, 0.015, 0.015, 0.014, 0.016, 0.014, 0.017, 0.017, 0.014, 0.015, 0.016, 0.017, 0.016, 0.015, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.017, 0.016, 0.016, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.013, 0.016, 0.015, 0.017, 0.013, 0.016, 0.017, 0.014, 0.014, 0.015, 0.015, 0.014, 0.017, 0.015, 0.016, 0.016, 0.016, 0.014, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.017, 0.016, 0.016, 0.013, 0.017, 0.014, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.016, 0.017, 0.014, 0.017, 0.015, 0.017, 0.013, 0.016, 0.016, 0.017, 0.015, 0.014, 0.015, 0.014, 0.015]\n",
      "[Feb 02, 22:31:13] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:31:13] #> Got bucket_cutoffs = tensor([-1.1138e-02, -1.1083e-07,  1.1151e-02]) and bucket_weights = tensor([-0.0222, -0.0046,  0.0046,  0.0222])\n",
      "[Feb 02, 22:31:13] avg_residual = 0.015406397171318531\n",
      "[Feb 02, 22:31:13] [0] \t\t #> Encoding 11118 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:28,  1.72it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:26,  1.82it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:25,  1.85it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:24,  1.86it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:24,  1.87it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:23,  1.87it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:22,  1.88it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:22,  1.88it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:21,  1.89it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:21,  1.89it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:20,  1.89it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:20,  1.88it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:19,  1.88it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:07<00:19,  1.88it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:17,  1.88it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:09<00:16,  1.88it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:10<00:16,  1.89it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:10<00:15,  1.89it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:11<00:15,  1.88it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:11<00:14,  1.88it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:12<00:14,  1.88it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:12<00:13,  1.89it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:13<00:13,  1.87it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:13<00:12,  1.88it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:14<00:12,  1.88it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:14<00:11,  1.88it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:15<00:11,  1.87it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:15<00:10,  1.87it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:16<00:10,  1.87it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:17<00:09,  1.89it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:17<00:08,  1.89it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:18<00:08,  1.89it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:18<00:07,  1.89it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:19<00:07,  1.88it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:19<00:06,  1.88it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:20<00:06,  1.89it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:21<00:05,  1.90it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:21<00:04,  1.91it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:22<00:04,  1.91it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:22<00:03,  1.91it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:23<00:03,  1.91it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:23<00:02,  1.91it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:24<00:02,  1.92it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:24<00:01,  1.92it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:25<00:01,  1.93it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:25<00:00,  1.93it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.89it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:31,  1.56it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:28,  1.66it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:27,  1.70it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:26,  1.72it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:26,  1.73it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:25,  1.73it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:04<00:24,  1.73it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:24,  1.72it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:05<00:23,  1.74it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:23,  1.74it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:06<00:22,  1.74it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:22,  1.73it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:07<00:21,  1.72it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:08<00:21,  1.70it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:20,  1.70it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:09<00:19,  1.71it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:19,  1.72it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:10<00:18,  1.73it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:11<00:17,  1.74it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:11<00:17,  1.74it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:12<00:16,  1.74it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:12<00:16,  1.74it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:13<00:15,  1.74it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:13<00:14,  1.74it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:14<00:14,  1.74it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:15<00:13,  1.72it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:15<00:13,  1.72it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:16<00:12,  1.71it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:16<00:12,  1.71it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:17<00:11,  1.73it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:17<00:10,  1.73it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:18<00:10,  1.74it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:19<00:09,  1.73it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:19<00:09,  1.72it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:20<00:08,  1.71it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:20<00:08,  1.71it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:21<00:07,  1.72it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:22<00:06,  1.73it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:22<00:06,  1.74it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:23<00:05,  1.73it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:23<00:05,  1.73it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:24<00:04,  1.73it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:24<00:04,  1.73it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:25<00:03,  1.73it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:26<00:02,  1.73it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:26<00:02,  1.74it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:27<00:01,  1.74it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:27<00:01,  1.74it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:28<00:00,  1.74it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.73it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:24,  2.00it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:22,  2.09it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:21,  2.13it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:21,  2.13it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:20,  2.13it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:20,  2.13it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:19,  2.13it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:19,  2.14it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:18,  2.14it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:18,  2.13it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:17,  2.14it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:17,  2.13it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:16,  2.13it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:16,  2.13it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:16,  2.12it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:15,  2.12it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.13it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:14,  2.13it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:14,  2.13it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:13,  2.12it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:10<00:13,  2.12it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:12,  2.12it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:12,  2.12it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:11<00:11,  2.13it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:12<00:11,  2.14it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:12<00:10,  2.09it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:13<00:10,  2.10it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:13<00:09,  2.11it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:14<00:09,  2.10it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:15<00:08,  2.09it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:16<00:07,  2.10it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:16<00:06,  2.13it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:17<00:06,  2.13it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:17<00:05,  2.14it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:18<00:05,  2.14it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:18<00:04,  2.14it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:19<00:04,  2.13it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:19<00:03,  2.12it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:20<00:03,  2.12it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:20<00:02,  2.12it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:21<00:02,  2.12it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:21<00:01,  2.13it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:22<00:01,  2.13it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:22<00:00,  2.13it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:23<00:00,  2.12it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:23<00:00,  2.12it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/24 [00:00<00:10,  2.23it/s]\u001b[A\n",
      "  8%|▊         | 2/24 [00:00<00:09,  2.26it/s]\u001b[A\n",
      " 12%|█▎        | 3/24 [00:01<00:09,  2.27it/s]\u001b[A\n",
      " 17%|█▋        | 4/24 [00:01<00:08,  2.27it/s]\u001b[A\n",
      " 21%|██        | 5/24 [00:02<00:08,  2.27it/s]\u001b[A\n",
      " 25%|██▌       | 6/24 [00:02<00:07,  2.27it/s]\u001b[A\n",
      " 29%|██▉       | 7/24 [00:03<00:07,  2.26it/s]\u001b[A\n",
      " 33%|███▎      | 8/24 [00:03<00:07,  2.26it/s]\u001b[A\n",
      " 38%|███▊      | 9/24 [00:03<00:06,  2.26it/s]\u001b[A\n",
      " 42%|████▏     | 10/24 [00:04<00:06,  2.26it/s]\u001b[A\n",
      " 46%|████▌     | 11/24 [00:04<00:05,  2.27it/s]\u001b[A\n",
      " 50%|█████     | 12/24 [00:05<00:05,  2.27it/s]\u001b[A\n",
      " 54%|█████▍    | 13/24 [00:05<00:04,  2.26it/s]\u001b[A\n",
      " 58%|█████▊    | 14/24 [00:06<00:04,  2.26it/s]\u001b[A\n",
      " 62%|██████▎   | 15/24 [00:06<00:03,  2.25it/s]\u001b[A\n",
      " 67%|██████▋   | 16/24 [00:07<00:03,  2.25it/s]\u001b[A\n",
      " 71%|███████   | 17/24 [00:07<00:03,  2.06it/s]\u001b[A\n",
      " 75%|███████▌  | 18/24 [00:08<00:02,  2.12it/s]\u001b[A\n",
      " 79%|███████▉  | 19/24 [00:08<00:02,  2.15it/s]\u001b[A\n",
      " 83%|████████▎ | 20/24 [00:08<00:01,  2.19it/s]\u001b[A\n",
      " 88%|████████▊ | 21/24 [00:09<00:01,  2.21it/s]\u001b[A\n",
      " 92%|█████████▏| 22/24 [00:09<00:00,  2.23it/s]\u001b[A\n",
      " 96%|█████████▌| 23/24 [00:10<00:00,  2.23it/s]\u001b[A\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.26it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:32:43] [0] \t\t #> Saving chunk 0: \t 11,118 passages and 100,864 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:32, 92.78s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1757.14it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 138923.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:32:46] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:32:46] [0] \t\t Found all files!\n",
      "[Feb 02, 22:32:46] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:32:46] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:32:46] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:32:46] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:32:46] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:32:46] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:32:46] len(emb2pid) = 100864\n",
      "[Feb 02, 22:32:46] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2102_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:32:46] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_2102_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:32:47] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:32:47] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3903_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3903_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3903_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:32:50] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:32:51] [0] \t\t # of sampled PIDs = 1452 \t sampled_pids[:3] = [853, 20, 611]\n",
      "[Feb 02, 22:32:51] [0] \t\t #> Encoding 1452 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.64it/s]\n",
      "WARNING clustering 11542 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:32:59] [0] \t\t avg_doclen_est = 8.367079734802246 \t len(local_sample) = 1,452\n",
      "[Feb 02, 22:32:59] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:32:59] [0] \t\t *Estimated* 12,148 embeddings.\n",
      "[Feb 02, 22:32:59] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3903_2bits/plan.json ..\n",
      "Clustering 11542 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 3 (0.15 s, search 0.14 s): objective=2264.63 imbalance=1.477 nsplit=0       \n",
      "[0.015, 0.014, 0.015, 0.012, 0.016, 0.013, 0.014, 0.014, 0.015, 0.014, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.014, 0.015, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.012, 0.013, 0.013, 0.014, 0.015, 0.015, 0.014, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.012, 0.015, 0.012, 0.013, 0.015, 0.015, 0.012, 0.012, 0.013, 0.011, 0.012, 0.013, 0.013, 0.015, 0.013, 0.012, 0.014, 0.013, 0.016, 0.012, 0.012, 0.014, 0.011, 0.013, 0.013, 0.013, 0.013, 0.014, 0.015, 0.013, 0.013, 0.015, 0.013, 0.012, 0.013, 0.013, 0.015, 0.015, 0.014, 0.012, 0.012, 0.012, 0.016, 0.013, 0.013, 0.013, 0.012, 0.011, 0.012, 0.016, 0.014, 0.014, 0.013, 0.013, 0.012, 0.014, 0.015, 0.012, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.013, 0.015, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.015, 0.013, 0.015, 0.013, 0.013, 0.016, 0.012, 0.012, 0.012, 0.016, 0.015, 0.014, 0.015, 0.012, 0.015, 0.013, 0.013, 0.011, 0.013, 0.014, 0.013, 0.015, 0.013, 0.014, 0.012, 0.015, 0.014, 0.012, 0.012, 0.015, 0.014, 0.014, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.015, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.012, 0.013, 0.014, 0.015, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.01, 0.012, 0.015, 0.01, 0.014, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.015, 0.015, 0.013, 0.015, 0.014, 0.013, 0.016, 0.012, 0.014, 0.013, 0.011, 0.014, 0.012, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.011, 0.011, 0.014, 0.012, 0.014, 0.014, 0.012, 0.012, 0.011, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.014, 0.013, 0.013, 0.015, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.014, 0.016, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.015, 0.014, 0.015, 0.014, 0.015, 0.014, 0.012, 0.014, 0.015, 0.013, 0.012, 0.013, 0.015, 0.015, 0.012, 0.013, 0.015, 0.012, 0.015, 0.011, 0.013, 0.012, 0.012, 0.012, 0.015, 0.013, 0.011, 0.015, 0.013, 0.013, 0.011, 0.014, 0.011, 0.013, 0.014, 0.012, 0.015, 0.013, 0.014, 0.015, 0.012, 0.013, 0.012, 0.011, 0.011, 0.015, 0.013, 0.013, 0.012, 0.014, 0.012, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.011, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.011, 0.014, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.016, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.013, 0.016, 0.012, 0.013, 0.016, 0.012, 0.014, 0.012, 0.013, 0.015, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.015, 0.013, 0.012, 0.016, 0.015, 0.013, 0.015, 0.013, 0.012, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.013, 0.014, 0.015, 0.014, 0.013, 0.014, 0.011, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.014, 0.013, 0.015, 0.011, 0.013, 0.014, 0.016, 0.012, 0.012, 0.015, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.016, 0.012, 0.014, 0.012, 0.014, 0.011, 0.013, 0.012, 0.014, 0.014, 0.012, 0.015, 0.015, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.011, 0.013, 0.013, 0.013, 0.015, 0.012, 0.012, 0.014, 0.013, 0.014, 0.012, 0.013, 0.015, 0.012, 0.012, 0.013, 0.016, 0.013, 0.013, 0.013, 0.013, 0.012, 0.011, 0.013, 0.012, 0.015, 0.015, 0.013, 0.012, 0.012, 0.015, 0.013, 0.012, 0.012, 0.013, 0.013, 0.012, 0.013, 0.013, 0.015, 0.014, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.016, 0.013, 0.014, 0.015, 0.015, 0.013, 0.015, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.016, 0.012, 0.012, 0.015, 0.013, 0.014, 0.014, 0.012, 0.014, 0.016, 0.014, 0.01, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.014, 0.013, 0.016, 0.011, 0.013, 0.014, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.012, 0.014, 0.013, 0.011, 0.015, 0.012, 0.015, 0.014, 0.011, 0.012, 0.014, 0.016, 0.013, 0.013, 0.012, 0.012, 0.013, 0.015, 0.014, 0.013, 0.012, 0.014, 0.013, 0.013, 0.012, 0.015, 0.013, 0.013, 0.011, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.014, 0.011, 0.014, 0.012, 0.014, 0.013, 0.013, 0.015, 0.012, 0.015, 0.014, 0.012, 0.013, 0.012, 0.015, 0.011, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.012, 0.016, 0.012, 0.014, 0.012, 0.013, 0.015, 0.013, 0.012, 0.013, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.014, 0.016, 0.014, 0.014, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.014, 0.011, 0.013, 0.012, 0.016, 0.011, 0.014, 0.016, 0.012, 0.012, 0.012, 0.013, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.012, 0.015, 0.014, 0.014, 0.014, 0.013, 0.012, 0.015, 0.012, 0.012, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.014, 0.015, 0.013, 0.015, 0.013, 0.015, 0.011, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013]\n",
      "[Feb 02, 22:33:00] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:33:00] #> Got bucket_cutoffs = tensor([-8.9867e-03,  6.2184e-06,  8.9502e-03]) and bucket_weights = tensor([-0.0191, -0.0035,  0.0035,  0.0190])\n",
      "[Feb 02, 22:33:00] avg_residual = 0.013261500746011734\n",
      "[Feb 02, 22:33:00] [0] \t\t #> Encoding 1452 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/23 [00:00<00:12,  1.83it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:09,  2.23it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:01<00:08,  2.41it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:01<00:07,  2.50it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:02<00:07,  2.55it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:02<00:06,  2.58it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:02<00:06,  2.60it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:03<00:05,  2.62it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:03<00:05,  2.62it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:03<00:04,  2.63it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:04<00:04,  2.62it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:04<00:04,  2.62it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:05<00:03,  2.62it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:05<00:03,  2.62it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:05<00:03,  2.63it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:06<00:02,  2.63it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:06<00:02,  2.64it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:06<00:01,  2.64it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:07<00:01,  2.60it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:07<00:01,  2.61it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:08<00:00,  2.61it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:08<00:00,  2.59it/s]\u001b[A\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.61it/s]\u001b[A\n",
      "1it [00:08,  8.99s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2241.74it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 150806.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:09] [0] \t\t #> Saving chunk 0: \t 1,452 passages and 12,149 embeddings. From #0 onward.\n",
      "[Feb 02, 22:33:09] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:33:09] [0] \t\t Found all files!\n",
      "[Feb 02, 22:33:09] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:33:09] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:33:09] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:33:09] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:33:09] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:33:09] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:33:09] len(emb2pid) = 12149\n",
      "[Feb 02, 22:33:09] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3903_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:33:09] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3903_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:33:09] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:33:09] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3907_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/3907_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3907_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:33:13] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:13] [0] \t\t # of sampled PIDs = 498 \t sampled_pids[:3] = [213, 375, 5]\n",
      "[Feb 02, 22:33:13] [0] \t\t #> Encoding 498 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:03<00:00,  2.34it/s]\n",
      "WARNING clustering 4208 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:17] [0] \t\t avg_doclen_est = 8.893574714660645 \t len(local_sample) = 498\n",
      "[Feb 02, 22:33:17] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:33:17] [0] \t\t *Estimated* 4,429 embeddings.\n",
      "[Feb 02, 22:33:17] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3907_2bits/plan.json ..\n",
      "Clustering 4208 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.06 s, search 0.05 s): objective=361.489 imbalance=1.457 nsplit=0       \n",
      "[0.011, 0.011, 0.012, 0.01, 0.011, 0.009, 0.011, 0.012, 0.012, 0.012, 0.01, 0.01, 0.011, 0.009, 0.009, 0.009, 0.011, 0.01, 0.012, 0.011, 0.01, 0.012, 0.009, 0.01, 0.01, 0.011, 0.009, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.009, 0.011, 0.008, 0.009, 0.01, 0.01, 0.008, 0.012, 0.01, 0.011, 0.009, 0.011, 0.009, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.011, 0.01, 0.009, 0.012, 0.009, 0.013, 0.009, 0.008, 0.01, 0.008, 0.011, 0.012, 0.01, 0.009, 0.012, 0.011, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.009, 0.013, 0.012, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.01, 0.009, 0.008, 0.009, 0.012, 0.011, 0.012, 0.01, 0.01, 0.009, 0.011, 0.012, 0.01, 0.012, 0.012, 0.01, 0.009, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.009, 0.01, 0.01, 0.012, 0.01, 0.012, 0.009, 0.01, 0.012, 0.009, 0.011, 0.009, 0.014, 0.012, 0.01, 0.012, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.009, 0.01, 0.011, 0.011, 0.009, 0.011, 0.01, 0.011, 0.01, 0.01, 0.009, 0.011, 0.009, 0.01, 0.01, 0.01, 0.009, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.009, 0.01, 0.011, 0.011, 0.011, 0.009, 0.011, 0.01, 0.013, 0.01, 0.009, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.01, 0.013, 0.01, 0.01, 0.01, 0.012, 0.009, 0.009, 0.012, 0.009, 0.01, 0.011, 0.009, 0.009, 0.01, 0.011, 0.011, 0.012, 0.01, 0.011, 0.01, 0.011, 0.009, 0.009, 0.012, 0.008, 0.01, 0.01, 0.009, 0.009, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.012, 0.011, 0.012, 0.011, 0.012, 0.008, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.009, 0.01, 0.014, 0.009, 0.01, 0.011, 0.009, 0.012, 0.01, 0.012, 0.011, 0.011, 0.01, 0.01, 0.009, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.009, 0.008, 0.009, 0.009, 0.011, 0.01, 0.009, 0.01, 0.009, 0.01, 0.009, 0.009, 0.009, 0.011, 0.011, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.009, 0.009, 0.009, 0.01, 0.01, 0.012, 0.012, 0.014, 0.011, 0.011, 0.011, 0.009, 0.01, 0.012, 0.01, 0.009, 0.009, 0.011, 0.011, 0.009, 0.01, 0.013, 0.009, 0.01, 0.009, 0.01, 0.009, 0.01, 0.01, 0.011, 0.01, 0.009, 0.01, 0.01, 0.009, 0.009, 0.011, 0.01, 0.011, 0.013, 0.009, 0.011, 0.01, 0.01, 0.012, 0.009, 0.01, 0.01, 0.009, 0.008, 0.01, 0.01, 0.009, 0.01, 0.011, 0.008, 0.009, 0.01, 0.01, 0.01, 0.01, 0.009, 0.01, 0.01, 0.01, 0.012, 0.009, 0.011, 0.009, 0.011, 0.012, 0.011, 0.011, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.011, 0.012, 0.013, 0.01, 0.013, 0.012, 0.01, 0.009, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.009, 0.011, 0.013, 0.009, 0.011, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.012, 0.009, 0.009, 0.011, 0.01, 0.009, 0.014, 0.009, 0.009, 0.01, 0.01, 0.009, 0.012, 0.012, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.009, 0.011, 0.01, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.01, 0.01, 0.009, 0.011, 0.011, 0.009, 0.011, 0.01, 0.009, 0.01, 0.01, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.011, 0.009, 0.009, 0.011, 0.009, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.01, 0.011, 0.01, 0.011, 0.011, 0.01, 0.011, 0.009, 0.012, 0.011, 0.011, 0.009, 0.011, 0.01, 0.01, 0.011, 0.01, 0.009, 0.011, 0.01, 0.012, 0.009, 0.01, 0.012, 0.009, 0.009, 0.01, 0.011, 0.01, 0.01, 0.013, 0.01, 0.008, 0.008, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.009, 0.012, 0.01, 0.009, 0.009, 0.01, 0.01, 0.009, 0.011, 0.01, 0.011, 0.012, 0.008, 0.01, 0.01, 0.01, 0.009, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.009, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.009, 0.013, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.009, 0.01, 0.012, 0.011, 0.011, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.011, 0.012, 0.008, 0.011, 0.011, 0.01, 0.009, 0.011, 0.009, 0.008, 0.009, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.008, 0.009, 0.012, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.01, 0.012, 0.01, 0.009, 0.011, 0.009, 0.009, 0.011, 0.009, 0.011, 0.011, 0.008, 0.011, 0.009, 0.011, 0.01, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.009, 0.01, 0.009, 0.008, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.01, 0.012, 0.008, 0.011, 0.009, 0.01, 0.01, 0.011, 0.012, 0.011, 0.012, 0.011, 0.009, 0.011, 0.009, 0.013, 0.009, 0.011, 0.011, 0.009, 0.01, 0.012, 0.011, 0.01, 0.01, 0.011, 0.009, 0.012, 0.009, 0.01, 0.011, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.009, 0.008, 0.009, 0.014, 0.01, 0.011, 0.01, 0.01, 0.013, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.009, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.008, 0.011, 0.011, 0.011, 0.01, 0.011, 0.01, 0.009, 0.011, 0.01, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.01, 0.009, 0.011, 0.01, 0.01, 0.009, 0.01, 0.009, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.009, 0.012, 0.01, 0.01, 0.009, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.01, 0.009, 0.008, 0.01, 0.01, 0.008, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.009, 0.011, 0.009, 0.012, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011]\n",
      "[Feb 02, 22:33:17] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:33:17] #> Got bucket_cutoffs = tensor([-6.5632e-03, -2.0458e-05,  6.4871e-03]) and bucket_weights = tensor([-0.0144, -0.0025,  0.0024,  0.0144])\n",
      "[Feb 02, 22:33:17] avg_residual = 0.010243379510939121\n",
      "[Feb 02, 22:33:17] [0] \t\t #> Encoding 498 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:03,  2.18it/s]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:01<00:02,  2.27it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [00:01<00:01,  2.30it/s]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:02<00:01,  2.31it/s]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:02<00:00,  2.32it/s]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:03<00:00,  2.32it/s]\u001b[A\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.37it/s]\u001b[A\n",
      "1it [00:03,  3.44s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2145.42it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 167923.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:20] [0] \t\t #> Saving chunk 0: \t 498 passages and 4,429 embeddings. From #0 onward.\n",
      "[Feb 02, 22:33:20] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:33:20] [0] \t\t Found all files!\n",
      "[Feb 02, 22:33:20] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:33:20] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:33:20] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:33:20] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:33:20] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:33:20] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:33:20] len(emb2pid) = 4429\n",
      "[Feb 02, 22:33:20] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3907_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:33:20] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_3907_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 02, 22:33:21] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 02, 22:33:21] #> Creating directory /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_280801_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tmp_tutorial\\/280801_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_280801_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"tutorial\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/02\\/22.21.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 02, 22:33:24] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:25] [0] \t\t # of sampled PIDs = 1709 \t sampled_pids[:3] = [853, 1500, 20]\n",
      "[Feb 02, 22:33:25] [0] \t\t #> Encoding 1709 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:15<00:00,  1.80it/s]\n",
      "WARNING clustering 15386 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:40] [0] \t\t avg_doclen_est = 9.476302146911621 \t len(local_sample) = 1,709\n",
      "[Feb 02, 22:33:40] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 02, 22:33:40] [0] \t\t *Estimated* 16,195 embeddings.\n",
      "[Feb 02, 22:33:40] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_280801_2bits/plan.json ..\n",
      "Clustering 15386 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 3 (0.22 s, search 0.21 s): objective=3269.42 imbalance=1.504 nsplit=0       \n",
      "[0.015, 0.014, 0.015, 0.013, 0.016, 0.013, 0.013, 0.014, 0.013, 0.016, 0.012, 0.012, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.012, 0.014, 0.014, 0.015, 0.011, 0.011, 0.012, 0.014, 0.014, 0.015, 0.013, 0.012, 0.013, 0.011, 0.013, 0.012, 0.013, 0.012, 0.014, 0.014, 0.014, 0.014, 0.014, 0.011, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.012, 0.013, 0.016, 0.012, 0.011, 0.014, 0.012, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.012, 0.011, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.017, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.012, 0.011, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.014, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.015, 0.013, 0.015, 0.014, 0.013, 0.014, 0.012, 0.012, 0.013, 0.014, 0.014, 0.014, 0.015, 0.011, 0.013, 0.013, 0.013, 0.011, 0.015, 0.014, 0.013, 0.014, 0.015, 0.013, 0.012, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.011, 0.014, 0.014, 0.012, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.012, 0.013, 0.012, 0.011, 0.013, 0.013, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.011, 0.012, 0.013, 0.01, 0.013, 0.013, 0.011, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.015, 0.012, 0.015, 0.014, 0.013, 0.016, 0.01, 0.012, 0.012, 0.011, 0.014, 0.013, 0.013, 0.013, 0.015, 0.014, 0.013, 0.012, 0.013, 0.012, 0.013, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.011, 0.011, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.011, 0.014, 0.013, 0.013, 0.012, 0.014, 0.014, 0.014, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.012, 0.012, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.014, 0.015, 0.012, 0.013, 0.015, 0.012, 0.014, 0.011, 0.013, 0.013, 0.012, 0.012, 0.013, 0.012, 0.011, 0.014, 0.013, 0.012, 0.011, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.012, 0.014, 0.012, 0.013, 0.012, 0.011, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.011, 0.012, 0.013, 0.013, 0.013, 0.013, 0.011, 0.015, 0.013, 0.014, 0.012, 0.014, 0.013, 0.015, 0.012, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.014, 0.013, 0.012, 0.014, 0.013, 0.015, 0.013, 0.015, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.013, 0.015, 0.011, 0.014, 0.016, 0.011, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.012, 0.012, 0.014, 0.013, 0.012, 0.014, 0.014, 0.011, 0.013, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.012, 0.013, 0.016, 0.012, 0.014, 0.011, 0.012, 0.013, 0.014, 0.011, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.012, 0.012, 0.013, 0.013, 0.015, 0.012, 0.015, 0.011, 0.014, 0.012, 0.014, 0.011, 0.012, 0.013, 0.014, 0.013, 0.012, 0.014, 0.015, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.011, 0.013, 0.012, 0.013, 0.015, 0.011, 0.011, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.012, 0.013, 0.012, 0.015, 0.015, 0.012, 0.013, 0.014, 0.012, 0.011, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.014, 0.013, 0.012, 0.011, 0.014, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.012, 0.015, 0.012, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.011, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.011, 0.012, 0.012, 0.012, 0.014, 0.014, 0.013, 0.015, 0.011, 0.012, 0.012, 0.014, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.013, 0.011, 0.014, 0.013, 0.01, 0.012, 0.011, 0.013, 0.014, 0.011, 0.013, 0.014, 0.013, 0.012, 0.013, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.013, 0.014, 0.012, 0.012, 0.011, 0.011, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.012, 0.013, 0.011, 0.014, 0.012, 0.014, 0.013, 0.012, 0.015, 0.013, 0.015, 0.014, 0.012, 0.014, 0.013, 0.014, 0.011, 0.013, 0.014, 0.012, 0.012, 0.013, 0.015, 0.014, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.012, 0.016, 0.013, 0.013, 0.012, 0.012, 0.015, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.011, 0.012, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.015, 0.014, 0.011, 0.013, 0.011, 0.014, 0.011, 0.013, 0.015, 0.012, 0.012, 0.013, 0.013, 0.012, 0.015, 0.013, 0.013, 0.014, 0.015, 0.012, 0.012, 0.012, 0.012, 0.014, 0.012, 0.013, 0.014, 0.013, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.014, 0.012, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.016, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.01, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.013]\n",
      "[Feb 02, 22:33:40] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 02, 22:33:40] #> Got bucket_cutoffs = tensor([-8.5474e-03, -1.2748e-05,  8.5449e-03]) and bucket_weights = tensor([-0.0184, -0.0033,  0.0033,  0.0185])\n",
      "[Feb 02, 22:33:40] avg_residual = 0.012927062809467316\n",
      "[Feb 02, 22:33:40] [0] \t\t #> Encoding 1709 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 1/27 [00:00<00:15,  1.68it/s]\u001b[A\n",
      "  7%|▋         | 2/27 [00:01<00:14,  1.75it/s]\u001b[A\n",
      " 11%|█         | 3/27 [00:01<00:13,  1.78it/s]\u001b[A\n",
      " 15%|█▍        | 4/27 [00:02<00:12,  1.78it/s]\u001b[A\n",
      " 19%|█▊        | 5/27 [00:02<00:12,  1.79it/s]\u001b[A\n",
      " 22%|██▏       | 6/27 [00:03<00:11,  1.78it/s]\u001b[A\n",
      " 26%|██▌       | 7/27 [00:03<00:11,  1.78it/s]\u001b[A\n",
      " 30%|██▉       | 8/27 [00:04<00:10,  1.78it/s]\u001b[A\n",
      " 33%|███▎      | 9/27 [00:05<00:10,  1.77it/s]\u001b[A\n",
      " 37%|███▋      | 10/27 [00:05<00:09,  1.77it/s]\u001b[A\n",
      " 41%|████      | 11/27 [00:06<00:09,  1.76it/s]\u001b[A\n",
      " 44%|████▍     | 12/27 [00:06<00:08,  1.75it/s]\u001b[A\n",
      " 48%|████▊     | 13/27 [00:07<00:07,  1.76it/s]\u001b[A\n",
      " 52%|█████▏    | 14/27 [00:07<00:07,  1.76it/s]\u001b[A\n",
      " 56%|█████▌    | 15/27 [00:08<00:06,  1.76it/s]\u001b[A\n",
      " 59%|█████▉    | 16/27 [00:09<00:06,  1.76it/s]\u001b[A\n",
      " 63%|██████▎   | 17/27 [00:09<00:05,  1.75it/s]\u001b[A\n",
      " 67%|██████▋   | 18/27 [00:10<00:05,  1.75it/s]\u001b[A\n",
      " 70%|███████   | 19/27 [00:10<00:04,  1.76it/s]\u001b[A\n",
      " 74%|███████▍  | 20/27 [00:11<00:03,  1.75it/s]\u001b[A\n",
      " 78%|███████▊  | 21/27 [00:11<00:03,  1.75it/s]\u001b[A\n",
      " 81%|████████▏ | 22/27 [00:12<00:02,  1.75it/s]\u001b[A\n",
      " 85%|████████▌ | 23/27 [00:13<00:02,  1.75it/s]\u001b[A\n",
      " 89%|████████▉ | 24/27 [00:13<00:01,  1.75it/s]\u001b[A\n",
      " 93%|█████████▎| 25/27 [00:14<00:01,  1.75it/s]\u001b[A\n",
      " 96%|█████████▋| 26/27 [00:14<00:00,  1.73it/s]\u001b[A\n",
      "100%|██████████| 27/27 [00:15<00:00,  1.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 22:33:55] [0] \t\t #> Saving chunk 0: \t 1,709 passages and 16,195 embeddings. From #0 onward.\n",
      "[Feb 02, 22:33:56] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 02, 22:33:56] [0] \t\t Found all files!\n",
      "[Feb 02, 22:33:56] [0] \t\t #> Building IVF...\n",
      "[Feb 02, 22:33:56] [0] \t\t #> Loading codes...\n",
      "[Feb 02, 22:33:56] [0] \t\t Sorting codes...\n",
      "[Feb 02, 22:33:56] [0] \t\t Getting unique codes...\n",
      "[Feb 02, 22:33:56] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 02, 22:33:56] #> Building the emb2pid mapping..\n",
      "[Feb 02, 22:33:56] len(emb2pid) = 16195\n",
      "[Feb 02, 22:33:56] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_280801_2bits/ivf.pid.pt\n",
      "[Feb 02, 22:33:56] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/experiments/tutorial/indexes/models_280801_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.44s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2012.62it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 107151.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "def save_index(ckpt_pth, doc_maxlen, nbits, nranks, experiment, collection, index_name):\n",
    "    with Run().context(RunConfig(nranks=nranks, experiment=experiment)):\n",
    "        config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "        indexer = Indexer(checkpoint=ckpt_pth, config=config)\n",
    "        indexer.index(name=index_name, collection=collection, overwrite=True)\n",
    "    return indexer\n",
    "\n",
    "ckpt_pth = \"/home/sondors/Documents/ColBERT_weights/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives/none/2024-01/27/16.55.29/checkpoints/colbert-2998-finish\"\n",
    "experiment = \"tutorial\"\n",
    "\n",
    "doc_maxlen = 300\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "nranks = 1  # nranks specifies the number of GPUs to use.\n",
    "\n",
    "for cat_id in categories_id:\n",
    "\n",
    "    models_colbert = Collection(path=os.path.join(dst_fld,f\"{cat_id}_models.tsv\"))\n",
    "    index_name = f'models_{cat_id}_{nbits}bits'\n",
    "    indexer = save_index(ckpt_pth, doc_maxlen, nbits, nranks, experiment, models_colbert, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск матча по индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 23:06:16] #> Loading collection...\n",
      "0M \n",
      "[Feb 02, 23:06:17] #> Loading codec...\n",
      "[Feb 02, 23:06:17] #> Loading IVF...\n",
      "[Feb 02, 23:06:17] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "100%|██████████| 1/1 [00:00<00:00, 1781.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 02, 23:06:17] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 288.65it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment = \"tutorial\"\n",
    "\n",
    "doc_maxlen = 300\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "nranks = 1  # nranks specifies the number of GPUs to use.\n",
    "cat_id = 2801 # мобильные телефоны\n",
    "cat_id = 510401 # планшеты\n",
    "index_name = f'models_{cat_id}_{nbits}bits'\n",
    "models_colbert = Collection(path=os.path.join(dst_fld,f\"{cat_id}_models.tsv\"))\n",
    "\n",
    "with Run().context(RunConfig(experiment=experiment)):\n",
    "    searcher = Searcher(index=index_name, collection=models_colbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global), \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([   101,    100,  37077,    524,  19079, 105694,  37077,  29723,  14248,\n",
      "         10457,    156,  11396,    117,    129,    512,  18683,    120,  16196,\n",
      "           512,  18683,    117,  52742,    118,  36448,    116,  29494,  18062,\n",
      "           117,  10956,  67459,  19954,    102])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 328.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(3730, 1, 28.973285675048828),\n",
       "  (3729, 2, 24.965259552001953),\n",
       "  (2722, 3, 23.999181747436523),\n",
       "  (3731, 4, 23.97533416748047),\n",
       "  (2331, 5, 23.638072967529297)],\n",
       " 1: [(3730, 1, 28.152374267578125),\n",
       "  (3729, 2, 23.93535041809082),\n",
       "  (2722, 3, 23.20758819580078),\n",
       "  (1790, 4, 22.81966781616211),\n",
       "  (2331, 5, 22.648759841918945)],\n",
       " 2: [(3729, 1, 26.02865982055664),\n",
       "  (3730, 2, 20.31949234008789),\n",
       "  (3581, 3, 18.63184928894043),\n",
       "  (2722, 4, 18.34659194946289),\n",
       "  (3731, 5, 18.30866813659668)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offers = {\n",
    "    0: 'Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global)',\n",
    "    1: 'Планшет Samsung Galaxy Tab S8 128GB 5G Silver (SM-X706B)',\n",
    "    2: 'Планшет Samsung Galaxy Tab S8+ 128GB Wi-Fi Pink Gold (SM-X800)'\n",
    "    }\n",
    "\n",
    "offers = Queries(data=offers)\n",
    "rankings = searcher.search_all(offers, k=5).todict()\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "offer: Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global)\n",
      "\t [1] \t 29.0 \t Samsung Galaxy Tab S8\n",
      "\t [2] \t 25.0 \t Samsung Galaxy Tab S8+\n",
      "\t [3] \t 24.0 \t Samsung Galaxy Tab S2 9.7 SM-T813\n",
      "\t [4] \t 24.0 \t Samsung Galaxy Tab S8 Ultra\n",
      "\t [5] \t 23.6 \t Samsung Galaxy Tab S2 9.7 SM-T810\n",
      "\n",
      "offer: Планшет Samsung Galaxy Tab S8 128GB 5G Silver (SM-X706B)\n",
      "\t [1] \t 28.2 \t Samsung Galaxy Tab S8\n",
      "\t [2] \t 23.9 \t Samsung Galaxy Tab S8+\n",
      "\t [3] \t 23.2 \t Samsung Galaxy Tab S2 9.7 SM-T813\n",
      "\t [4] \t 22.8 \t Samsung Galaxy Tab S 8.4 SM-T705\n",
      "\t [5] \t 22.6 \t Samsung Galaxy Tab S2 9.7 SM-T810\n",
      "\n",
      "offer: Планшет Samsung Galaxy Tab S8+ 128GB Wi-Fi Pink Gold (SM-X800)\n",
      "\t [1] \t 26.0 \t Samsung Galaxy Tab S8+\n",
      "\t [2] \t 20.3 \t Samsung Galaxy Tab S8\n",
      "\t [3] \t 18.6 \t Samsung Galaxy Tab S7+ 12.4 128Gb\n",
      "\t [4] \t 18.3 \t Samsung Galaxy Tab S2 9.7 SM-T813\n",
      "\t [5] \t 18.3 \t Samsung Galaxy Tab S8 Ultra\n"
     ]
    }
   ],
   "source": [
    "for offer_index in rankings.keys():\n",
    "    print(f\"\\noffer: {offers[offer_index]}\")\n",
    "    for passage_id, passage_rank, passage_score in rankings[offer_index]:\n",
    "        print(f\"\\t [{passage_rank}] \\t {passage_score:.1f} \\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем выдачу топ N моделей для каждого оффера [переписать под colbert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global)\n",
      "\t5144478: Samsung Galaxy Tab S8 --> 0.89\n",
      "\t410416: Starway Andromeda S8 --> 0.78\n",
      "\t4509801: Samsung Galaxy Tab S7 11 128Gb --> 0.76\n",
      "____________________________________________________________\n",
      "Планшет Samsung Galaxy Tab S8 128GB 5G Silver (SM-X706B)\n",
      "\t5144478: Samsung Galaxy Tab S8 --> 0.89\n",
      "\t4509801: Samsung Galaxy Tab S7 11 128Gb --> 0.77\n",
      "\t410416: Starway Andromeda S8 --> 0.77\n",
      "____________________________________________________________\n",
      "Планшет Samsung Galaxy Tab S8+ 128GB Wi-Fi Pink Gold (SM-X800)\n",
      "\t5144477: Samsung Galaxy Tab S8+ --> 0.78\n",
      "\t631587: Haier G781-S --> 0.19\n",
      "\t4509801: Samsung Galaxy Tab S7 11 128Gb --> 0.15\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def top_n_similar(offer_embs: List[np.ndarray], model_embs: List[np.ndarray], model_ids: List[int], batch_size: int = 1000, n: int = 5) -> List[Dict[str, Union[List[int], np.ndarray]]]:\n",
    "    \"\"\"\n",
    "    Find the top N similar embeddings for each offer embedding.\n",
    "\n",
    "    Args:\n",
    "        offer_embs (List[np.ndarray]): List of offer embeddings.\n",
    "        model_embs (List[np.ndarray]): List of model embeddings.\n",
    "        model_ids (List[int]): List of model IDs corresponding to model embeddings.\n",
    "        batch_size (int, optional): Batch size for chunking the offer embeddings. Defaults to 1000.\n",
    "        n (int, optional): Number of top similar embeddings to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Union[List[int], np.ndarray]]]: A list of dictionaries containing model IDs and their \n",
    "        corresponding cosine similarity scores for the top N similar embeddings for each offer embedding.\n",
    "\n",
    "    Example:\n",
    "        >>> offer_embeddings = [np.array([0.1, 0.2, 0.3]), np.array([0.4, 0.5, 0.6])]\n",
    "        >>> model_embeddings = [np.array([0.2, 0.3, 0.4]), np.array([0.5, 0.6, 0.7])]\n",
    "        >>> model_ids = [123, 456]\n",
    "        >>> print(top_n_similar(offer_embeddings, model_embeddings, model_ids))\n",
    "            # Output: [{'model_ids': [123, 456], 'cosine_sims': array([0.99258333, 0.96832966])},\n",
    "            {'model_ids': [456, 123], 'cosine_sims': array([0.99964575, 0.99461155])}]\n",
    "    \"\"\"\n",
    "\n",
    "    cosine_sims = cosine_similarity_batch(offer_embs, model_embs, batch_size)\n",
    "    top_n_list = []\n",
    "    for i in range(len(offer_embs)):\n",
    "        # Find the indices of the top N similar embeddings\n",
    "        top_n_indices = np.argsort(cosine_sims[i])[::-1][:n]\n",
    "        # Get the corresponding model IDs and cosine similarity scores for the top N similar embeddings\n",
    "        top_n_model_ids = [model_ids[i] for i in top_n_indices]\n",
    "        top_n_cosine_sims = cosine_sims[i][top_n_indices]\n",
    "        top_n_list.append({\"model_ids\": top_n_model_ids, \"cosine_sims\": top_n_cosine_sims})\n",
    "    return top_n_list\n",
    "\n",
    "top_n = top_n_similar(offer_embs, model_embs, model_ids, batch_size = 1000, n=3)\n",
    "\n",
    "for i in range(len(top_n)):\n",
    "    print(offers[i])\n",
    "    for j in range(len(top_n[i]['model_ids'])):\n",
    "        id = top_n[i]['model_ids'][j]\n",
    "        sim = top_n[i]['cosine_sims'][j]\n",
    "        print(f\"\\t{id}: {models[model_ids.index(id)]} --> {round(float(sim), 2)}\")\n",
    "    print(\"_\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model_ids': [5144478, 410416, 4509801], 'cosine_sims': array([0.8934316, 0.7825217, 0.7603698], dtype=float32)}, {'model_ids': [5144478, 4509801, 410416], 'cosine_sims': array([0.88650525, 0.77400523, 0.76853645], dtype=float32)}, {'model_ids': [5144477, 631587, 4509801], 'cosine_sims': array([0.78279126, 0.18910483, 0.15151899], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "print(top_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
