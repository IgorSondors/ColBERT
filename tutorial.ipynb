{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты, параметры ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interface import prepare_tsv, save_index, top_n_similar, Collection\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ckpt_pth = \"/home/sondors/Documents/ColBERT_weights/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives/none/2024-01/27/16.55.29/checkpoints/colbert-2998-finish\"\n",
    "experiment = \"bert-base-multilingual-cased-2998\"\n",
    "\n",
    "doc_maxlen = 300\n",
    "nbits = 2   # bits определяет количество битов у каждого измерения в семантическом пространстве во время индексации\n",
    "nranks = 1  # nranks определяет количество GPU для использования, если они доступны\n",
    "kmeans_niters = 4 # kmeans_niters указывает количество итераций k-means кластеризации; 4 — хороший и быстрый вариант по умолчанию.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка моделей (на что матчить оффера - это может быть и смесь офферов и моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20366/17807022.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_models = pd.read_csv(pth_models, sep=\";\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>average_price</th>\n",
       "      <th>name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>category_name</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>920-005619</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>Logitech 920-005619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zipper Bag</td>\n",
       "      <td>Hama</td>\n",
       "      <td>Hama Zipper Bag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CC-3064</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>Nokia CC-3064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751488</td>\n",
       "      <td>990.0</td>\n",
       "      <td>CKS-X7/R</td>\n",
       "      <td>Sony</td>\n",
       "      <td>Sony CKS-X7/R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-031023</td>\n",
       "      <td>Era Pro</td>\n",
       "      <td>Era Pro EP-031023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехлы, обложки для гаджетов (телефонов, планше...</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103209</th>\n",
       "      <td>7049424</td>\n",
       "      <td>16459.0</td>\n",
       "      <td>MD-108</td>\n",
       "      <td>Mivo</td>\n",
       "      <td>Mivo MD-108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103210</th>\n",
       "      <td>7049425</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>MD-165</td>\n",
       "      <td>Mivo</td>\n",
       "      <td>Mivo MD-165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103211</th>\n",
       "      <td>7049426</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>Boost 20W</td>\n",
       "      <td>Rocket</td>\n",
       "      <td>Rocket Boost 20W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103212</th>\n",
       "      <td>7049427</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>Motion 10W</td>\n",
       "      <td>Rocket</td>\n",
       "      <td>Rocket Motion 10W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103213</th>\n",
       "      <td>7049428</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>Z1</td>\n",
       "      <td>SmartBuy</td>\n",
       "      <td>SmartBuy Z1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>портативная акустика</td>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103214 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_id  average_price        name brand_name            full_name  \\\n",
       "0         623742            NaN  920-005619   Logitech  Logitech 920-005619   \n",
       "1         721952            NaN  Zipper Bag       Hama      Hama Zipper Bag   \n",
       "2         721970            NaN     CC-3064      Nokia        Nokia CC-3064   \n",
       "3         751488          990.0    CKS-X7/R       Sony        Sony CKS-X7/R   \n",
       "4         751989            NaN   EP-031023    Era Pro    Era Pro EP-031023   \n",
       "...          ...            ...         ...        ...                  ...   \n",
       "103209   7049424        16459.0      MD-108       Mivo          Mivo MD-108   \n",
       "103210   7049425         8812.0      MD-165       Mivo          Mivo MD-165   \n",
       "103211   7049426         4240.0   Boost 20W     Rocket     Rocket Boost 20W   \n",
       "103212   7049427         2990.0  Motion 10W     Rocket    Rocket Motion 10W   \n",
       "103213   7049428         8900.0          Z1   SmartBuy          SmartBuy Z1   \n",
       "\n",
       "       comment                                      category_name  category_id  \n",
       "0          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "1          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "2          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "3          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "4          NaN  чехлы, обложки для гаджетов (телефонов, планше...         3994  \n",
       "...        ...                                                ...          ...  \n",
       "103209     NaN                               портативная акустика         3904  \n",
       "103210     NaN                               портативная акустика         3904  \n",
       "103211     NaN                               портативная акустика         3904  \n",
       "103212     NaN                               портативная акустика         3904  \n",
       "103213     NaN                               портативная акустика         3904  \n",
       "\n",
       "[103214 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_models = \"/home/sondors/Documents/price/ColBERT_data/18_categories/test/models_18_categories.csv\"\n",
    "df_models = pd.read_csv(pth_models, sep=\";\")\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_category = {\n",
    "    3902: 'диктофоны, портативные рекордеры',\n",
    "    510402: 'электронные книги',\n",
    "    4302: 'автомобильные телевизоры, мониторы',\n",
    "    2815: 'смарт-часы и браслеты',\n",
    "    3901: 'портативные медиаплееры',\n",
    "    3904: 'портативная акустика',\n",
    "    2801: 'мобильные телефоны',\n",
    "    3908: 'VR-гарнитуры (VR-очки, шлемы, очки виртуальной реальности, FPV очки для квадрокоптеров)',\n",
    "    510401: 'планшетные компьютеры и мини-планшеты',\n",
    "    2102: 'наушники, гарнитуры, наушники c микрофоном',\n",
    "    3903: 'радиоприемники, радиобудильники, радиочасы',\n",
    "    3907: 'магнитолы',\n",
    "    280801: 'GPS-навигаторы'\n",
    "    }\n",
    "\n",
    "dst_fld = \"/home/sondors/Documents/price/ColBERT/tutorial\"\n",
    "for cat_id in id_category.keys():\n",
    "    category_models = df_models[df_models.category_id == cat_id].reset_index(drop=True)\n",
    "    prepare_tsv(category_models, dst_fld, cat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индексируем модели и сохраняем индекс на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:07] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:36:07] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3902_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3902_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3902_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:36:10] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:11] [0] \t\t # of sampled PIDs = 488 \t sampled_pids[:3] = [213, 375, 5]\n",
      "[Feb 07, 16:36:11] [0] \t\t #> Encoding 488 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.50it/s]\n",
      "WARNING clustering 4691 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:16] [0] \t\t avg_doclen_est = 10.116803169250488 \t len(local_sample) = 488\n",
      "[Feb 07, 16:36:16] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:36:16] [0] \t\t *Estimated* 4,936 embeddings.\n",
      "[Feb 07, 16:36:16] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3902_2bits/plan.json ..\n",
      "Clustering 4691 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.07 s, search 0.06 s): objective=392.74 imbalance=1.505 nsplit=0        \n",
      "[0.009, 0.008, 0.01, 0.007, 0.011, 0.008, 0.01, 0.01, 0.009, 0.009, 0.008, 0.009, 0.008, 0.007, 0.008, 0.008, 0.01, 0.008, 0.01, 0.008, 0.009, 0.01, 0.009, 0.009, 0.008, 0.011, 0.008, 0.008, 0.007, 0.009, 0.01, 0.01, 0.008, 0.009, 0.009, 0.009, 0.008, 0.008, 0.009, 0.007, 0.011, 0.009, 0.009, 0.01, 0.01, 0.009, 0.008, 0.008, 0.007, 0.007, 0.009, 0.007, 0.009, 0.008, 0.008, 0.008, 0.008, 0.01, 0.009, 0.007, 0.009, 0.007, 0.009, 0.01, 0.008, 0.008, 0.008, 0.01, 0.007, 0.008, 0.009, 0.009, 0.008, 0.007, 0.008, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.009, 0.007, 0.008, 0.008, 0.007, 0.008, 0.01, 0.009, 0.009, 0.008, 0.008, 0.007, 0.009, 0.01, 0.007, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.007, 0.009, 0.009, 0.009, 0.008, 0.01, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.01, 0.01, 0.009, 0.008, 0.008, 0.009, 0.01, 0.008, 0.007, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.01, 0.008, 0.008, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.011, 0.009, 0.009, 0.009, 0.009, 0.007, 0.011, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.01, 0.009, 0.009, 0.009, 0.009, 0.008, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.01, 0.006, 0.01, 0.008, 0.007, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.009, 0.011, 0.008, 0.009, 0.008, 0.007, 0.011, 0.009, 0.009, 0.009, 0.009, 0.009, 0.007, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.01, 0.007, 0.009, 0.008, 0.008, 0.009, 0.006, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.007, 0.008, 0.009, 0.01, 0.008, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.01, 0.008, 0.01, 0.01, 0.008, 0.011, 0.008, 0.01, 0.008, 0.008, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.01, 0.007, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.007, 0.009, 0.007, 0.009, 0.009, 0.008, 0.008, 0.01, 0.009, 0.009, 0.008, 0.009, 0.009, 0.007, 0.007, 0.01, 0.008, 0.008, 0.007, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.009, 0.008, 0.011, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.007, 0.009, 0.008, 0.007, 0.009, 0.009, 0.01, 0.009, 0.012, 0.008, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.008, 0.009, 0.008, 0.008, 0.011, 0.008, 0.009, 0.01, 0.007, 0.01, 0.007, 0.011, 0.008, 0.008, 0.008, 0.009, 0.007, 0.008, 0.009, 0.009, 0.009, 0.009, 0.007, 0.01, 0.01, 0.007, 0.012, 0.008, 0.008, 0.01, 0.008, 0.009, 0.01, 0.011, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.008, 0.006, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.01, 0.008, 0.01, 0.008, 0.008, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.008, 0.009, 0.009, 0.009, 0.007, 0.01, 0.008, 0.01, 0.008, 0.008, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.008, 0.007, 0.008, 0.007, 0.008, 0.007, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.01, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.011, 0.01, 0.009, 0.008, 0.01, 0.008, 0.007, 0.009, 0.009, 0.009, 0.009, 0.007, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.008, 0.009, 0.01, 0.01, 0.007, 0.009, 0.008, 0.009, 0.008, 0.008, 0.011, 0.009, 0.009, 0.009, 0.01, 0.009, 0.009, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.009, 0.009, 0.011, 0.008, 0.008, 0.008, 0.009, 0.008, 0.007, 0.009, 0.008, 0.009, 0.01, 0.009, 0.009, 0.009, 0.007, 0.008, 0.01, 0.009, 0.008, 0.01, 0.01, 0.007, 0.009, 0.01, 0.007, 0.008, 0.009, 0.01, 0.008, 0.008, 0.008, 0.008, 0.009, 0.01, 0.009, 0.009, 0.007, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.009, 0.009, 0.008, 0.009, 0.009, 0.007, 0.008, 0.007, 0.009, 0.009, 0.007, 0.009, 0.009, 0.01, 0.009, 0.007, 0.008, 0.009, 0.009, 0.008, 0.009, 0.008, 0.01, 0.009, 0.009, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.007, 0.009, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.009, 0.008, 0.011, 0.007, 0.01, 0.009, 0.007, 0.009, 0.008, 0.01, 0.008, 0.009, 0.01, 0.008, 0.009, 0.007, 0.009, 0.009, 0.008, 0.007, 0.008, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.009, 0.009, 0.008, 0.008, 0.008, 0.008, 0.007, 0.008, 0.01, 0.009, 0.008, 0.009, 0.008, 0.009, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.01, 0.008, 0.007, 0.008, 0.007, 0.01, 0.01, 0.008, 0.009, 0.008, 0.007, 0.008, 0.008, 0.009, 0.009, 0.01, 0.008, 0.009, 0.01, 0.01, 0.009, 0.009, 0.009, 0.009, 0.01, 0.007, 0.008, 0.008, 0.009, 0.008, 0.009, 0.009, 0.007, 0.008, 0.008, 0.009, 0.009, 0.009, 0.008, 0.008, 0.009, 0.01, 0.007, 0.008, 0.008, 0.008, 0.01, 0.008, 0.009, 0.009, 0.009, 0.009, 0.008, 0.01, 0.01, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.009, 0.008, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.008, 0.008, 0.007, 0.009, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008]\n",
      "[Feb 07, 16:36:16] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:36:16] #> Got bucket_cutoffs = tensor([-5.3006e-03, -2.9246e-05,  5.1356e-03]) and bucket_weights = tensor([-0.0118, -0.0021,  0.0020,  0.0116])\n",
      "[Feb 07, 16:36:16] avg_residual = 0.008613561280071735\n",
      "[Feb 07, 16:36:16] [0] \t\t #> Encoding 488 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:01<00:04,  1.29it/s]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:02<00:03,  1.35it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [00:02<00:02,  1.63it/s]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:03<00:01,  1.83it/s]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:03<00:01,  1.98it/s]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:03<00:00,  2.10it/s]\u001b[A\n",
      "100%|██████████| 8/8 [00:04<00:00,  1.90it/s]\u001b[A\n",
      "1it [00:04,  4.28s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2757.60it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 164678.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:21] [0] \t\t #> Saving chunk 0: \t 488 passages and 4,937 embeddings. From #0 onward.\n",
      "[Feb 07, 16:36:21] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:36:21] [0] \t\t Found all files!\n",
      "[Feb 07, 16:36:21] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:36:21] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:36:21] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:36:21] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:36:21] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:36:21] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:36:21] len(emb2pid) = 4937\n",
      "[Feb 07, 16:36:21] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3902_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:36:21] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3902_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:36:21] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:36:21] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510402_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/510402_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_510402_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:36:25] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:25] [0] \t\t # of sampled PIDs = 680 \t sampled_pids[:3] = [426, 10, 305]\n",
      "[Feb 07, 16:36:25] [0] \t\t #> Encoding 680 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:08<00:00,  1.30it/s]\n",
      "WARNING clustering 6031 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:34] [0] \t\t avg_doclen_est = 9.335293769836426 \t len(local_sample) = 680\n",
      "[Feb 07, 16:36:34] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:36:34] [0] \t\t *Estimated* 6,347 embeddings.\n",
      "[Feb 07, 16:36:34] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510402_2bits/plan.json ..\n",
      "Clustering 6031 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 3 (0.15 s, search 0.15 s): objective=767.196 imbalance=1.570 nsplit=0       \n",
      "[0.012, 0.011, 0.012, 0.011, 0.014, 0.011, 0.012, 0.012, 0.012, 0.014, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.013, 0.012, 0.01, 0.011, 0.009, 0.01, 0.011, 0.01, 0.01, 0.013, 0.011, 0.011, 0.009, 0.012, 0.01, 0.011, 0.01, 0.009, 0.01, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.013, 0.01, 0.01, 0.012, 0.009, 0.01, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.009, 0.011, 0.012, 0.012, 0.01, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.012, 0.009, 0.011, 0.011, 0.014, 0.012, 0.012, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.01, 0.012, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.013, 0.011, 0.009, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.012, 0.01, 0.011, 0.01, 0.01, 0.012, 0.01, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.012, 0.011, 0.011, 0.013, 0.01, 0.01, 0.01, 0.009, 0.009, 0.009, 0.012, 0.009, 0.009, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.01, 0.014, 0.01, 0.01, 0.011, 0.01, 0.012, 0.011, 0.01, 0.013, 0.012, 0.012, 0.009, 0.01, 0.01, 0.01, 0.012, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.009, 0.01, 0.011, 0.011, 0.012, 0.012, 0.01, 0.01, 0.009, 0.012, 0.011, 0.01, 0.01, 0.012, 0.013, 0.012, 0.011, 0.012, 0.012, 0.01, 0.013, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.013, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.01, 0.012, 0.01, 0.01, 0.012, 0.012, 0.01, 0.01, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.009, 0.012, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.012, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.01, 0.01, 0.012, 0.009, 0.011, 0.012, 0.01, 0.013, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.011, 0.011, 0.01, 0.013, 0.01, 0.013, 0.01, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.013, 0.012, 0.012, 0.011, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.012, 0.01, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.013, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.014, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.009, 0.01, 0.011, 0.013, 0.009, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.01, 0.011, 0.01, 0.01, 0.011, 0.012, 0.012, 0.01, 0.013, 0.01, 0.011, 0.011, 0.011, 0.009, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.01, 0.011, 0.01, 0.009, 0.012, 0.01, 0.009, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.01, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.009, 0.011, 0.01, 0.012, 0.013, 0.009, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.009, 0.009, 0.01, 0.011, 0.012, 0.011, 0.012, 0.012, 0.01, 0.013, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.011, 0.011, 0.013, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.013, 0.01, 0.011, 0.013, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.011, 0.008, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.014, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.012, 0.011, 0.009, 0.012, 0.01, 0.009, 0.012, 0.009, 0.01, 0.012, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.009, 0.009, 0.01, 0.012, 0.012, 0.01, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.01, 0.012, 0.01, 0.013, 0.01, 0.011, 0.012, 0.01, 0.01, 0.012, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.009, 0.012, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.01, 0.012, 0.01, 0.01, 0.013, 0.01, 0.01, 0.012, 0.01, 0.011, 0.01, 0.011, 0.012, 0.009, 0.01, 0.011, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.01, 0.011, 0.01, 0.012, 0.009, 0.01, 0.012, 0.01, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.011, 0.009, 0.011, 0.01, 0.01, 0.012, 0.011, 0.012, 0.013, 0.012, 0.01, 0.01, 0.012, 0.014, 0.011, 0.011, 0.012, 0.009, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.011, 0.013, 0.01, 0.012, 0.01, 0.012, 0.01, 0.012, 0.011, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012]\n",
      "[Feb 07, 16:36:34] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:36:34] #> Got bucket_cutoffs = tensor([-7.3699e-03,  3.0249e-05,  7.3755e-03]) and bucket_weights = tensor([-0.0153, -0.0029,  0.0029,  0.0153])\n",
      "[Feb 07, 16:36:34] avg_residual = 0.010851368308067322\n",
      "[Feb 07, 16:36:34] [0] \t\t #> Encoding 680 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:09,  1.04it/s]\u001b[A\n",
      " 18%|█▊        | 2/11 [00:01<00:08,  1.04it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:02<00:07,  1.04it/s]\u001b[A\n",
      " 36%|███▋      | 4/11 [00:03<00:06,  1.04it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:04<00:05,  1.04it/s]\u001b[A\n",
      " 55%|█████▍    | 6/11 [00:05<00:04,  1.05it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:06<00:03,  1.10it/s]\u001b[A\n",
      " 73%|███████▎  | 8/11 [00:07<00:02,  1.22it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:07<00:01,  1.32it/s]\u001b[A\n",
      " 91%|█████████ | 10/11 [00:08<00:00,  1.37it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.24it/s]\u001b[A\n",
      "1it [00:08,  8.98s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2439.97it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 153814.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:43] [0] \t\t #> Saving chunk 0: \t 680 passages and 6,348 embeddings. From #0 onward.\n",
      "[Feb 07, 16:36:43] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:36:43] [0] \t\t Found all files!\n",
      "[Feb 07, 16:36:43] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:36:43] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:36:43] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:36:43] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:36:43] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:36:43] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:36:43] len(emb2pid) = 6348\n",
      "[Feb 07, 16:36:43] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510402_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:36:43] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510402_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:36:43] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:36:43] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_4302_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/4302_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_4302_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:36:47] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:47] [0] \t\t # of sampled PIDs = 790 \t sampled_pids[:3] = [426, 750, 10]\n",
      "[Feb 07, 16:36:47] [0] \t\t #> Encoding 790 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:54] [0] \t\t avg_doclen_est = 9.383543968200684 \t len(local_sample) = 790\n",
      "[Feb 07, 16:36:54] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:36:54] [0] \t\t *Estimated* 7,412 embeddings.\n",
      "[Feb 07, 16:36:54] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_4302_2bits/plan.json ..\n",
      "Clustering 7043 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.09 s, search 0.09 s): objective=962.952 imbalance=1.495 nsplit=0       \n",
      "[0.013, 0.012, 0.013, 0.01, 0.016, 0.01, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.014, 0.011, 0.012, 0.013, 0.013, 0.011, 0.011, 0.011, 0.012, 0.012, 0.013, 0.012, 0.012, 0.011, 0.012, 0.01, 0.011, 0.01, 0.01, 0.013, 0.012, 0.011, 0.011, 0.013, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.01, 0.013, 0.012, 0.011, 0.012, 0.011, 0.014, 0.01, 0.01, 0.012, 0.01, 0.011, 0.013, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.012, 0.012, 0.011, 0.011, 0.01, 0.012, 0.013, 0.012, 0.012, 0.011, 0.011, 0.013, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.014, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.015, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.011, 0.01, 0.012, 0.011, 0.012, 0.01, 0.012, 0.013, 0.01, 0.01, 0.011, 0.013, 0.013, 0.011, 0.013, 0.01, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.011, 0.012, 0.011, 0.011, 0.009, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.013, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.013, 0.01, 0.01, 0.012, 0.012, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.012, 0.009, 0.011, 0.011, 0.011, 0.011, 0.014, 0.01, 0.011, 0.011, 0.011, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.012, 0.009, 0.011, 0.01, 0.009, 0.01, 0.011, 0.009, 0.011, 0.01, 0.012, 0.012, 0.013, 0.011, 0.013, 0.013, 0.012, 0.011, 0.011, 0.012, 0.012, 0.013, 0.011, 0.012, 0.012, 0.011, 0.015, 0.01, 0.011, 0.011, 0.01, 0.012, 0.01, 0.012, 0.012, 0.013, 0.013, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.014, 0.01, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.013, 0.012, 0.01, 0.011, 0.01, 0.012, 0.01, 0.011, 0.01, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.009, 0.013, 0.012, 0.011, 0.009, 0.01, 0.011, 0.013, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.012, 0.014, 0.01, 0.011, 0.013, 0.01, 0.013, 0.01, 0.011, 0.011, 0.01, 0.011, 0.013, 0.011, 0.01, 0.012, 0.011, 0.012, 0.009, 0.011, 0.01, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.011, 0.013, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.012, 0.013, 0.013, 0.012, 0.014, 0.011, 0.012, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.012, 0.012, 0.01, 0.012, 0.014, 0.01, 0.012, 0.011, 0.013, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.013, 0.01, 0.011, 0.012, 0.012, 0.01, 0.015, 0.011, 0.012, 0.012, 0.01, 0.011, 0.012, 0.014, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.012, 0.011, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.013, 0.011, 0.012, 0.01, 0.01, 0.011, 0.013, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.011, 0.009, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.009, 0.011, 0.011, 0.009, 0.012, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.012, 0.013, 0.011, 0.011, 0.011, 0.013, 0.013, 0.011, 0.012, 0.013, 0.011, 0.01, 0.011, 0.012, 0.012, 0.013, 0.01, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.011, 0.013, 0.011, 0.013, 0.01, 0.012, 0.013, 0.01, 0.012, 0.011, 0.011, 0.011, 0.011, 0.014, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.013, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.009, 0.012, 0.012, 0.012, 0.01, 0.012, 0.013, 0.011, 0.011, 0.01, 0.011, 0.011, 0.012, 0.012, 0.012, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.013, 0.011, 0.009, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.011, 0.013, 0.011, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.011, 0.013, 0.01, 0.013, 0.01, 0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.011, 0.01, 0.011, 0.01, 0.012, 0.008, 0.012, 0.012, 0.011, 0.01, 0.013, 0.013, 0.012, 0.011, 0.01, 0.011, 0.012, 0.011, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.011, 0.013, 0.012, 0.012, 0.01, 0.011, 0.015, 0.011, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.012, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.013, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.013, 0.009, 0.011, 0.013, 0.01, 0.01, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.01, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.013, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.012, 0.009, 0.012, 0.012, 0.012, 0.01, 0.012, 0.01, 0.01, 0.011, 0.014, 0.012, 0.011, 0.011, 0.014, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.013, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011]\n",
      "[Feb 07, 16:36:54] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:36:54] #> Got bucket_cutoffs = tensor([-7.4085e-03,  5.5323e-06,  7.3854e-03]) and bucket_weights = tensor([-0.0161, -0.0029,  0.0029,  0.0161])\n",
      "[Feb 07, 16:36:54] avg_residual = 0.011337448842823505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 7043 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:54] [0] \t\t #> Encoding 790 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 1/13 [00:00<00:05,  2.27it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:00<00:04,  2.28it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:01<00:04,  2.28it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:01<00:03,  2.26it/s]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:02<00:03,  2.15it/s]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:02<00:03,  2.14it/s]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:03<00:02,  2.11it/s]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:03<00:02,  2.12it/s]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:04<00:01,  2.17it/s]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:04<00:01,  2.18it/s]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:05<00:00,  2.20it/s]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:05<00:00,  2.22it/s]\u001b[A\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:36:59] [0] \t\t #> Saving chunk 0: \t 790 passages and 7,413 embeddings. From #0 onward.\n",
      "[Feb 07, 16:37:00] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:37:00] [0] \t\t Found all files!\n",
      "[Feb 07, 16:37:00] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:37:00] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:37:00] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:37:00] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:37:00] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:37:00] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:37:00] len(emb2pid) = 7413\n",
      "[Feb 07, 16:37:00] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_4302_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:37:00] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_4302_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.74s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2384.48it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 163842.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "[Feb 07, 16:37:00] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:37:00] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2815_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/2815_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2815_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:37:03] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:37:04] [0] \t\t # of sampled PIDs = 2577 \t sampled_pids[:3] = [1706, 41, 1223]\n",
      "[Feb 07, 16:37:04] [0] \t\t #> Encoding 2577 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:25<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:37:29] [0] \t\t avg_doclen_est = 10.490104675292969 \t len(local_sample) = 2,577\n",
      "[Feb 07, 16:37:29] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 07, 16:37:29] [0] \t\t *Estimated* 27,032 embeddings.\n",
      "[Feb 07, 16:37:29] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2815_2bits/plan.json ..\n",
      "Clustering 25682 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 25682 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (0.65 s, search 0.63 s): objective=5036.17 imbalance=1.456 nsplit=0       \n",
      "[0.014, 0.013, 0.017, 0.013, 0.016, 0.013, 0.013, 0.014, 0.014, 0.015, 0.013, 0.012, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.014, 0.015, 0.014, 0.012, 0.012, 0.013, 0.014, 0.014, 0.016, 0.014, 0.013, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.014, 0.014, 0.015, 0.013, 0.015, 0.012, 0.014, 0.013, 0.013, 0.012, 0.012, 0.012, 0.014, 0.015, 0.012, 0.013, 0.014, 0.016, 0.012, 0.012, 0.014, 0.013, 0.013, 0.016, 0.014, 0.013, 0.014, 0.016, 0.013, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.012, 0.016, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.017, 0.012, 0.013, 0.014, 0.014, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.012, 0.012, 0.015, 0.015, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.014, 0.015, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.011, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.015, 0.012, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.012, 0.012, 0.015, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.012, 0.015, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.013, 0.012, 0.013, 0.015, 0.012, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.016, 0.012, 0.013, 0.013, 0.013, 0.011, 0.013, 0.014, 0.011, 0.013, 0.013, 0.012, 0.011, 0.012, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.017, 0.011, 0.013, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.012, 0.011, 0.014, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.011, 0.013, 0.012, 0.013, 0.012, 0.013, 0.012, 0.012, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.014, 0.015, 0.014, 0.013, 0.012, 0.014, 0.014, 0.015, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.015, 0.013, 0.016, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.013, 0.011, 0.014, 0.015, 0.015, 0.012, 0.013, 0.016, 0.012, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.015, 0.012, 0.013, 0.012, 0.011, 0.012, 0.014, 0.013, 0.013, 0.012, 0.014, 0.012, 0.012, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.014, 0.015, 0.013, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.011, 0.013, 0.013, 0.012, 0.013, 0.013, 0.015, 0.012, 0.014, 0.016, 0.011, 0.013, 0.013, 0.015, 0.015, 0.012, 0.015, 0.012, 0.012, 0.013, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.017, 0.013, 0.013, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.013, 0.011, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.015, 0.013, 0.015, 0.011, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.012, 0.012, 0.013, 0.014, 0.013, 0.014, 0.012, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.011, 0.013, 0.012, 0.013, 0.015, 0.013, 0.011, 0.014, 0.013, 0.014, 0.012, 0.012, 0.015, 0.013, 0.013, 0.013, 0.014, 0.014, 0.012, 0.014, 0.014, 0.013, 0.011, 0.013, 0.014, 0.014, 0.015, 0.011, 0.014, 0.011, 0.014, 0.013, 0.013, 0.012, 0.015, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.012, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.015, 0.014, 0.012, 0.015, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.013, 0.012, 0.011, 0.014, 0.013, 0.012, 0.011, 0.013, 0.015, 0.013, 0.012, 0.013, 0.012, 0.013, 0.014, 0.014, 0.015, 0.011, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.014, 0.013, 0.011, 0.014, 0.013, 0.011, 0.014, 0.011, 0.013, 0.014, 0.012, 0.013, 0.015, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.015, 0.013, 0.014, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.011, 0.015, 0.014, 0.013, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.012, 0.015, 0.012, 0.014, 0.011, 0.014, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.012, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.01, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.012, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.014, 0.012, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.012, 0.014, 0.011, 0.013, 0.014, 0.014, 0.014, 0.012, 0.013, 0.012, 0.013]\n",
      "[Feb 07, 16:37:30] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:37:30] #> Got bucket_cutoffs = tensor([-9.7640e-03,  8.7796e-06,  9.7949e-03]) and bucket_weights = tensor([-0.0185, -0.0042,  0.0043,  0.0187])\n",
      "[Feb 07, 16:37:30] avg_residual = 0.013215147890150547\n",
      "[Feb 07, 16:37:30] [0] \t\t #> Encoding 2577 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/41 [00:00<00:22,  1.75it/s]\u001b[A\n",
      "  5%|▍         | 2/41 [00:01<00:22,  1.76it/s]\u001b[A\n",
      "  7%|▋         | 3/41 [00:01<00:21,  1.78it/s]\u001b[A\n",
      " 10%|▉         | 4/41 [00:02<00:20,  1.78it/s]\u001b[A\n",
      " 12%|█▏        | 5/41 [00:02<00:20,  1.73it/s]\u001b[A\n",
      " 15%|█▍        | 6/41 [00:03<00:20,  1.74it/s]\u001b[A\n",
      " 17%|█▋        | 7/41 [00:04<00:20,  1.68it/s]\u001b[A\n",
      " 20%|█▉        | 8/41 [00:04<00:20,  1.62it/s]\u001b[A\n",
      " 22%|██▏       | 9/41 [00:05<00:20,  1.56it/s]\u001b[A\n",
      " 24%|██▍       | 10/41 [00:06<00:19,  1.60it/s]\u001b[A\n",
      " 27%|██▋       | 11/41 [00:06<00:18,  1.59it/s]\u001b[A\n",
      " 29%|██▉       | 12/41 [00:07<00:18,  1.61it/s]\u001b[A\n",
      " 32%|███▏      | 13/41 [00:07<00:17,  1.62it/s]\u001b[A\n",
      " 34%|███▍      | 14/41 [00:08<00:16,  1.61it/s]\u001b[A\n",
      " 37%|███▋      | 15/41 [00:09<00:15,  1.65it/s]\u001b[A\n",
      " 39%|███▉      | 16/41 [00:09<00:14,  1.67it/s]\u001b[A\n",
      " 41%|████▏     | 17/41 [00:10<00:14,  1.70it/s]\u001b[A\n",
      " 44%|████▍     | 18/41 [00:10<00:13,  1.71it/s]\u001b[A\n",
      " 46%|████▋     | 19/41 [00:11<00:13,  1.69it/s]\u001b[A\n",
      " 49%|████▉     | 20/41 [00:11<00:12,  1.70it/s]\u001b[A\n",
      " 51%|█████     | 21/41 [00:12<00:11,  1.67it/s]\u001b[A\n",
      " 54%|█████▎    | 22/41 [00:13<00:11,  1.61it/s]\u001b[A\n",
      " 56%|█████▌    | 23/41 [00:13<00:11,  1.60it/s]\u001b[A\n",
      " 59%|█████▊    | 24/41 [00:14<00:10,  1.64it/s]\u001b[A\n",
      " 61%|██████    | 25/41 [00:15<00:09,  1.61it/s]\u001b[A\n",
      " 63%|██████▎   | 26/41 [00:15<00:09,  1.62it/s]\u001b[A\n",
      " 66%|██████▌   | 27/41 [00:16<00:08,  1.61it/s]\u001b[A\n",
      " 68%|██████▊   | 28/41 [00:16<00:08,  1.60it/s]\u001b[A\n",
      " 71%|███████   | 29/41 [00:17<00:07,  1.59it/s]\u001b[A\n",
      " 73%|███████▎  | 30/41 [00:18<00:06,  1.59it/s]\u001b[A\n",
      " 76%|███████▌  | 31/41 [00:18<00:06,  1.59it/s]\u001b[A\n",
      " 78%|███████▊  | 32/41 [00:19<00:05,  1.60it/s]\u001b[A\n",
      " 80%|████████  | 33/41 [00:20<00:04,  1.60it/s]\u001b[A\n",
      " 83%|████████▎ | 34/41 [00:20<00:04,  1.61it/s]\u001b[A\n",
      " 85%|████████▌ | 35/41 [00:21<00:03,  1.60it/s]\u001b[A\n",
      " 88%|████████▊ | 36/41 [00:21<00:03,  1.60it/s]\u001b[A\n",
      " 90%|█████████ | 37/41 [00:22<00:02,  1.61it/s]\u001b[A\n",
      " 93%|█████████▎| 38/41 [00:23<00:01,  1.63it/s]\u001b[A\n",
      " 95%|█████████▌| 39/41 [00:23<00:01,  1.65it/s]\u001b[A\n",
      " 98%|█████████▊| 40/41 [00:24<00:00,  1.65it/s]\u001b[A\n",
      "100%|██████████| 41/41 [00:24<00:00,  1.67it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:37:55] [0] \t\t #> Saving chunk 0: \t 2,577 passages and 27,033 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:25, 25.14s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2111.94it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 112444.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:37:56] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:37:56] [0] \t\t Found all files!\n",
      "[Feb 07, 16:37:56] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:37:56] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:37:56] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:37:56] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:37:56] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:37:56] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:37:56] len(emb2pid) = 27033\n",
      "[Feb 07, 16:37:56] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2815_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:37:56] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2815_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:37:56] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:37:56] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3901_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3901_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3901_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:37:59] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:38:00] [0] \t\t # of sampled PIDs = 847 \t sampled_pids[:3] = [426, 750, 10]\n",
      "[Feb 07, 16:38:00] [0] \t\t #> Encoding 847 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:09<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:38:10] [0] \t\t avg_doclen_est = 10.403778076171875 \t len(local_sample) = 847\n",
      "[Feb 07, 16:38:10] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:38:10] [0] \t\t *Estimated* 8,812 embeddings.\n",
      "[Feb 07, 16:38:10] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3901_2bits/plan.json ..\n",
      "Clustering 8372 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 2 (0.15 s, search 0.15 s): objective=1270.15 imbalance=1.477 nsplit=0       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 8372 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (0.20 s, search 0.20 s): objective=1212.59 imbalance=1.478 nsplit=0       \n",
      "[0.013, 0.013, 0.012, 0.01, 0.013, 0.011, 0.013, 0.012, 0.013, 0.013, 0.011, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.01, 0.012, 0.012, 0.013, 0.012, 0.012, 0.011, 0.01, 0.01, 0.014, 0.012, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.012, 0.01, 0.011, 0.012, 0.013, 0.012, 0.012, 0.014, 0.01, 0.011, 0.011, 0.01, 0.009, 0.012, 0.01, 0.013, 0.01, 0.011, 0.011, 0.012, 0.013, 0.011, 0.009, 0.012, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.01, 0.012, 0.01, 0.009, 0.011, 0.01, 0.01, 0.011, 0.013, 0.011, 0.013, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.01, 0.012, 0.013, 0.01, 0.011, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.013, 0.012, 0.01, 0.011, 0.01, 0.01, 0.011, 0.013, 0.012, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.011, 0.012, 0.013, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.013, 0.011, 0.01, 0.01, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.013, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.012, 0.01, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.01, 0.011, 0.011, 0.01, 0.011, 0.013, 0.01, 0.011, 0.01, 0.01, 0.011, 0.012, 0.011, 0.012, 0.014, 0.01, 0.012, 0.011, 0.011, 0.009, 0.01, 0.012, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.01, 0.012, 0.012, 0.012, 0.01, 0.011, 0.01, 0.012, 0.011, 0.012, 0.012, 0.011, 0.01, 0.015, 0.01, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.013, 0.012, 0.011, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.01, 0.01, 0.012, 0.012, 0.01, 0.011, 0.011, 0.011, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.011, 0.012, 0.014, 0.012, 0.012, 0.011, 0.013, 0.011, 0.014, 0.011, 0.012, 0.01, 0.012, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.012, 0.011, 0.012, 0.011, 0.01, 0.009, 0.012, 0.012, 0.013, 0.01, 0.011, 0.013, 0.01, 0.012, 0.01, 0.01, 0.011, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.009, 0.011, 0.012, 0.01, 0.009, 0.012, 0.011, 0.011, 0.01, 0.013, 0.01, 0.01, 0.011, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.013, 0.012, 0.011, 0.011, 0.012, 0.01, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.013, 0.013, 0.011, 0.013, 0.012, 0.013, 0.01, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.013, 0.01, 0.012, 0.014, 0.009, 0.014, 0.01, 0.013, 0.011, 0.011, 0.012, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.009, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.013, 0.011, 0.013, 0.009, 0.011, 0.01, 0.013, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.012, 0.012, 0.013, 0.01, 0.013, 0.01, 0.013, 0.009, 0.01, 0.011, 0.012, 0.012, 0.011, 0.012, 0.012, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.01, 0.013, 0.011, 0.011, 0.012, 0.011, 0.013, 0.009, 0.01, 0.012, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.01, 0.011, 0.01, 0.01, 0.012, 0.012, 0.013, 0.011, 0.011, 0.01, 0.013, 0.011, 0.012, 0.01, 0.012, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.01, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.012, 0.013, 0.01, 0.012, 0.01, 0.011, 0.013, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.012, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.012, 0.009, 0.012, 0.012, 0.011, 0.012, 0.012, 0.01, 0.012, 0.013, 0.012, 0.01, 0.013, 0.011, 0.01, 0.01, 0.012, 0.011, 0.011, 0.011, 0.013, 0.011, 0.01, 0.011, 0.011, 0.011, 0.013, 0.011, 0.014, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.012, 0.01, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.01, 0.01, 0.011, 0.011, 0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.012, 0.011, 0.012, 0.011, 0.012, 0.013, 0.011, 0.012, 0.012, 0.01, 0.012, 0.011, 0.012, 0.01, 0.011, 0.012, 0.01, 0.011, 0.012, 0.012, 0.012, 0.01, 0.011, 0.009, 0.012, 0.012, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.01, 0.011, 0.014, 0.01, 0.012, 0.012, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.01, 0.012, 0.012, 0.011, 0.011, 0.011, 0.014, 0.011, 0.011, 0.01, 0.011, 0.012, 0.012, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.009, 0.011, 0.011, 0.013, 0.01, 0.012, 0.013, 0.011, 0.01, 0.012, 0.011, 0.012, 0.012, 0.012, 0.011, 0.013, 0.013, 0.01, 0.01, 0.01, 0.011, 0.012, 0.01, 0.012, 0.013, 0.011, 0.01, 0.01, 0.012, 0.013, 0.011, 0.011, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.01, 0.011, 0.012, 0.014, 0.01, 0.012, 0.011, 0.012, 0.009, 0.011, 0.012, 0.013, 0.012, 0.01, 0.011, 0.011, 0.011]\n",
      "[Feb 07, 16:38:10] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:38:10] #> Got bucket_cutoffs = tensor([-7.5208e-03, -1.3013e-05,  7.4679e-03]) and bucket_weights = tensor([-0.0159, -0.0031,  0.0030,  0.0157])\n",
      "[Feb 07, 16:38:10] avg_residual = 0.011194181628525257\n",
      "[Feb 07, 16:38:10] [0] \t\t #> Encoding 847 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/14 [00:00<00:09,  1.41it/s]\u001b[A\n",
      " 14%|█▍        | 2/14 [00:01<00:08,  1.41it/s]\u001b[A\n",
      " 21%|██▏       | 3/14 [00:02<00:07,  1.40it/s]\u001b[A\n",
      " 29%|██▊       | 4/14 [00:02<00:07,  1.40it/s]\u001b[A\n",
      " 36%|███▌      | 5/14 [00:03<00:06,  1.39it/s]\u001b[A\n",
      " 43%|████▎     | 6/14 [00:04<00:05,  1.39it/s]\u001b[A\n",
      " 50%|█████     | 7/14 [00:04<00:04,  1.41it/s]\u001b[A\n",
      " 57%|█████▋    | 8/14 [00:05<00:03,  1.64it/s]\u001b[A\n",
      " 64%|██████▍   | 9/14 [00:05<00:02,  1.85it/s]\u001b[A\n",
      " 71%|███████▏  | 10/14 [00:06<00:01,  2.02it/s]\u001b[A\n",
      " 79%|███████▊  | 11/14 [00:06<00:01,  2.15it/s]\u001b[A\n",
      " 86%|████████▌ | 12/14 [00:06<00:00,  2.23it/s]\u001b[A\n",
      " 93%|█████████▎| 13/14 [00:07<00:00,  2.22it/s]\u001b[A\n",
      "100%|██████████| 14/14 [00:07<00:00,  1.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:38:17] [0] \t\t #> Saving chunk 0: \t 847 passages and 8,812 embeddings. From #0 onward.\n",
      "[Feb 07, 16:38:18] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:38:18] [0] \t\t Found all files!\n",
      "[Feb 07, 16:38:18] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:38:18] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:38:18] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:38:18] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:38:18] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:38:18] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:38:18] len(emb2pid) = 8812\n",
      "[Feb 07, 16:38:18] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3901_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:38:18] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3901_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:07,  7.69s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1785.57it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 91353.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "[Feb 07, 16:38:18] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:38:18] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3904_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3904_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3904_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:38:22] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:38:22] [0] \t\t # of sampled PIDs = 7399 \t sampled_pids[:3] = [3412, 6002, 83]\n",
      "[Feb 07, 16:38:22] [0] \t\t #> Encoding 7399 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:24<00:00,  2.08it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.76it/s]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:39:22] [0] \t\t avg_doclen_est = 8.691850662231445 \t len(local_sample) = 7,399\n",
      "[Feb 07, 16:39:22] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 07, 16:39:22] [0] \t\t *Estimated* 64,311 embeddings.\n",
      "[Feb 07, 16:39:22] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3904_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 61096 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 61096 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.04 s\n",
      "  Iteration 3 (1.57 s, search 1.53 s): objective=16036 imbalance=1.450 nsplit=0          \n",
      "[0.016, 0.015, 0.017, 0.014, 0.018, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.017, 0.015, 0.016, 0.015, 0.016, 0.014, 0.014, 0.014, 0.016, 0.016, 0.017, 0.015, 0.014, 0.014, 0.014, 0.014, 0.015, 0.014, 0.013, 0.017, 0.016, 0.015, 0.016, 0.016, 0.013, 0.016, 0.013, 0.014, 0.014, 0.014, 0.015, 0.017, 0.015, 0.015, 0.015, 0.016, 0.017, 0.014, 0.014, 0.016, 0.013, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.014, 0.015, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.017, 0.016, 0.015, 0.014, 0.013, 0.018, 0.015, 0.014, 0.015, 0.014, 0.014, 0.014, 0.018, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.017, 0.014, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.014, 0.017, 0.015, 0.016, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.017, 0.016, 0.016, 0.017, 0.014, 0.016, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.015, 0.017, 0.015, 0.016, 0.014, 0.014, 0.014, 0.015, 0.015, 0.014, 0.014, 0.017, 0.016, 0.015, 0.016, 0.014, 0.015, 0.015, 0.015, 0.013, 0.016, 0.016, 0.015, 0.016, 0.014, 0.014, 0.015, 0.017, 0.016, 0.015, 0.015, 0.017, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.017, 0.015, 0.016, 0.014, 0.015, 0.017, 0.014, 0.014, 0.017, 0.014, 0.014, 0.016, 0.014, 0.013, 0.016, 0.015, 0.016, 0.017, 0.014, 0.015, 0.014, 0.014, 0.013, 0.015, 0.016, 0.012, 0.015, 0.014, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.017, 0.014, 0.016, 0.016, 0.015, 0.02, 0.013, 0.014, 0.014, 0.013, 0.016, 0.015, 0.016, 0.016, 0.016, 0.016, 0.014, 0.013, 0.015, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.016, 0.013, 0.016, 0.014, 0.015, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.016, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.014, 0.014, 0.017, 0.015, 0.017, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.016, 0.016, 0.014, 0.014, 0.018, 0.013, 0.016, 0.013, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.013, 0.016, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.017, 0.014, 0.015, 0.014, 0.013, 0.013, 0.017, 0.014, 0.015, 0.014, 0.016, 0.014, 0.014, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.014, 0.016, 0.015, 0.016, 0.014, 0.016, 0.016, 0.017, 0.016, 0.014, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.017, 0.015, 0.016, 0.015, 0.016, 0.013, 0.013, 0.015, 0.015, 0.013, 0.014, 0.015, 0.017, 0.014, 0.015, 0.017, 0.013, 0.016, 0.014, 0.016, 0.016, 0.015, 0.017, 0.014, 0.014, 0.015, 0.015, 0.015, 0.016, 0.014, 0.014, 0.016, 0.017, 0.014, 0.016, 0.015, 0.014, 0.016, 0.013, 0.015, 0.016, 0.018, 0.015, 0.016, 0.015, 0.014, 0.015, 0.016, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.014, 0.016, 0.012, 0.015, 0.015, 0.016, 0.013, 0.014, 0.015, 0.015, 0.014, 0.015, 0.014, 0.013, 0.016, 0.014, 0.014, 0.015, 0.014, 0.016, 0.014, 0.017, 0.013, 0.016, 0.014, 0.015, 0.013, 0.015, 0.014, 0.015, 0.015, 0.014, 0.016, 0.017, 0.015, 0.014, 0.015, 0.016, 0.015, 0.017, 0.013, 0.015, 0.015, 0.014, 0.017, 0.014, 0.013, 0.016, 0.014, 0.015, 0.015, 0.014, 0.017, 0.015, 0.014, 0.015, 0.017, 0.016, 0.014, 0.017, 0.015, 0.014, 0.013, 0.015, 0.015, 0.016, 0.017, 0.015, 0.015, 0.014, 0.016, 0.016, 0.014, 0.013, 0.017, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.017, 0.016, 0.016, 0.015, 0.016, 0.015, 0.017, 0.014, 0.014, 0.017, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.017, 0.014, 0.015, 0.014, 0.015, 0.015, 0.014, 0.016, 0.014, 0.015, 0.016, 0.014, 0.016, 0.016, 0.014, 0.015, 0.017, 0.015, 0.013, 0.016, 0.014, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.017, 0.013, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.014, 0.012, 0.015, 0.013, 0.015, 0.016, 0.013, 0.014, 0.016, 0.016, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.015, 0.016, 0.015, 0.016, 0.015, 0.014, 0.014, 0.017, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.015, 0.013, 0.017, 0.015, 0.016, 0.014, 0.015, 0.016, 0.014, 0.017, 0.016, 0.013, 0.016, 0.014, 0.017, 0.012, 0.015, 0.015, 0.013, 0.014, 0.016, 0.017, 0.015, 0.014, 0.014, 0.014, 0.017, 0.014, 0.014, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.015, 0.018, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.016, 0.016, 0.016, 0.015, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.016, 0.017, 0.016, 0.015, 0.014, 0.015, 0.015, 0.014, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.017, 0.013, 0.014, 0.016, 0.014, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.016, 0.016, 0.016, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.016, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.016, 0.013, 0.016, 0.014, 0.015, 0.014, 0.016, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.017, 0.014, 0.016, 0.014, 0.016, 0.013, 0.016, 0.015, 0.016, 0.015, 0.014, 0.014, 0.014, 0.014]\n",
      "[Feb 07, 16:39:24] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:39:24] #> Got bucket_cutoffs = tensor([-1.1059e-02, -5.8170e-06,  1.0992e-02]) and bucket_weights = tensor([-0.0216, -0.0045,  0.0045,  0.0215])\n",
      "[Feb 07, 16:39:24] avg_residual = 0.014988665468990803\n",
      "[Feb 07, 16:39:24] [0] \t\t #> Encoding 7399 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:21,  2.32it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:20,  2.32it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:20,  2.32it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:19,  2.32it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:19,  2.31it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:18,  2.32it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:18,  2.32it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:18,  2.32it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:03<00:18,  2.28it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:17,  2.28it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:04<00:17,  2.28it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:17,  2.14it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:05<00:17,  2.08it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:06<00:17,  2.03it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:16,  2.05it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:15,  2.10it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.10it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:14,  2.16it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:13,  2.21it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:13,  2.22it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:09<00:12,  2.21it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:12,  2.22it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:10<00:11,  2.17it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:11<00:11,  2.14it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:11<00:11,  2.15it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:12<00:10,  2.15it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:13<00:10,  2.09it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:13<00:09,  2.09it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:14<00:08,  2.12it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:14<00:08,  2.15it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:15<00:07,  2.16it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:15<00:07,  2.19it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:16<00:06,  2.20it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:16<00:06,  2.18it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:17<00:06,  2.14it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:17<00:05,  2.15it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:17<00:05,  2.17it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:18<00:04,  2.16it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:18<00:04,  2.17it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:19<00:03,  2.19it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:19<00:03,  2.20it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:20<00:02,  2.14it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:20<00:02,  2.15it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:21<00:01,  2.16it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:21<00:01,  2.17it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:22<00:00,  2.20it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:22<00:00,  2.22it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.18it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:26,  1.88it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:24,  1.97it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:23,  2.01it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:22,  2.04it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:21,  2.05it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:21,  2.05it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:21,  2.05it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:20,  2.05it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:20,  2.04it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:19,  2.05it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:19,  2.03it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:18,  2.03it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:18,  2.04it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:17,  2.01it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:17,  2.01it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:16,  2.02it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:08<00:16,  2.03it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.03it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:09<00:15,  2.01it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:14,  2.03it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:10<00:14,  2.04it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:11<00:13,  2.03it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:12,  2.04it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:12<00:12,  2.05it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:13<00:11,  2.04it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:13<00:10,  2.03it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:14<00:10,  2.04it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:14<00:09,  2.04it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:15<00:09,  2.01it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:15<00:09,  1.97it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:16<00:08,  1.96it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:17<00:07,  1.96it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:17<00:07,  1.98it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:18<00:06,  1.97it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:19<00:05,  1.97it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:19<00:05,  1.97it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:20<00:04,  1.98it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:20<00:04,  1.96it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:21<00:03,  1.96it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:21<00:03,  1.96it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:22<00:02,  1.89it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:23<00:02,  1.81it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:23<00:01,  1.82it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:24<00:01,  1.86it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:24<00:00,  1.90it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:06,  2.17it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:00<00:05,  2.41it/s]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:01<00:05,  2.50it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:01<00:04,  2.55it/s]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:01<00:04,  2.58it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:02<00:03,  2.60it/s]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:02<00:03,  2.60it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:03<00:03,  2.61it/s]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:03<00:02,  2.61it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:03<00:02,  2.61it/s]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:04<00:01,  2.60it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:04<00:01,  2.60it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:05<00:01,  2.59it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:05<00:00,  2.55it/s]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:05<00:00,  2.56it/s]\u001b[A\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.62it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:40:19] [0] \t\t #> Saving chunk 0: \t 7,399 passages and 64,311 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:55, 55.51s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2108.75it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 132880.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:40:20] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:40:20] [0] \t\t Found all files!\n",
      "[Feb 07, 16:40:20] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:40:20] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:40:20] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:40:20] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:40:20] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:40:20] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:40:20] len(emb2pid) = 64311\n",
      "[Feb 07, 16:40:20] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3904_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:40:20] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3904_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:40:20] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:40:20] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2801_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/2801_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2801_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:40:24] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:40:24] [0] \t\t # of sampled PIDs = 9494 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Feb 07, 16:40:24] [0] \t\t #> Encoding 9494 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.97it/s]\n",
      "100%|██████████| 50/50 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████| 49/49 [00:23<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:41:37] [0] \t\t avg_doclen_est = 8.817779541015625 \t len(local_sample) = 9,494\n",
      "[Feb 07, 16:41:37] [0] \t\t Creaing 4,096 partitions.\n",
      "[Feb 07, 16:41:37] [0] \t\t *Estimated* 83,715 embeddings.\n",
      "[Feb 07, 16:41:37] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2801_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 79531 points to 4096 centroids: please provide at least 159744 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 79531 points in 768D to 4096 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 3 (3.89 s, search 3.84 s): objective=19720.9 imbalance=1.472 nsplit=0       \n",
      "[0.015, 0.015, 0.018, 0.014, 0.019, 0.015, 0.015, 0.016, 0.016, 0.017, 0.014, 0.014, 0.015, 0.014, 0.015, 0.015, 0.014, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.016, 0.016, 0.017, 0.014, 0.014, 0.014, 0.016, 0.016, 0.017, 0.015, 0.014, 0.015, 0.013, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.017, 0.017, 0.014, 0.015, 0.015, 0.017, 0.014, 0.013, 0.015, 0.013, 0.015, 0.016, 0.015, 0.015, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.013, 0.017, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.018, 0.014, 0.015, 0.014, 0.015, 0.013, 0.014, 0.016, 0.014, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.016, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.017, 0.017, 0.016, 0.017, 0.014, 0.016, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.016, 0.017, 0.015, 0.014, 0.016, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.016, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.014, 0.014, 0.016, 0.016, 0.015, 0.016, 0.014, 0.015, 0.015, 0.015, 0.013, 0.016, 0.016, 0.014, 0.016, 0.015, 0.014, 0.015, 0.016, 0.016, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.014, 0.015, 0.015, 0.013, 0.015, 0.016, 0.014, 0.014, 0.015, 0.014, 0.013, 0.015, 0.015, 0.015, 0.017, 0.014, 0.016, 0.015, 0.014, 0.013, 0.014, 0.017, 0.012, 0.015, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.019, 0.013, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.016, 0.016, 0.016, 0.014, 0.014, 0.015, 0.014, 0.015, 0.014, 0.016, 0.014, 0.015, 0.015, 0.016, 0.013, 0.015, 0.014, 0.015, 0.014, 0.013, 0.014, 0.013, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.014, 0.014, 0.016, 0.017, 0.015, 0.015, 0.015, 0.016, 0.014, 0.016, 0.015, 0.015, 0.014, 0.014, 0.015, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.017, 0.015, 0.017, 0.015, 0.017, 0.015, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.016, 0.017, 0.014, 0.015, 0.018, 0.013, 0.016, 0.012, 0.014, 0.015, 0.013, 0.015, 0.016, 0.014, 0.014, 0.016, 0.015, 0.014, 0.013, 0.014, 0.013, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.017, 0.013, 0.015, 0.015, 0.013, 0.013, 0.017, 0.015, 0.014, 0.013, 0.016, 0.014, 0.014, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.015, 0.013, 0.016, 0.015, 0.016, 0.014, 0.015, 0.016, 0.016, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.015, 0.014, 0.015, 0.016, 0.014, 0.014, 0.016, 0.015, 0.016, 0.016, 0.017, 0.015, 0.015, 0.015, 0.016, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.013, 0.015, 0.016, 0.013, 0.016, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.013, 0.014, 0.015, 0.015, 0.016, 0.014, 0.014, 0.016, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.017, 0.014, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.014, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.017, 0.014, 0.016, 0.012, 0.015, 0.015, 0.016, 0.013, 0.014, 0.014, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.014, 0.016, 0.014, 0.016, 0.013, 0.015, 0.013, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.014, 0.015, 0.015, 0.015, 0.013, 0.015, 0.015, 0.014, 0.016, 0.014, 0.013, 0.015, 0.015, 0.016, 0.014, 0.013, 0.017, 0.015, 0.014, 0.015, 0.017, 0.016, 0.015, 0.015, 0.015, 0.014, 0.013, 0.015, 0.014, 0.016, 0.017, 0.014, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.014, 0.015, 0.014, 0.014, 0.016, 0.015, 0.016, 0.015, 0.017, 0.015, 0.017, 0.014, 0.015, 0.016, 0.013, 0.015, 0.016, 0.015, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.016, 0.016, 0.014, 0.015, 0.017, 0.015, 0.013, 0.016, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.014, 0.014, 0.014, 0.014, 0.016, 0.015, 0.015, 0.017, 0.013, 0.015, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.016, 0.015, 0.015, 0.013, 0.016, 0.014, 0.012, 0.015, 0.013, 0.015, 0.016, 0.013, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.014, 0.017, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.017, 0.014, 0.014, 0.013, 0.013, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.012, 0.016, 0.014, 0.015, 0.015, 0.015, 0.016, 0.014, 0.016, 0.016, 0.013, 0.016, 0.014, 0.016, 0.013, 0.015, 0.016, 0.013, 0.014, 0.016, 0.017, 0.015, 0.013, 0.014, 0.014, 0.016, 0.014, 0.014, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.015, 0.014, 0.015, 0.014, 0.014, 0.017, 0.015, 0.015, 0.013, 0.015, 0.018, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.016, 0.017, 0.013, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.016, 0.014, 0.014, 0.014, 0.015, 0.016, 0.016, 0.016, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.016, 0.013, 0.015, 0.014, 0.017, 0.013, 0.015, 0.016, 0.015, 0.014, 0.015, 0.015, 0.013, 0.016, 0.014, 0.015, 0.016, 0.016, 0.014, 0.014, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.015, 0.013, 0.016, 0.014, 0.015, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.015, 0.014, 0.015, 0.016, 0.014, 0.015, 0.014, 0.015, 0.013, 0.016, 0.015, 0.016, 0.015, 0.015, 0.015, 0.014, 0.014]\n",
      "[Feb 07, 16:41:42] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:41:42] #> Got bucket_cutoffs = tensor([-1.1020e-02,  6.5882e-06,  1.1029e-02]) and bucket_weights = tensor([-0.0213, -0.0047,  0.0047,  0.0213])\n",
      "[Feb 07, 16:41:42] avg_residual = 0.014890016056597233\n",
      "[Feb 07, 16:41:42] [0] \t\t #> Encoding 9494 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:24,  2.03it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:23,  2.05it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:22,  2.04it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:21,  2.04it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:20,  2.02it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:20,  2.04it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:19,  2.05it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:19,  2.03it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:19,  1.97it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:06<00:18,  2.00it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:17,  2.02it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:17,  2.04it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:16,  2.01it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:08<00:16,  2.03it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.05it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:09<00:15,  2.06it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:12,  2.07it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:12<00:12,  2.07it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:13<00:11,  2.07it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:13<00:10,  2.08it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:14<00:10,  2.07it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:15<00:09,  2.07it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:15<00:08,  2.08it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:16<00:08,  2.08it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:17<00:07,  2.08it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:18<00:06,  2.06it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:19<00:04,  2.05it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:19<00:04,  2.03it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:20<00:03,  2.04it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:20<00:03,  2.04it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:21<00:02,  2.02it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:21<00:02,  2.00it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:22<00:01,  2.02it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:22<00:01,  2.01it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:23<00:00,  2.02it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:23<00:00,  2.01it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:21,  2.27it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:21,  2.22it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:20,  2.25it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:20,  2.27it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:19,  2.29it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:19,  2.30it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:18,  2.29it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:18,  2.29it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:03<00:17,  2.30it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:17,  2.28it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:04<00:17,  2.28it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:16,  2.28it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:05<00:16,  2.29it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:15,  2.30it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:06<00:15,  2.30it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:06<00:14,  2.30it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:07<00:14,  2.27it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:13,  2.24it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:08<00:13,  2.22it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:13,  2.20it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:09<00:12,  2.23it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:12,  2.21it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:10<00:11,  2.24it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:11<00:11,  2.27it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:11<00:10,  2.24it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:11<00:10,  2.23it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:12<00:09,  2.22it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:12<00:09,  2.24it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:13<00:08,  2.26it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:13<00:08,  2.26it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:14<00:07,  2.27it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:14<00:07,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:15<00:07,  2.29it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:15<00:06,  2.29it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:15<00:06,  2.30it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:16<00:05,  2.30it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:16<00:05,  2.31it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:17<00:04,  2.31it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:17<00:04,  2.29it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:18<00:03,  2.26it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:18<00:03,  2.27it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:18<00:03,  2.27it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:19<00:02,  2.28it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:19<00:02,  2.30it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:20<00:01,  2.30it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:20<00:01,  2.29it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:21<00:00,  2.26it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:21<00:00,  2.22it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.27it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/49 [00:00<00:23,  2.04it/s]\u001b[A\n",
      "  4%|▍         | 2/49 [00:00<00:22,  2.13it/s]\u001b[A\n",
      "  6%|▌         | 3/49 [00:01<00:21,  2.12it/s]\u001b[A\n",
      "  8%|▊         | 4/49 [00:01<00:21,  2.10it/s]\u001b[A\n",
      " 10%|█         | 5/49 [00:02<00:21,  2.09it/s]\u001b[A\n",
      " 12%|█▏        | 6/49 [00:02<00:20,  2.13it/s]\u001b[A\n",
      " 14%|█▍        | 7/49 [00:03<00:19,  2.15it/s]\u001b[A\n",
      " 16%|█▋        | 8/49 [00:03<00:18,  2.16it/s]\u001b[A\n",
      " 18%|█▊        | 9/49 [00:04<00:18,  2.17it/s]\u001b[A\n",
      " 20%|██        | 10/49 [00:04<00:17,  2.18it/s]\u001b[A\n",
      " 22%|██▏       | 11/49 [00:05<00:17,  2.19it/s]\u001b[A\n",
      " 24%|██▍       | 12/49 [00:05<00:16,  2.19it/s]\u001b[A\n",
      " 27%|██▋       | 13/49 [00:06<00:16,  2.16it/s]\u001b[A\n",
      " 29%|██▊       | 14/49 [00:06<00:16,  2.15it/s]\u001b[A\n",
      " 31%|███       | 15/49 [00:07<00:16,  2.12it/s]\u001b[A\n",
      " 33%|███▎      | 16/49 [00:07<00:15,  2.11it/s]\u001b[A\n",
      " 35%|███▍      | 17/49 [00:07<00:15,  2.11it/s]\u001b[A\n",
      " 37%|███▋      | 18/49 [00:08<00:15,  2.00it/s]\u001b[A\n",
      " 39%|███▉      | 19/49 [00:09<00:15,  1.93it/s]\u001b[A\n",
      " 41%|████      | 20/49 [00:09<00:15,  1.90it/s]\u001b[A\n",
      " 43%|████▎     | 21/49 [00:10<00:14,  1.96it/s]\u001b[A\n",
      " 45%|████▍     | 22/49 [00:10<00:14,  1.91it/s]\u001b[A\n",
      " 47%|████▋     | 23/49 [00:11<00:13,  1.96it/s]\u001b[A\n",
      " 49%|████▉     | 24/49 [00:11<00:13,  1.91it/s]\u001b[A\n",
      " 51%|█████     | 25/49 [00:12<00:12,  1.91it/s]\u001b[A\n",
      " 53%|█████▎    | 26/49 [00:12<00:11,  1.95it/s]\u001b[A\n",
      " 55%|█████▌    | 27/49 [00:13<00:11,  1.95it/s]\u001b[A\n",
      " 57%|█████▋    | 28/49 [00:13<00:10,  1.92it/s]\u001b[A\n",
      " 59%|█████▉    | 29/49 [00:14<00:10,  1.88it/s]\u001b[A\n",
      " 61%|██████    | 30/49 [00:14<00:09,  1.94it/s]\u001b[A\n",
      " 63%|██████▎   | 31/49 [00:15<00:09,  1.97it/s]\u001b[A\n",
      " 65%|██████▌   | 32/49 [00:15<00:08,  2.00it/s]\u001b[A\n",
      " 67%|██████▋   | 33/49 [00:16<00:07,  2.03it/s]\u001b[A\n",
      " 69%|██████▉   | 34/49 [00:16<00:07,  2.07it/s]\u001b[A\n",
      " 71%|███████▏  | 35/49 [00:17<00:06,  2.10it/s]\u001b[A\n",
      " 73%|███████▎  | 36/49 [00:17<00:06,  2.11it/s]\u001b[A\n",
      " 76%|███████▌  | 37/49 [00:18<00:05,  2.13it/s]\u001b[A\n",
      " 78%|███████▊  | 38/49 [00:18<00:05,  2.13it/s]\u001b[A\n",
      " 80%|███████▉  | 39/49 [00:19<00:04,  2.13it/s]\u001b[A\n",
      " 82%|████████▏ | 40/49 [00:19<00:04,  2.11it/s]\u001b[A\n",
      " 84%|████████▎ | 41/49 [00:19<00:03,  2.12it/s]\u001b[A\n",
      " 86%|████████▌ | 42/49 [00:20<00:03,  2.13it/s]\u001b[A\n",
      " 88%|████████▊ | 43/49 [00:20<00:02,  2.11it/s]\u001b[A\n",
      " 90%|████████▉ | 44/49 [00:21<00:02,  2.12it/s]\u001b[A\n",
      " 92%|█████████▏| 45/49 [00:21<00:01,  2.13it/s]\u001b[A\n",
      " 94%|█████████▍| 46/49 [00:22<00:01,  2.14it/s]\u001b[A\n",
      " 96%|█████████▌| 47/49 [00:22<00:00,  2.13it/s]\u001b[A\n",
      " 98%|█████████▊| 48/49 [00:23<00:00,  2.08it/s]\u001b[A\n",
      "100%|██████████| 49/49 [00:23<00:00,  2.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:42:52] [0] \t\t #> Saving chunk 0: \t 9,494 passages and 83,716 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:12, 72.51s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1665.07it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 113994.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:42:54] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:42:54] [0] \t\t Found all files!\n",
      "[Feb 07, 16:42:54] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:42:54] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:42:54] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:42:54] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:42:54] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:42:54] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:42:54] len(emb2pid) = 83716\n",
      "[Feb 07, 16:42:54] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2801_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:42:54] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2801_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:42:55] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:42:55] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3908_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3908_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3908_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:42:58] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:42:59] [0] \t\t # of sampled PIDs = 146 \t sampled_pids[:3] = [106, 2, 76]\n",
      "[Feb 07, 16:42:59] [0] \t\t #> Encoding 146 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.04it/s]\n",
      "WARNING clustering 1247 points to 512 centroids: please provide at least 19968 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:43:00] [0] \t\t avg_doclen_est = 8.98630142211914 \t len(local_sample) = 146\n",
      "[Feb 07, 16:43:00] [0] \t\t Creaing 512 partitions.\n",
      "[Feb 07, 16:43:00] [0] \t\t *Estimated* 1,312 embeddings.\n",
      "[Feb 07, 16:43:00] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3908_2bits/plan.json ..\n",
      "Clustering 1247 points in 768D to 512 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.01 s, search 0.01 s): objective=66.8126 imbalance=1.415 nsplit=0       \n",
      "[0.011, 0.008, 0.009, 0.01, 0.01, 0.011, 0.009, 0.009, 0.01, 0.011, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.008, 0.01, 0.012, 0.007, 0.007, 0.012, 0.01, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.01, 0.011, 0.011, 0.01, 0.01, 0.009, 0.01, 0.008, 0.01, 0.007, 0.008, 0.01, 0.01, 0.007, 0.007, 0.008, 0.011, 0.007, 0.007, 0.009, 0.007, 0.009, 0.01, 0.012, 0.011, 0.011, 0.01, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.008, 0.011, 0.009, 0.011, 0.01, 0.012, 0.008, 0.009, 0.011, 0.009, 0.009, 0.01, 0.007, 0.007, 0.011, 0.009, 0.009, 0.01, 0.008, 0.01, 0.01, 0.009, 0.011, 0.009, 0.007, 0.009, 0.009, 0.01, 0.011, 0.008, 0.007, 0.01, 0.008, 0.01, 0.01, 0.01, 0.01, 0.01, 0.008, 0.012, 0.012, 0.01, 0.008, 0.009, 0.012, 0.009, 0.01, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.01, 0.009, 0.008, 0.008, 0.009, 0.009, 0.008, 0.008, 0.008, 0.011, 0.009, 0.009, 0.011, 0.009, 0.01, 0.008, 0.01, 0.007, 0.009, 0.008, 0.009, 0.011, 0.01, 0.01, 0.009, 0.013, 0.009, 0.01, 0.008, 0.01, 0.011, 0.01, 0.01, 0.01, 0.008, 0.009, 0.008, 0.009, 0.01, 0.011, 0.009, 0.012, 0.008, 0.009, 0.008, 0.01, 0.008, 0.01, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.011, 0.008, 0.009, 0.009, 0.009, 0.01, 0.009, 0.01, 0.01, 0.008, 0.01, 0.01, 0.008, 0.009, 0.009, 0.011, 0.01, 0.008, 0.011, 0.009, 0.009, 0.007, 0.006, 0.01, 0.01, 0.01, 0.009, 0.01, 0.008, 0.008, 0.009, 0.009, 0.01, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.009, 0.007, 0.009, 0.01, 0.008, 0.009, 0.01, 0.008, 0.008, 0.009, 0.008, 0.008, 0.008, 0.009, 0.012, 0.01, 0.011, 0.011, 0.01, 0.01, 0.008, 0.008, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.007, 0.017, 0.007, 0.008, 0.009, 0.008, 0.011, 0.009, 0.008, 0.009, 0.013, 0.009, 0.008, 0.01, 0.007, 0.008, 0.009, 0.008, 0.009, 0.009, 0.011, 0.008, 0.01, 0.009, 0.008, 0.008, 0.008, 0.009, 0.008, 0.009, 0.008, 0.008, 0.01, 0.009, 0.007, 0.008, 0.011, 0.007, 0.009, 0.009, 0.007, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.009, 0.007, 0.011, 0.011, 0.009, 0.009, 0.009, 0.008, 0.007, 0.007, 0.012, 0.011, 0.009, 0.008, 0.009, 0.009, 0.009, 0.01, 0.01, 0.008, 0.009, 0.01, 0.011, 0.011, 0.01, 0.011, 0.009, 0.011, 0.01, 0.012, 0.007, 0.009, 0.012, 0.008, 0.011, 0.008, 0.008, 0.008, 0.008, 0.01, 0.009, 0.009, 0.008, 0.01, 0.009, 0.008, 0.008, 0.01, 0.008, 0.009, 0.009, 0.009, 0.012, 0.008, 0.009, 0.01, 0.006, 0.008, 0.009, 0.008, 0.008, 0.009, 0.009, 0.01, 0.009, 0.009, 0.008, 0.009, 0.007, 0.008, 0.007, 0.009, 0.011, 0.01, 0.011, 0.007, 0.008, 0.008, 0.01, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.01, 0.009, 0.009, 0.009, 0.009, 0.01, 0.008, 0.008, 0.008, 0.009, 0.009, 0.008, 0.009, 0.01, 0.01, 0.011, 0.01, 0.01, 0.008, 0.008, 0.008, 0.009, 0.007, 0.009, 0.008, 0.011, 0.011, 0.009, 0.008, 0.01, 0.006, 0.01, 0.009, 0.009, 0.01, 0.009, 0.008, 0.008, 0.007, 0.01, 0.011, 0.009, 0.008, 0.008, 0.007, 0.01, 0.009, 0.009, 0.011, 0.01, 0.009, 0.01, 0.008, 0.009, 0.01, 0.011, 0.009, 0.011, 0.007, 0.008, 0.009, 0.011, 0.009, 0.007, 0.009, 0.008, 0.009, 0.01, 0.008, 0.009, 0.009, 0.008, 0.009, 0.007, 0.009, 0.007, 0.009, 0.01, 0.009, 0.008, 0.01, 0.01, 0.009, 0.013, 0.01, 0.012, 0.008, 0.01, 0.007, 0.009, 0.01, 0.008, 0.011, 0.01, 0.011, 0.009, 0.01, 0.008, 0.01, 0.007, 0.011, 0.01, 0.007, 0.008, 0.007, 0.009, 0.01, 0.008, 0.01, 0.01, 0.008, 0.009, 0.009, 0.01, 0.01, 0.008, 0.009, 0.009, 0.007, 0.008, 0.011, 0.006, 0.011, 0.009, 0.009, 0.009, 0.007, 0.01, 0.008, 0.009, 0.01, 0.007, 0.011, 0.009, 0.006, 0.009, 0.009, 0.008, 0.01, 0.012, 0.008, 0.009, 0.009, 0.012, 0.01, 0.008, 0.008, 0.01, 0.007, 0.008, 0.008, 0.01, 0.008, 0.01, 0.011, 0.01, 0.01, 0.008, 0.009, 0.01, 0.01, 0.011, 0.011, 0.009, 0.01, 0.008, 0.009, 0.008, 0.011, 0.011, 0.008, 0.008, 0.011, 0.009, 0.01, 0.008, 0.013, 0.007, 0.007, 0.009, 0.009, 0.008, 0.01, 0.009, 0.009, 0.008, 0.011, 0.009, 0.01, 0.008, 0.01, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.008, 0.009, 0.009, 0.009, 0.009, 0.01, 0.01, 0.007, 0.008, 0.008, 0.009, 0.011, 0.01, 0.01, 0.011, 0.008, 0.008, 0.011, 0.009, 0.01, 0.011, 0.009, 0.008, 0.01, 0.013, 0.01, 0.008, 0.008, 0.011, 0.009, 0.007, 0.011, 0.007, 0.01, 0.011, 0.008, 0.008, 0.01, 0.009, 0.008, 0.01, 0.008, 0.009, 0.01, 0.009, 0.008, 0.009, 0.01, 0.012, 0.008, 0.009, 0.007, 0.011, 0.01, 0.008, 0.009, 0.009, 0.009, 0.009, 0.009, 0.013, 0.007, 0.007, 0.011, 0.008, 0.009, 0.01, 0.008, 0.009, 0.01, 0.011, 0.013, 0.01, 0.013, 0.009, 0.01, 0.01, 0.007, 0.01, 0.01, 0.009, 0.008, 0.01, 0.01, 0.007, 0.007, 0.009, 0.01, 0.01, 0.008, 0.01, 0.008, 0.01, 0.01, 0.009, 0.009, 0.008, 0.009, 0.008, 0.007, 0.01, 0.008, 0.009, 0.009, 0.008, 0.01, 0.012, 0.009, 0.009, 0.008, 0.009, 0.01, 0.01, 0.008, 0.011, 0.008, 0.012, 0.012, 0.011, 0.008, 0.01, 0.008, 0.01, 0.011, 0.008, 0.009, 0.009, 0.013, 0.009, 0.007, 0.009, 0.008, 0.008, 0.011, 0.011, 0.01, 0.008, 0.011, 0.008, 0.009, 0.008, 0.009, 0.01, 0.007, 0.01, 0.007, 0.011, 0.01, 0.009, 0.008, 0.008, 0.009, 0.009, 0.011, 0.008, 0.011, 0.008, 0.01, 0.011, 0.013, 0.007, 0.007, 0.009, 0.009, 0.01, 0.009, 0.009, 0.01, 0.008, 0.01, 0.009, 0.009, 0.011, 0.01, 0.011, 0.01, 0.009, 0.009, 0.008, 0.009, 0.009, 0.01, 0.009, 0.008, 0.01, 0.011, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.01, 0.008, 0.01, 0.007, 0.009, 0.009, 0.008, 0.01]\n",
      "[Feb 07, 16:43:00] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:43:00] #> Got bucket_cutoffs = tensor([-5.7416e-03, -1.5033e-05,  5.8384e-03]) and bucket_weights = tensor([-0.0126, -0.0023,  0.0023,  0.0127])\n",
      "[Feb 07, 16:43:00] avg_residual = 0.009199653752148151\n",
      "[Feb 07, 16:43:00] [0] \t\t #> Encoding 146 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  2.13it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.91it/s]\u001b[A\n",
      "1it [00:01,  1.05s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2417.47it/s]\n",
      "100%|██████████| 512/512 [00:00<00:00, 119238.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:43:01] [0] \t\t #> Saving chunk 0: \t 146 passages and 1,312 embeddings. From #0 onward.\n",
      "[Feb 07, 16:43:01] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:43:01] [0] \t\t Found all files!\n",
      "[Feb 07, 16:43:01] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:43:01] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:43:01] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:43:01] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:43:01] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:43:01] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:43:01] len(emb2pid) = 1312\n",
      "[Feb 07, 16:43:01] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3908_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:43:01] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3908_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:43:02] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:43:02] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510401_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/510401_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_510401_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:43:05] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:43:05] [0] \t\t # of sampled PIDs = 3839 \t sampled_pids[:3] = [1706, 3001, 41]\n",
      "[Feb 07, 16:43:05] [0] \t\t #> Encoding 3839 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.67it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:43:42] [0] \t\t avg_doclen_est = 10.477989196777344 \t len(local_sample) = 3,839\n",
      "[Feb 07, 16:43:43] [0] \t\t Creaing 2,048 partitions.\n",
      "[Feb 07, 16:43:43] [0] \t\t *Estimated* 40,225 embeddings.\n",
      "[Feb 07, 16:43:43] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510401_2bits/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 38214 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 38214 points in 768D to 2048 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.03 s\n",
      "  Iteration 3 (0.99 s, search 0.96 s): objective=8413.87 imbalance=1.475 nsplit=0       \n",
      "[0.014, 0.013, 0.015, 0.013, 0.017, 0.013, 0.014, 0.014, 0.015, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.016, 0.013, 0.014, 0.014, 0.016, 0.013, 0.013, 0.012, 0.015, 0.015, 0.015, 0.013, 0.014, 0.015, 0.012, 0.012, 0.013, 0.013, 0.012, 0.016, 0.015, 0.015, 0.014, 0.016, 0.012, 0.013, 0.012, 0.013, 0.012, 0.014, 0.012, 0.016, 0.014, 0.013, 0.014, 0.014, 0.016, 0.012, 0.012, 0.014, 0.013, 0.015, 0.015, 0.014, 0.013, 0.014, 0.015, 0.013, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.012, 0.012, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.016, 0.014, 0.014, 0.014, 0.014, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.014, 0.012, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.015, 0.014, 0.015, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.016, 0.014, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.012, 0.014, 0.015, 0.015, 0.014, 0.016, 0.014, 0.013, 0.016, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.016, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.014, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.015, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.014, 0.013, 0.013, 0.015, 0.013, 0.014, 0.015, 0.012, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.017, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.015, 0.011, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.013, 0.013, 0.013, 0.014, 0.015, 0.013, 0.015, 0.013, 0.014, 0.018, 0.011, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.015, 0.014, 0.015, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.015, 0.015, 0.014, 0.014, 0.014, 0.016, 0.012, 0.015, 0.014, 0.014, 0.012, 0.013, 0.014, 0.015, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.014, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.015, 0.014, 0.012, 0.014, 0.015, 0.016, 0.012, 0.013, 0.016, 0.012, 0.014, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.012, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.013, 0.012, 0.015, 0.012, 0.013, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.015, 0.014, 0.015, 0.013, 0.014, 0.015, 0.015, 0.014, 0.013, 0.013, 0.013, 0.015, 0.014, 0.013, 0.012, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.016, 0.014, 0.015, 0.014, 0.013, 0.012, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.015, 0.012, 0.014, 0.016, 0.012, 0.015, 0.013, 0.015, 0.014, 0.014, 0.015, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.015, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.016, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.012, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.017, 0.013, 0.015, 0.012, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.012, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.015, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.012, 0.014, 0.014, 0.012, 0.014, 0.013, 0.015, 0.013, 0.013, 0.016, 0.014, 0.013, 0.015, 0.016, 0.015, 0.013, 0.014, 0.014, 0.013, 0.012, 0.014, 0.013, 0.015, 0.015, 0.012, 0.014, 0.012, 0.014, 0.014, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.015, 0.013, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.013, 0.014, 0.013, 0.015, 0.015, 0.012, 0.013, 0.017, 0.014, 0.011, 0.015, 0.013, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.015, 0.013, 0.016, 0.012, 0.014, 0.013, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.012, 0.014, 0.013, 0.012, 0.014, 0.012, 0.014, 0.015, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.015, 0.015, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.015, 0.013, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.014, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.016, 0.013, 0.016, 0.014, 0.012, 0.014, 0.013, 0.015, 0.011, 0.014, 0.014, 0.012, 0.013, 0.015, 0.016, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.012, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.013, 0.014, 0.016, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.015, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.014, 0.014, 0.014, 0.011, 0.013, 0.012, 0.016, 0.012, 0.014, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.016, 0.013, 0.013, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.015, 0.015, 0.014, 0.014, 0.014, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.016, 0.015, 0.013, 0.014, 0.015, 0.013, 0.014, 0.013, 0.014, 0.011, 0.015, 0.015, 0.015, 0.013, 0.013, 0.014, 0.013, 0.013]\n",
      "[Feb 07, 16:43:44] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:43:44] #> Got bucket_cutoffs = tensor([-9.5907e-03, -2.2310e-05,  9.5237e-03]) and bucket_weights = tensor([-0.0195, -0.0038,  0.0038,  0.0195])\n",
      "[Feb 07, 16:43:44] avg_residual = 0.013620569370687008\n",
      "[Feb 07, 16:43:44] [0] \t\t #> Encoding 3839 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:29,  1.69it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:27,  1.75it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:26,  1.77it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:25,  1.77it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:25,  1.77it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:24,  1.77it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:24,  1.77it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:23,  1.78it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:05<00:23,  1.76it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:22,  1.76it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:06<00:22,  1.76it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:21,  1.76it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:07<00:20,  1.76it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:07<00:20,  1.76it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:20,  1.75it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:09<00:19,  1.74it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:18,  1.74it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:10<00:18,  1.74it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:10<00:17,  1.74it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:11<00:17,  1.72it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:12<00:17,  1.68it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:12<00:16,  1.70it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:13<00:15,  1.71it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:13<00:15,  1.73it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:14<00:14,  1.71it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:14<00:14,  1.69it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:15<00:13,  1.70it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:16<00:13,  1.65it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:16<00:12,  1.65it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:17<00:12,  1.65it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:18<00:13,  1.38it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:19<00:15,  1.19it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:20<00:15,  1.10it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:21<00:13,  1.15it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:22<00:12,  1.24it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:22<00:10,  1.34it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:23<00:09,  1.43it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:23<00:08,  1.45it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:24<00:07,  1.52it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:25<00:06,  1.57it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:25<00:05,  1.61it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:26<00:04,  1.64it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:26<00:04,  1.67it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:27<00:03,  1.68it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:27<00:02,  1.68it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:28<00:02,  1.67it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:29<00:01,  1.66it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:29<00:01,  1.67it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:30<00:00,  1.63it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:31<00:00,  1.61it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.29it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:06,  1.30it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:05,  1.36it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:04,  1.42it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.40it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.37it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:02,  1.38it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.41it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.43it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:44:22] [0] \t\t #> Saving chunk 0: \t 3,839 passages and 40,225 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:38, 38.92s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2152.03it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 142332.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:44:23] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:44:23] [0] \t\t Found all files!\n",
      "[Feb 07, 16:44:23] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:44:23] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:44:23] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:44:23] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:44:23] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:44:23] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:44:23] len(emb2pid) = 40225\n",
      "[Feb 07, 16:44:23] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510401_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:44:23] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_510401_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:44:24] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:44:24] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2102_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/2102_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_2102_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:44:27] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:44:27] [0] \t\t # of sampled PIDs = 11118 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Feb 07, 16:44:27] [0] \t\t #> Encoding 11118 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 50/50 [00:29<00:00,  1.72it/s]\n",
      "100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:45:59] [0] \t\t avg_doclen_est = 9.072134971618652 \t len(local_sample) = 11,118\n",
      "[Feb 07, 16:45:59] [0] \t\t Creaing 4,096 partitions.\n",
      "[Feb 07, 16:45:59] [0] \t\t *Estimated* 100,863 embeddings.\n",
      "[Feb 07, 16:45:59] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2102_2bits/plan.json ..\n",
      "Clustering 95821 points in 768D to 4096 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 95821 points to 4096 centroids: please provide at least 159744 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (4.65 s, search 4.59 s): objective=26311.4 imbalance=1.525 nsplit=0       \n",
      "[0.017, 0.016, 0.018, 0.015, 0.02, 0.015, 0.016, 0.017, 0.016, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.016, 0.016, 0.014, 0.018, 0.015, 0.016, 0.017, 0.017, 0.015, 0.014, 0.015, 0.017, 0.017, 0.017, 0.016, 0.015, 0.016, 0.014, 0.014, 0.015, 0.015, 0.014, 0.017, 0.017, 0.016, 0.016, 0.016, 0.014, 0.016, 0.014, 0.015, 0.015, 0.015, 0.014, 0.017, 0.016, 0.016, 0.015, 0.015, 0.019, 0.015, 0.014, 0.017, 0.014, 0.016, 0.017, 0.016, 0.016, 0.016, 0.017, 0.015, 0.016, 0.016, 0.014, 0.014, 0.015, 0.016, 0.016, 0.017, 0.017, 0.016, 0.015, 0.013, 0.018, 0.015, 0.014, 0.016, 0.015, 0.015, 0.014, 0.018, 0.016, 0.015, 0.016, 0.016, 0.014, 0.015, 0.017, 0.014, 0.016, 0.015, 0.015, 0.015, 0.016, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.016, 0.017, 0.015, 0.016, 0.016, 0.014, 0.014, 0.015, 0.018, 0.016, 0.016, 0.017, 0.014, 0.016, 0.015, 0.016, 0.014, 0.016, 0.015, 0.016, 0.017, 0.017, 0.017, 0.014, 0.016, 0.014, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.017, 0.015, 0.016, 0.015, 0.016, 0.016, 0.016, 0.014, 0.017, 0.016, 0.015, 0.017, 0.015, 0.015, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.016, 0.016, 0.017, 0.015, 0.016, 0.015, 0.017, 0.016, 0.016, 0.014, 0.015, 0.017, 0.014, 0.015, 0.017, 0.014, 0.014, 0.015, 0.014, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.016, 0.015, 0.015, 0.013, 0.016, 0.016, 0.012, 0.015, 0.015, 0.013, 0.014, 0.015, 0.015, 0.016, 0.014, 0.015, 0.017, 0.016, 0.016, 0.016, 0.016, 0.015, 0.014, 0.016, 0.015, 0.016, 0.017, 0.015, 0.016, 0.016, 0.015, 0.019, 0.013, 0.015, 0.015, 0.014, 0.016, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.017, 0.014, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.013, 0.015, 0.015, 0.016, 0.016, 0.014, 0.015, 0.014, 0.016, 0.015, 0.015, 0.014, 0.016, 0.017, 0.016, 0.015, 0.016, 0.017, 0.015, 0.017, 0.015, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.017, 0.015, 0.015, 0.015, 0.015, 0.014, 0.017, 0.015, 0.017, 0.015, 0.018, 0.016, 0.015, 0.016, 0.017, 0.015, 0.014, 0.016, 0.016, 0.017, 0.015, 0.015, 0.018, 0.013, 0.016, 0.013, 0.015, 0.015, 0.014, 0.016, 0.017, 0.014, 0.014, 0.016, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.016, 0.015, 0.016, 0.016, 0.016, 0.018, 0.014, 0.015, 0.015, 0.013, 0.013, 0.017, 0.015, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.017, 0.016, 0.014, 0.016, 0.015, 0.016, 0.015, 0.016, 0.017, 0.017, 0.016, 0.015, 0.016, 0.015, 0.017, 0.016, 0.016, 0.015, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.016, 0.016, 0.017, 0.016, 0.016, 0.015, 0.017, 0.014, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.018, 0.015, 0.015, 0.018, 0.013, 0.016, 0.015, 0.017, 0.016, 0.015, 0.017, 0.015, 0.014, 0.015, 0.016, 0.016, 0.016, 0.015, 0.015, 0.017, 0.017, 0.014, 0.017, 0.015, 0.015, 0.016, 0.014, 0.016, 0.016, 0.018, 0.015, 0.016, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.013, 0.015, 0.016, 0.015, 0.014, 0.014, 0.015, 0.016, 0.015, 0.016, 0.012, 0.015, 0.015, 0.017, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.016, 0.014, 0.015, 0.015, 0.015, 0.016, 0.015, 0.017, 0.014, 0.017, 0.014, 0.016, 0.013, 0.016, 0.015, 0.016, 0.015, 0.016, 0.016, 0.017, 0.016, 0.015, 0.015, 0.016, 0.016, 0.016, 0.013, 0.016, 0.015, 0.015, 0.017, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.014, 0.018, 0.015, 0.015, 0.015, 0.017, 0.017, 0.014, 0.016, 0.016, 0.014, 0.013, 0.015, 0.015, 0.016, 0.017, 0.014, 0.016, 0.014, 0.016, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.015, 0.015, 0.017, 0.016, 0.014, 0.016, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.016, 0.016, 0.017, 0.015, 0.017, 0.015, 0.015, 0.017, 0.015, 0.015, 0.016, 0.015, 0.016, 0.014, 0.017, 0.015, 0.015, 0.014, 0.016, 0.016, 0.015, 0.015, 0.015, 0.016, 0.017, 0.015, 0.017, 0.017, 0.014, 0.016, 0.016, 0.015, 0.013, 0.016, 0.015, 0.014, 0.016, 0.015, 0.014, 0.014, 0.015, 0.017, 0.014, 0.014, 0.015, 0.014, 0.016, 0.016, 0.015, 0.017, 0.013, 0.016, 0.015, 0.016, 0.016, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.016, 0.014, 0.017, 0.015, 0.013, 0.016, 0.013, 0.015, 0.017, 0.013, 0.015, 0.016, 0.016, 0.014, 0.015, 0.015, 0.014, 0.017, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.014, 0.014, 0.013, 0.015, 0.016, 0.016, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.016, 0.013, 0.017, 0.015, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.017, 0.014, 0.017, 0.015, 0.017, 0.013, 0.016, 0.016, 0.014, 0.016, 0.016, 0.017, 0.016, 0.014, 0.015, 0.015, 0.016, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.015, 0.014, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.014, 0.016, 0.018, 0.015, 0.015, 0.015, 0.014, 0.016, 0.014, 0.017, 0.017, 0.014, 0.015, 0.016, 0.017, 0.016, 0.015, 0.016, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.017, 0.016, 0.016, 0.014, 0.015, 0.015, 0.015, 0.015, 0.017, 0.016, 0.013, 0.016, 0.015, 0.017, 0.013, 0.016, 0.017, 0.014, 0.014, 0.015, 0.015, 0.014, 0.017, 0.015, 0.016, 0.016, 0.016, 0.014, 0.016, 0.015, 0.015, 0.016, 0.015, 0.017, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.017, 0.016, 0.016, 0.013, 0.017, 0.014, 0.015, 0.014, 0.016, 0.015, 0.015, 0.015, 0.017, 0.015, 0.015, 0.016, 0.017, 0.014, 0.017, 0.015, 0.017, 0.013, 0.016, 0.016, 0.017, 0.015, 0.014, 0.015, 0.014, 0.015]\n",
      "[Feb 07, 16:46:06] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:46:06] #> Got bucket_cutoffs = tensor([-1.1138e-02, -1.1083e-07,  1.1151e-02]) and bucket_weights = tensor([-0.0222, -0.0046,  0.0046,  0.0222])\n",
      "[Feb 07, 16:46:06] avg_residual = 0.015406397171318531\n",
      "[Feb 07, 16:46:06] [0] \t\t #> Encoding 11118 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:24,  1.97it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:24,  1.98it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:26,  1.81it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:24,  1.87it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:24,  1.87it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:24,  1.78it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:23,  1.79it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:05<00:23,  1.74it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:22,  1.76it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:06<00:22,  1.77it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:21,  1.80it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:07<00:19,  1.85it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:07<00:19,  1.86it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:18,  1.90it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:08<00:17,  1.90it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:17,  1.89it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:09<00:16,  1.92it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:10<00:15,  1.94it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:10<00:15,  1.96it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:11<00:14,  1.97it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:11<00:14,  1.97it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:12<00:13,  1.98it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:12<00:13,  1.97it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:13<00:12,  1.97it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:13<00:12,  1.97it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:14<00:11,  1.97it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:14<00:11,  1.97it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:15<00:10,  1.97it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:15<00:10,  1.96it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:16<00:09,  1.95it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:16<00:09,  1.95it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:17<00:08,  1.96it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:17<00:08,  1.97it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:18<00:07,  1.98it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:18<00:07,  1.99it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:19<00:06,  1.99it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:19<00:06,  2.00it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:20<00:05,  1.98it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:20<00:05,  1.99it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:21<00:04,  1.99it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:21<00:04,  1.99it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:22<00:03,  1.99it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:22<00:03,  1.98it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:23<00:02,  1.98it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:23<00:02,  1.98it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:24<00:01,  1.98it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:24<00:01,  1.99it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:25<00:00,  1.99it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:28,  1.73it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:01<00:27,  1.76it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:26,  1.77it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:02<00:25,  1.78it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:25,  1.78it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:24,  1.77it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:04<00:23,  1.78it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:05<00:23,  1.78it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:05<00:22,  1.78it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:06<00:21,  1.78it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:06<00:21,  1.73it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:07<00:21,  1.69it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:08<00:21,  1.69it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:08<00:20,  1.69it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:09<00:20,  1.70it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:09<00:19,  1.72it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:10<00:18,  1.75it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:10<00:17,  1.76it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:11<00:16,  1.78it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:11<00:16,  1.79it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:13<00:15,  1.76it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:13<00:14,  1.77it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:14<00:14,  1.77it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:14<00:13,  1.78it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:15<00:13,  1.76it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:15<00:12,  1.77it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:16<00:11,  1.78it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:17<00:11,  1.79it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:17<00:10,  1.79it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:18<00:10,  1.80it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:18<00:09,  1.77it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:19<00:08,  1.78it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:19<00:08,  1.79it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:20<00:07,  1.79it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:20<00:07,  1.79it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:21<00:06,  1.77it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:22<00:06,  1.76it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:22<00:05,  1.68it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:23<00:05,  1.66it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:23<00:04,  1.70it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:24<00:04,  1.73it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:25<00:03,  1.73it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:25<00:02,  1.68it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:26<00:02,  1.70it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:26<00:01,  1.73it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:27<00:01,  1.75it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:27<00:00,  1.77it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.75it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:22,  2.20it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:21,  2.21it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:21,  2.20it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:20,  2.20it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:02<00:20,  2.19it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:20,  2.19it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:03<00:19,  2.19it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:19,  2.19it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:04<00:18,  2.19it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:18,  2.19it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:17,  2.19it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:05<00:17,  2.19it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:05<00:16,  2.19it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:06<00:16,  2.19it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:06<00:15,  2.19it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:07<00:15,  2.15it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:08<00:15,  2.12it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:08<00:15,  2.02it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:09<00:14,  2.00it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:09<00:14,  2.01it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:10<00:14,  1.85it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:14,  1.81it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:12<00:13,  1.88it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:12<00:11,  2.00it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:13<00:10,  2.02it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:13<00:10,  1.99it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:14<00:10,  1.96it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:14<00:09,  1.99it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:15<00:09,  1.99it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:15<00:08,  2.05it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:18<00:05,  2.01it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:18<00:05,  2.04it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:19<00:04,  2.04it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:19<00:04,  2.08it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:20<00:03,  2.10it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:20<00:03,  2.13it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:21<00:02,  2.11it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:21<00:02,  2.13it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:22<00:01,  2.15it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:22<00:01,  2.17it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:23<00:00,  2.18it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:23<00:00,  2.18it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/24 [00:00<00:11,  1.99it/s]\u001b[A\n",
      "  8%|▊         | 2/24 [00:00<00:10,  2.14it/s]\u001b[A\n",
      " 12%|█▎        | 3/24 [00:01<00:09,  2.20it/s]\u001b[A\n",
      " 17%|█▋        | 4/24 [00:01<00:08,  2.25it/s]\u001b[A\n",
      " 21%|██        | 5/24 [00:02<00:08,  2.28it/s]\u001b[A\n",
      " 25%|██▌       | 6/24 [00:02<00:07,  2.30it/s]\u001b[A\n",
      " 29%|██▉       | 7/24 [00:03<00:07,  2.30it/s]\u001b[A\n",
      " 33%|███▎      | 8/24 [00:03<00:06,  2.31it/s]\u001b[A\n",
      " 38%|███▊      | 9/24 [00:03<00:06,  2.31it/s]\u001b[A\n",
      " 42%|████▏     | 10/24 [00:04<00:06,  2.31it/s]\u001b[A\n",
      " 46%|████▌     | 11/24 [00:04<00:05,  2.29it/s]\u001b[A\n",
      " 50%|█████     | 12/24 [00:05<00:05,  2.28it/s]\u001b[A\n",
      " 54%|█████▍    | 13/24 [00:05<00:04,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 14/24 [00:06<00:05,  1.87it/s]\u001b[A\n",
      " 62%|██████▎   | 15/24 [00:06<00:04,  1.97it/s]\u001b[A\n",
      " 67%|██████▋   | 16/24 [00:07<00:03,  2.05it/s]\u001b[A\n",
      " 71%|███████   | 17/24 [00:07<00:03,  2.12it/s]\u001b[A\n",
      " 75%|███████▌  | 18/24 [00:08<00:02,  2.17it/s]\u001b[A\n",
      " 79%|███████▉  | 19/24 [00:08<00:02,  2.21it/s]\u001b[A\n",
      " 83%|████████▎ | 20/24 [00:09<00:01,  2.24it/s]\u001b[A\n",
      " 88%|████████▊ | 21/24 [00:09<00:01,  2.27it/s]\u001b[A\n",
      " 92%|█████████▏| 22/24 [00:09<00:00,  2.29it/s]\u001b[A\n",
      " 96%|█████████▌| 23/24 [00:10<00:00,  2.29it/s]\u001b[A\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:47:35] [0] \t\t #> Saving chunk 0: \t 11,118 passages and 100,864 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:32, 92.21s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1796.28it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 135607.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:47:38] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:47:38] [0] \t\t Found all files!\n",
      "[Feb 07, 16:47:38] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:47:38] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:47:38] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:47:38] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:47:38] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:47:38] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:47:38] len(emb2pid) = 100864\n",
      "[Feb 07, 16:47:38] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2102_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:47:38] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_2102_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:47:38] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:47:38] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3903_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3903_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3903_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:47:42] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:47:42] [0] \t\t # of sampled PIDs = 1452 \t sampled_pids[:3] = [853, 20, 611]\n",
      "[Feb 07, 16:47:42] [0] \t\t #> Encoding 1452 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.64it/s]\n",
      "WARNING clustering 11542 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:47:51] [0] \t\t avg_doclen_est = 8.367079734802246 \t len(local_sample) = 1,452\n",
      "[Feb 07, 16:47:51] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:47:51] [0] \t\t *Estimated* 12,148 embeddings.\n",
      "[Feb 07, 16:47:51] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3903_2bits/plan.json ..\n",
      "Clustering 11542 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 3 (0.15 s, search 0.14 s): objective=2264.63 imbalance=1.477 nsplit=0       \n",
      "[0.015, 0.014, 0.015, 0.012, 0.016, 0.013, 0.014, 0.014, 0.015, 0.014, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.014, 0.015, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.012, 0.013, 0.013, 0.014, 0.015, 0.015, 0.014, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.012, 0.015, 0.012, 0.013, 0.015, 0.015, 0.012, 0.012, 0.013, 0.011, 0.012, 0.013, 0.013, 0.015, 0.013, 0.012, 0.014, 0.013, 0.016, 0.012, 0.012, 0.014, 0.011, 0.013, 0.013, 0.013, 0.013, 0.014, 0.015, 0.013, 0.013, 0.015, 0.013, 0.012, 0.013, 0.013, 0.015, 0.015, 0.014, 0.012, 0.012, 0.012, 0.016, 0.013, 0.013, 0.013, 0.012, 0.011, 0.012, 0.016, 0.014, 0.014, 0.013, 0.013, 0.012, 0.014, 0.015, 0.012, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.013, 0.015, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.015, 0.013, 0.015, 0.013, 0.013, 0.016, 0.012, 0.012, 0.012, 0.016, 0.015, 0.014, 0.015, 0.012, 0.015, 0.013, 0.013, 0.011, 0.013, 0.014, 0.013, 0.015, 0.013, 0.014, 0.012, 0.015, 0.014, 0.012, 0.012, 0.015, 0.014, 0.014, 0.013, 0.013, 0.013, 0.015, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.015, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.014, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.012, 0.013, 0.014, 0.015, 0.012, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.015, 0.013, 0.013, 0.013, 0.013, 0.01, 0.012, 0.015, 0.01, 0.014, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.015, 0.015, 0.013, 0.015, 0.014, 0.013, 0.016, 0.012, 0.014, 0.013, 0.011, 0.014, 0.012, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.014, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.013, 0.015, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.011, 0.011, 0.014, 0.012, 0.014, 0.014, 0.012, 0.012, 0.011, 0.014, 0.013, 0.013, 0.012, 0.013, 0.015, 0.014, 0.013, 0.013, 0.015, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.014, 0.016, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.012, 0.015, 0.014, 0.015, 0.014, 0.015, 0.014, 0.012, 0.014, 0.015, 0.013, 0.012, 0.013, 0.015, 0.015, 0.012, 0.013, 0.015, 0.012, 0.015, 0.011, 0.013, 0.012, 0.012, 0.012, 0.015, 0.013, 0.011, 0.015, 0.013, 0.013, 0.011, 0.014, 0.011, 0.013, 0.014, 0.012, 0.015, 0.013, 0.014, 0.015, 0.012, 0.013, 0.012, 0.011, 0.011, 0.015, 0.013, 0.013, 0.012, 0.014, 0.012, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.011, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.014, 0.011, 0.014, 0.013, 0.013, 0.014, 0.012, 0.014, 0.014, 0.016, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.013, 0.014, 0.013, 0.016, 0.012, 0.013, 0.016, 0.012, 0.014, 0.012, 0.013, 0.015, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.015, 0.013, 0.012, 0.016, 0.015, 0.013, 0.015, 0.013, 0.012, 0.014, 0.013, 0.014, 0.014, 0.015, 0.014, 0.014, 0.014, 0.013, 0.014, 0.015, 0.014, 0.013, 0.014, 0.011, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.014, 0.013, 0.015, 0.011, 0.013, 0.014, 0.016, 0.012, 0.012, 0.015, 0.012, 0.014, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.012, 0.016, 0.012, 0.014, 0.012, 0.014, 0.011, 0.013, 0.012, 0.014, 0.014, 0.012, 0.015, 0.015, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.011, 0.013, 0.013, 0.013, 0.015, 0.012, 0.012, 0.014, 0.013, 0.014, 0.012, 0.013, 0.015, 0.012, 0.012, 0.013, 0.016, 0.013, 0.013, 0.013, 0.013, 0.012, 0.011, 0.013, 0.012, 0.015, 0.015, 0.013, 0.012, 0.012, 0.015, 0.013, 0.012, 0.012, 0.013, 0.013, 0.012, 0.013, 0.013, 0.015, 0.014, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.016, 0.013, 0.014, 0.015, 0.015, 0.013, 0.015, 0.012, 0.013, 0.015, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.016, 0.012, 0.012, 0.015, 0.013, 0.014, 0.014, 0.012, 0.014, 0.016, 0.014, 0.01, 0.014, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.014, 0.013, 0.016, 0.011, 0.013, 0.014, 0.015, 0.013, 0.013, 0.012, 0.013, 0.013, 0.014, 0.014, 0.013, 0.012, 0.014, 0.013, 0.011, 0.015, 0.012, 0.015, 0.014, 0.011, 0.012, 0.014, 0.016, 0.013, 0.013, 0.012, 0.012, 0.013, 0.015, 0.014, 0.013, 0.012, 0.014, 0.013, 0.013, 0.012, 0.015, 0.013, 0.013, 0.011, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.014, 0.011, 0.014, 0.012, 0.014, 0.013, 0.013, 0.015, 0.012, 0.015, 0.014, 0.012, 0.013, 0.012, 0.015, 0.011, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.012, 0.016, 0.012, 0.014, 0.012, 0.013, 0.015, 0.013, 0.012, 0.013, 0.014, 0.012, 0.012, 0.015, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.012, 0.014, 0.013, 0.014, 0.014, 0.016, 0.014, 0.014, 0.012, 0.014, 0.014, 0.013, 0.014, 0.014, 0.014, 0.011, 0.013, 0.012, 0.016, 0.011, 0.014, 0.016, 0.012, 0.012, 0.012, 0.013, 0.012, 0.015, 0.013, 0.014, 0.014, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.012, 0.015, 0.014, 0.014, 0.014, 0.013, 0.012, 0.015, 0.012, 0.012, 0.013, 0.014, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.014, 0.015, 0.013, 0.015, 0.013, 0.015, 0.011, 0.014, 0.014, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013]\n",
      "[Feb 07, 16:47:51] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:47:51] #> Got bucket_cutoffs = tensor([-8.9867e-03,  6.2184e-06,  8.9502e-03]) and bucket_weights = tensor([-0.0191, -0.0035,  0.0035,  0.0190])\n",
      "[Feb 07, 16:47:51] avg_residual = 0.013261500746011734\n",
      "[Feb 07, 16:47:51] [0] \t\t #> Encoding 1452 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/23 [00:00<00:08,  2.49it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:08,  2.58it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:01<00:07,  2.63it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:01<00:07,  2.63it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:06,  2.63it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:02<00:06,  2.60it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:02<00:06,  2.62it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:03<00:05,  2.61it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:03<00:05,  2.53it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:03<00:05,  2.45it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:04<00:05,  2.36it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:04<00:04,  2.45it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:05<00:04,  2.46it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:05<00:03,  2.51it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:05<00:03,  2.56it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:06<00:02,  2.59it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:06<00:02,  2.59it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:07<00:01,  2.62it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:07<00:01,  2.62it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:07<00:01,  2.57it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:08<00:00,  2.54it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:08<00:00,  2.59it/s]\u001b[A\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.60it/s]\u001b[A\n",
      "1it [00:09,  9.04s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1984.06it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 153495.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:00] [0] \t\t #> Saving chunk 0: \t 1,452 passages and 12,149 embeddings. From #0 onward.\n",
      "[Feb 07, 16:48:00] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:48:00] [0] \t\t Found all files!\n",
      "[Feb 07, 16:48:00] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:48:00] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:48:00] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:48:00] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:48:00] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:48:00] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:48:00] len(emb2pid) = 12149\n",
      "[Feb 07, 16:48:00] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3903_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:48:00] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3903_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:48:01] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:48:01] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3907_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/3907_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_3907_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:48:04] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:05] [0] \t\t # of sampled PIDs = 498 \t sampled_pids[:3] = [213, 375, 5]\n",
      "[Feb 07, 16:48:05] [0] \t\t #> Encoding 498 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:03<00:00,  2.34it/s]\n",
      "WARNING clustering 4208 points to 1024 centroids: please provide at least 39936 training points\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:08] [0] \t\t avg_doclen_est = 8.893574714660645 \t len(local_sample) = 498\n",
      "[Feb 07, 16:48:08] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:48:08] [0] \t\t *Estimated* 4,429 embeddings.\n",
      "[Feb 07, 16:48:08] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3907_2bits/plan.json ..\n",
      "Clustering 4208 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.06 s, search 0.05 s): objective=361.489 imbalance=1.457 nsplit=0       \n",
      "[0.011, 0.011, 0.012, 0.01, 0.011, 0.009, 0.011, 0.012, 0.012, 0.012, 0.01, 0.01, 0.011, 0.009, 0.009, 0.009, 0.011, 0.01, 0.012, 0.011, 0.01, 0.012, 0.009, 0.01, 0.01, 0.011, 0.009, 0.01, 0.011, 0.01, 0.011, 0.01, 0.011, 0.009, 0.011, 0.008, 0.009, 0.01, 0.01, 0.008, 0.012, 0.01, 0.011, 0.009, 0.011, 0.009, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.011, 0.01, 0.009, 0.012, 0.009, 0.013, 0.009, 0.008, 0.01, 0.008, 0.011, 0.012, 0.01, 0.009, 0.012, 0.011, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.009, 0.013, 0.012, 0.011, 0.01, 0.01, 0.01, 0.012, 0.01, 0.011, 0.01, 0.009, 0.008, 0.009, 0.012, 0.011, 0.012, 0.01, 0.01, 0.009, 0.011, 0.012, 0.01, 0.012, 0.012, 0.01, 0.009, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.009, 0.01, 0.01, 0.012, 0.01, 0.012, 0.009, 0.01, 0.012, 0.009, 0.011, 0.009, 0.014, 0.012, 0.01, 0.012, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.012, 0.01, 0.01, 0.01, 0.01, 0.012, 0.01, 0.009, 0.01, 0.011, 0.011, 0.009, 0.011, 0.01, 0.011, 0.01, 0.01, 0.009, 0.011, 0.009, 0.01, 0.01, 0.01, 0.009, 0.012, 0.011, 0.011, 0.012, 0.01, 0.01, 0.01, 0.011, 0.009, 0.01, 0.011, 0.011, 0.011, 0.009, 0.011, 0.01, 0.013, 0.01, 0.009, 0.011, 0.012, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.01, 0.013, 0.01, 0.01, 0.01, 0.012, 0.009, 0.009, 0.012, 0.009, 0.01, 0.011, 0.009, 0.009, 0.01, 0.011, 0.011, 0.012, 0.01, 0.011, 0.01, 0.011, 0.009, 0.009, 0.012, 0.008, 0.01, 0.01, 0.009, 0.009, 0.009, 0.01, 0.01, 0.009, 0.009, 0.01, 0.012, 0.011, 0.012, 0.011, 0.012, 0.008, 0.01, 0.01, 0.01, 0.011, 0.01, 0.01, 0.009, 0.01, 0.014, 0.009, 0.01, 0.011, 0.009, 0.012, 0.01, 0.012, 0.011, 0.011, 0.01, 0.01, 0.009, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.009, 0.008, 0.009, 0.009, 0.011, 0.01, 0.009, 0.01, 0.009, 0.01, 0.009, 0.009, 0.009, 0.011, 0.011, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.011, 0.011, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.009, 0.009, 0.009, 0.01, 0.01, 0.012, 0.012, 0.014, 0.011, 0.011, 0.011, 0.009, 0.01, 0.012, 0.01, 0.009, 0.009, 0.011, 0.011, 0.009, 0.01, 0.013, 0.009, 0.01, 0.009, 0.01, 0.009, 0.01, 0.01, 0.011, 0.01, 0.009, 0.01, 0.01, 0.009, 0.009, 0.011, 0.01, 0.011, 0.013, 0.009, 0.011, 0.01, 0.01, 0.012, 0.009, 0.01, 0.01, 0.009, 0.008, 0.01, 0.01, 0.009, 0.01, 0.011, 0.008, 0.009, 0.01, 0.01, 0.01, 0.01, 0.009, 0.01, 0.01, 0.01, 0.012, 0.009, 0.011, 0.009, 0.011, 0.012, 0.011, 0.011, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.011, 0.012, 0.013, 0.01, 0.013, 0.012, 0.01, 0.009, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.013, 0.009, 0.011, 0.013, 0.009, 0.011, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.012, 0.009, 0.009, 0.011, 0.01, 0.009, 0.014, 0.009, 0.009, 0.01, 0.01, 0.009, 0.012, 0.012, 0.01, 0.011, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.009, 0.011, 0.01, 0.009, 0.009, 0.01, 0.009, 0.012, 0.009, 0.01, 0.01, 0.009, 0.011, 0.011, 0.009, 0.011, 0.01, 0.009, 0.01, 0.01, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.011, 0.009, 0.009, 0.011, 0.009, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.01, 0.011, 0.01, 0.011, 0.011, 0.01, 0.011, 0.009, 0.012, 0.011, 0.011, 0.009, 0.011, 0.01, 0.01, 0.011, 0.01, 0.009, 0.011, 0.01, 0.012, 0.009, 0.01, 0.012, 0.009, 0.009, 0.01, 0.011, 0.01, 0.01, 0.013, 0.01, 0.008, 0.008, 0.01, 0.011, 0.011, 0.012, 0.01, 0.01, 0.009, 0.012, 0.01, 0.009, 0.009, 0.01, 0.01, 0.009, 0.011, 0.01, 0.011, 0.012, 0.008, 0.01, 0.01, 0.01, 0.009, 0.01, 0.012, 0.011, 0.01, 0.012, 0.01, 0.009, 0.011, 0.01, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.009, 0.013, 0.011, 0.009, 0.011, 0.011, 0.01, 0.009, 0.011, 0.009, 0.01, 0.012, 0.011, 0.011, 0.012, 0.009, 0.01, 0.011, 0.011, 0.01, 0.011, 0.012, 0.008, 0.011, 0.011, 0.01, 0.009, 0.011, 0.009, 0.008, 0.009, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.008, 0.009, 0.012, 0.01, 0.01, 0.011, 0.009, 0.01, 0.01, 0.01, 0.012, 0.01, 0.009, 0.011, 0.009, 0.009, 0.011, 0.009, 0.011, 0.011, 0.008, 0.011, 0.009, 0.011, 0.01, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.01, 0.01, 0.011, 0.009, 0.01, 0.009, 0.008, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.01, 0.012, 0.008, 0.011, 0.009, 0.01, 0.01, 0.011, 0.012, 0.011, 0.012, 0.011, 0.009, 0.011, 0.009, 0.013, 0.009, 0.011, 0.011, 0.009, 0.01, 0.012, 0.011, 0.01, 0.01, 0.011, 0.009, 0.012, 0.009, 0.01, 0.011, 0.01, 0.011, 0.01, 0.01, 0.01, 0.01, 0.01, 0.009, 0.008, 0.009, 0.014, 0.01, 0.011, 0.01, 0.01, 0.013, 0.01, 0.009, 0.011, 0.01, 0.01, 0.01, 0.013, 0.011, 0.011, 0.009, 0.01, 0.012, 0.011, 0.01, 0.011, 0.011, 0.01, 0.008, 0.011, 0.011, 0.011, 0.01, 0.011, 0.01, 0.009, 0.011, 0.01, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.009, 0.011, 0.01, 0.009, 0.011, 0.01, 0.01, 0.009, 0.01, 0.009, 0.01, 0.011, 0.011, 0.012, 0.011, 0.01, 0.01, 0.011, 0.01, 0.011, 0.01, 0.009, 0.012, 0.01, 0.01, 0.009, 0.011, 0.012, 0.01, 0.01, 0.011, 0.009, 0.011, 0.01, 0.009, 0.008, 0.01, 0.01, 0.008, 0.01, 0.012, 0.01, 0.011, 0.011, 0.012, 0.01, 0.012, 0.009, 0.011, 0.009, 0.012, 0.009, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011]\n",
      "[Feb 07, 16:48:08] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:48:08] #> Got bucket_cutoffs = tensor([-6.5632e-03, -2.0458e-05,  6.4871e-03]) and bucket_weights = tensor([-0.0144, -0.0025,  0.0024,  0.0144])\n",
      "[Feb 07, 16:48:08] avg_residual = 0.010243379510939121\n",
      "[Feb 07, 16:48:08] [0] \t\t #> Encoding 498 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:03,  2.28it/s]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [00:01<00:01,  2.42it/s]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:02<00:01,  2.42it/s]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:02<00:00,  2.42it/s]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:02<00:00,  2.43it/s]\u001b[A\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.49it/s]\u001b[A\n",
      "1it [00:03,  3.28s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2513.06it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 172343.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:12] [0] \t\t #> Saving chunk 0: \t 498 passages and 4,429 embeddings. From #0 onward.\n",
      "[Feb 07, 16:48:12] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:48:12] [0] \t\t Found all files!\n",
      "[Feb 07, 16:48:12] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:48:12] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:48:12] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:48:12] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:48:12] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:48:12] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:48:12] len(emb2pid) = 4429\n",
      "[Feb 07, 16:48:12] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3907_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:48:12] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_3907_2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Feb 07, 16:48:12] #> Loading collection...\n",
      "0M \n",
      "\n",
      "\n",
      "[Feb 07, 16:48:12] #> Creating directory /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_280801_2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 0.0001,\n",
      "    \"maxsteps\": 2998,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 768,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/home\\/sondors\\/Documents\\/ColBERT_weights\\/bert-base-multilingual-cased_dim_768_bsize_230_lr04_use_ib_negatives\\/none\\/2024-01\\/27\\/16.55.29\\/checkpoints\\/colbert-2998-finish\",\n",
      "    \"triples\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/triples_X1_13_categories_shuffle.json\",\n",
      "    \"collection\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\\/tsv\\/280801_models.tsv\",\n",
      "    \"queries\": \"\\/mnt\\/vdb1\\/Datasets\\/ColBERT_data\\/13_categories\\/train\\/queries_train_13_categories.tsv\",\n",
      "    \"index_name\": \"models_280801_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/sondors\\/Documents\\/price\\/ColBERT\\/tutorial\",\n",
      "    \"experiment\": \"bert-base-multilingual-cased-2998\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/07\\/16.36.05\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Feb 07, 16:48:16] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:16] [0] \t\t # of sampled PIDs = 1709 \t sampled_pids[:3] = [853, 1500, 20]\n",
      "[Feb 07, 16:48:16] [0] \t\t #> Encoding 1709 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:15<00:00,  1.76it/s]\n",
      "WARNING clustering 15386 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:32] [0] \t\t avg_doclen_est = 9.476302146911621 \t len(local_sample) = 1,709\n",
      "[Feb 07, 16:48:32] [0] \t\t Creaing 1,024 partitions.\n",
      "[Feb 07, 16:48:32] [0] \t\t *Estimated* 16,195 embeddings.\n",
      "[Feb 07, 16:48:32] [0] \t\t #> Saving the indexing plan to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_280801_2bits/plan.json ..\n",
      "Clustering 15386 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 3 (0.20 s, search 0.19 s): objective=3269.42 imbalance=1.504 nsplit=0       \n",
      "[0.015, 0.014, 0.015, 0.013, 0.016, 0.013, 0.013, 0.014, 0.013, 0.016, 0.012, 0.012, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.015, 0.012, 0.014, 0.014, 0.015, 0.011, 0.011, 0.012, 0.014, 0.014, 0.015, 0.013, 0.012, 0.013, 0.011, 0.013, 0.012, 0.013, 0.012, 0.014, 0.014, 0.014, 0.014, 0.014, 0.011, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.013, 0.012, 0.013, 0.016, 0.012, 0.011, 0.014, 0.012, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.014, 0.014, 0.013, 0.013, 0.012, 0.011, 0.014, 0.012, 0.013, 0.014, 0.013, 0.013, 0.012, 0.017, 0.013, 0.014, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.012, 0.011, 0.014, 0.014, 0.013, 0.014, 0.012, 0.013, 0.014, 0.014, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.015, 0.013, 0.015, 0.014, 0.013, 0.014, 0.012, 0.012, 0.013, 0.014, 0.014, 0.014, 0.015, 0.011, 0.013, 0.013, 0.013, 0.011, 0.015, 0.014, 0.013, 0.014, 0.015, 0.013, 0.012, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.011, 0.014, 0.014, 0.012, 0.015, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.014, 0.012, 0.013, 0.014, 0.013, 0.012, 0.013, 0.012, 0.011, 0.013, 0.013, 0.013, 0.015, 0.013, 0.013, 0.012, 0.013, 0.011, 0.012, 0.013, 0.01, 0.013, 0.013, 0.011, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.014, 0.014, 0.013, 0.013, 0.014, 0.013, 0.012, 0.013, 0.013, 0.014, 0.015, 0.012, 0.015, 0.014, 0.013, 0.016, 0.01, 0.012, 0.012, 0.011, 0.014, 0.013, 0.013, 0.013, 0.015, 0.014, 0.013, 0.012, 0.013, 0.012, 0.013, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.011, 0.011, 0.013, 0.013, 0.014, 0.012, 0.012, 0.013, 0.011, 0.014, 0.013, 0.013, 0.012, 0.014, 0.014, 0.014, 0.013, 0.013, 0.014, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.014, 0.013, 0.015, 0.013, 0.013, 0.012, 0.012, 0.012, 0.014, 0.013, 0.014, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.014, 0.015, 0.012, 0.013, 0.015, 0.012, 0.014, 0.011, 0.013, 0.013, 0.012, 0.012, 0.013, 0.012, 0.011, 0.014, 0.013, 0.012, 0.011, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.013, 0.012, 0.014, 0.012, 0.013, 0.012, 0.011, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.012, 0.011, 0.012, 0.013, 0.013, 0.013, 0.013, 0.011, 0.015, 0.013, 0.014, 0.012, 0.014, 0.013, 0.015, 0.012, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.014, 0.013, 0.012, 0.014, 0.013, 0.015, 0.013, 0.015, 0.013, 0.014, 0.014, 0.014, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.013, 0.015, 0.011, 0.014, 0.016, 0.011, 0.014, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.012, 0.013, 0.013, 0.014, 0.013, 0.012, 0.012, 0.014, 0.013, 0.012, 0.014, 0.014, 0.011, 0.013, 0.012, 0.013, 0.014, 0.015, 0.013, 0.013, 0.013, 0.012, 0.014, 0.014, 0.013, 0.012, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.012, 0.013, 0.016, 0.012, 0.014, 0.011, 0.012, 0.013, 0.014, 0.011, 0.013, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.012, 0.012, 0.012, 0.013, 0.013, 0.015, 0.012, 0.015, 0.011, 0.014, 0.012, 0.014, 0.011, 0.012, 0.013, 0.014, 0.013, 0.012, 0.014, 0.015, 0.013, 0.012, 0.012, 0.013, 0.014, 0.014, 0.011, 0.013, 0.012, 0.013, 0.015, 0.011, 0.011, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.012, 0.013, 0.012, 0.015, 0.015, 0.012, 0.013, 0.014, 0.012, 0.011, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.014, 0.013, 0.012, 0.011, 0.014, 0.013, 0.013, 0.013, 0.013, 0.014, 0.013, 0.013, 0.014, 0.013, 0.013, 0.011, 0.013, 0.015, 0.013, 0.013, 0.014, 0.014, 0.013, 0.014, 0.012, 0.012, 0.015, 0.012, 0.013, 0.014, 0.012, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.012, 0.013, 0.013, 0.012, 0.013, 0.014, 0.013, 0.014, 0.014, 0.011, 0.013, 0.014, 0.012, 0.012, 0.013, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.013, 0.014, 0.011, 0.012, 0.012, 0.012, 0.014, 0.014, 0.013, 0.015, 0.011, 0.012, 0.012, 0.014, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.013, 0.011, 0.014, 0.013, 0.01, 0.012, 0.011, 0.013, 0.014, 0.011, 0.013, 0.014, 0.013, 0.012, 0.013, 0.012, 0.012, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.014, 0.012, 0.013, 0.014, 0.012, 0.012, 0.011, 0.011, 0.012, 0.014, 0.013, 0.013, 0.013, 0.013, 0.012, 0.013, 0.012, 0.013, 0.011, 0.014, 0.012, 0.014, 0.013, 0.012, 0.015, 0.013, 0.015, 0.014, 0.012, 0.014, 0.013, 0.014, 0.011, 0.013, 0.014, 0.012, 0.012, 0.013, 0.015, 0.014, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.012, 0.012, 0.014, 0.012, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.012, 0.016, 0.013, 0.013, 0.012, 0.012, 0.015, 0.012, 0.013, 0.013, 0.012, 0.013, 0.012, 0.014, 0.014, 0.012, 0.013, 0.013, 0.013, 0.014, 0.012, 0.013, 0.013, 0.011, 0.012, 0.012, 0.013, 0.014, 0.013, 0.013, 0.013, 0.012, 0.013, 0.013, 0.012, 0.014, 0.015, 0.014, 0.011, 0.013, 0.011, 0.014, 0.011, 0.013, 0.015, 0.012, 0.012, 0.013, 0.013, 0.012, 0.015, 0.013, 0.013, 0.014, 0.015, 0.012, 0.012, 0.012, 0.012, 0.014, 0.012, 0.013, 0.014, 0.013, 0.012, 0.012, 0.014, 0.014, 0.014, 0.013, 0.014, 0.012, 0.014, 0.012, 0.013, 0.012, 0.014, 0.013, 0.013, 0.013, 0.016, 0.012, 0.013, 0.014, 0.014, 0.013, 0.014, 0.013, 0.014, 0.01, 0.014, 0.013, 0.014, 0.013, 0.012, 0.012, 0.013, 0.013]\n",
      "[Feb 07, 16:48:32] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Feb 07, 16:48:32] #> Got bucket_cutoffs = tensor([-8.5474e-03, -1.2748e-05,  8.5449e-03]) and bucket_weights = tensor([-0.0184, -0.0033,  0.0033,  0.0185])\n",
      "[Feb 07, 16:48:32] avg_residual = 0.012927062809467316\n",
      "[Feb 07, 16:48:32] [0] \t\t #> Encoding 1709 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 1/27 [00:00<00:14,  1.80it/s]\u001b[A\n",
      "  7%|▋         | 2/27 [00:01<00:13,  1.83it/s]\u001b[A\n",
      " 11%|█         | 3/27 [00:01<00:12,  1.85it/s]\u001b[A\n",
      " 15%|█▍        | 4/27 [00:02<00:12,  1.85it/s]\u001b[A\n",
      " 19%|█▊        | 5/27 [00:02<00:12,  1.76it/s]\u001b[A\n",
      " 22%|██▏       | 6/27 [00:03<00:11,  1.79it/s]\u001b[A\n",
      " 26%|██▌       | 7/27 [00:03<00:10,  1.82it/s]\u001b[A\n",
      " 30%|██▉       | 8/27 [00:04<00:10,  1.83it/s]\u001b[A\n",
      " 33%|███▎      | 9/27 [00:04<00:09,  1.84it/s]\u001b[A\n",
      " 37%|███▋      | 10/27 [00:05<00:09,  1.86it/s]\u001b[A\n",
      " 41%|████      | 11/27 [00:05<00:08,  1.86it/s]\u001b[A\n",
      " 44%|████▍     | 12/27 [00:06<00:08,  1.86it/s]\u001b[A\n",
      " 48%|████▊     | 13/27 [00:07<00:07,  1.82it/s]\u001b[A\n",
      " 52%|█████▏    | 14/27 [00:07<00:07,  1.76it/s]\u001b[A\n",
      " 56%|█████▌    | 15/27 [00:08<00:06,  1.80it/s]\u001b[A\n",
      " 59%|█████▉    | 16/27 [00:08<00:06,  1.82it/s]\u001b[A\n",
      " 63%|██████▎   | 17/27 [00:09<00:05,  1.84it/s]\u001b[A\n",
      " 67%|██████▋   | 18/27 [00:09<00:04,  1.85it/s]\u001b[A\n",
      " 70%|███████   | 19/27 [00:10<00:04,  1.86it/s]\u001b[A\n",
      " 74%|███████▍  | 20/27 [00:10<00:03,  1.85it/s]\u001b[A\n",
      " 78%|███████▊  | 21/27 [00:11<00:03,  1.84it/s]\u001b[A\n",
      " 81%|████████▏ | 22/27 [00:12<00:02,  1.83it/s]\u001b[A\n",
      " 85%|████████▌ | 23/27 [00:12<00:02,  1.82it/s]\u001b[A\n",
      " 89%|████████▉ | 24/27 [00:13<00:01,  1.82it/s]\u001b[A\n",
      " 93%|█████████▎| 25/27 [00:13<00:01,  1.80it/s]\u001b[A\n",
      " 96%|█████████▋| 26/27 [00:14<00:00,  1.80it/s]\u001b[A\n",
      "100%|██████████| 27/27 [00:14<00:00,  1.84it/s]\u001b[A\n",
      "1it [00:14, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:47] [0] \t\t #> Saving chunk 0: \t 1,709 passages and 16,195 embeddings. From #0 onward.\n",
      "[Feb 07, 16:48:47] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 07, 16:48:47] [0] \t\t Found all files!\n",
      "[Feb 07, 16:48:47] [0] \t\t #> Building IVF...\n",
      "[Feb 07, 16:48:47] [0] \t\t #> Loading codes...\n",
      "[Feb 07, 16:48:47] [0] \t\t Sorting codes...\n",
      "[Feb 07, 16:48:47] [0] \t\t Getting unique codes...\n",
      "[Feb 07, 16:48:47] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 07, 16:48:47] #> Building the emb2pid mapping..\n",
      "[Feb 07, 16:48:47] len(emb2pid) = 16195\n",
      "[Feb 07, 16:48:47] #> Saved optimized IVF to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_280801_2bits/ivf.pid.pt\n",
      "[Feb 07, 16:48:47] [0] \t\t #> Saving the indexing metadata to /home/sondors/Documents/price/ColBERT/tutorial/bert-base-multilingual-cased-2998/indexes/models_280801_2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1797.05it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 135176.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "for cat_id in id_category.keys():\n",
    "    models_colbert = Collection(path=os.path.join(dst_fld, \"tsv\", f\"{cat_id}_models.tsv\"))\n",
    "    index_name = f'models_{cat_id}_{nbits}bits'\n",
    "    indexer = save_index(ckpt_pth, doc_maxlen, nbits, kmeans_niters, nranks, dst_fld, experiment, models_colbert, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск матча по индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:48] #> Loading collection...\n",
      "0M \n",
      "[Feb 07, 16:48:48] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Feb 07, 16:48:49] #> Loading codec...\n",
      "[Feb 07, 16:48:49] #> Loading IVF...\n",
      "[Feb 07, 16:48:49] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:49] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1470.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:49] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 149.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:49] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 07, 16:48:49] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondors/anaconda3/envs/colbert_cpu/lib/python3.8/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global), \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([   101,    100,  37077,    524,  19079, 105694,  37077,  29723,  14248,\n",
      "         10457,    156,  11396,    117,    129,    512,  18683,    120,  16196,\n",
      "           512,  18683,    117,  52742,    118,  36448,    116,  29494,  18062,\n",
      "           117,  10956,  67459,  19954,    102])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 302.45it/s]\n"
     ]
    }
   ],
   "source": [
    "offers = {\n",
    "    0: 'Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global)',\n",
    "    1: 'Планшет Samsung Galaxy Tab S8 128GB 5G Silver (SM-X706B)',\n",
    "    2: 'Планшет Samsung Galaxy Tab S8+ 128GB Wi-Fi Pink Gold (SM-X800)'\n",
    "    }\n",
    "\n",
    "src_fld = \"/home/sondors/Documents/price/ColBERT/tutorial\"\n",
    "cat_id = 2801 # мобильные телефоны\n",
    "cat_id = 510401 # планшеты\n",
    "index_name = f'models_{cat_id}_{nbits}bits'\n",
    "models_id_colbert = Collection(path=os.path.join(src_fld, \"tsv\", f\"{cat_id}_models_id.tsv\"))\n",
    "\n",
    "top_n = top_n_similar(offers, src_fld, nranks, experiment, index_name, models_id_colbert, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем выдачу топ N моделей для каждого оффера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung Планшет Samsung Galaxy Tab S8, 8 ГБ/128 ГБ, Wi-Fi + Cellular, со стилусом, графит (Global)\n",
      "\t5144478: Samsung Galaxy Tab S8 --> 28.97\n",
      "\t5144477: Samsung Galaxy Tab S8+ --> 24.97\n",
      "\t937550: Samsung Galaxy Tab S2 9.7 SM-T813 --> 24.0\n",
      "\t5144479: Samsung Galaxy Tab S8 Ultra --> 23.98\n",
      "\t816746: Samsung Galaxy Tab S2 9.7 SM-T810 --> 23.64\n",
      "____________________________________________________________\n",
      "Планшет Samsung Galaxy Tab S8 128GB 5G Silver (SM-X706B)\n",
      "\t5144478: Samsung Galaxy Tab S8 --> 28.15\n",
      "\t5144477: Samsung Galaxy Tab S8+ --> 23.94\n",
      "\t937550: Samsung Galaxy Tab S2 9.7 SM-T813 --> 23.21\n",
      "\t623216: Samsung Galaxy Tab S 8.4 SM-T705 --> 22.82\n",
      "\t816746: Samsung Galaxy Tab S2 9.7 SM-T810 --> 22.65\n",
      "____________________________________________________________\n",
      "Планшет Samsung Galaxy Tab S8+ 128GB Wi-Fi Pink Gold (SM-X800)\n",
      "\t5144477: Samsung Galaxy Tab S8+ --> 26.03\n",
      "\t5144478: Samsung Galaxy Tab S8 --> 20.32\n",
      "\t4509798: Samsung Galaxy Tab S7+ 12.4 128Gb --> 18.63\n",
      "\t937550: Samsung Galaxy Tab S2 9.7 SM-T813 --> 18.35\n",
      "\t5144479: Samsung Galaxy Tab S8 Ultra --> 18.31\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_n)):\n",
    "    print(offers[i])\n",
    "    for j in range(len(top_n[i]['model_ids'])):\n",
    "        id = top_n[i]['model_ids'][j]\n",
    "        sim = top_n[i]['similarity'][j]\n",
    "        model = list(df_models[df_models.model_id == id]['full_name'])[0]\n",
    "        print(f\"\\t{id}: {model} --> {round(float(sim), 2)}\")\n",
    "    print(\"_\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_ids': [5144478, 5144477, 937550, 5144479, 816746],\n",
       "  'similarity': [28.973285675048828,\n",
       "   24.965259552001953,\n",
       "   23.999181747436523,\n",
       "   23.97533416748047,\n",
       "   23.638072967529297]},\n",
       " {'model_ids': [5144478, 5144477, 937550, 623216, 816746],\n",
       "  'similarity': [28.152374267578125,\n",
       "   23.93535041809082,\n",
       "   23.20758819580078,\n",
       "   22.81966781616211,\n",
       "   22.648759841918945]},\n",
       " {'model_ids': [5144477, 5144478, 4509798, 937550, 5144479],\n",
       "  'similarity': [26.02865982055664,\n",
       "   20.31949234008789,\n",
       "   18.63184928894043,\n",
       "   18.34659194946289,\n",
       "   18.30866813659668]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
